
# Release Notes worksheet distributed

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please clean up and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## distributed
### bc breaking
### deprecation
### new features
### improvements
- [BE] Update TensorPipe pin ([#159834](https://github.com/pytorch/pytorch/pull/159834))
### bug fixes
- [SymmMem] Use host/nvshmem_api.h for backward compat ([#159061](https://github.com/pytorch/pytorch/pull/159061))
- [SymmMem] Use global pe for put and get ([#162394](https://github.com/pytorch/pytorch/pull/162394))
### performance
### docs
### devs
### Untopiced
- [PG/nccl] improvements to eager init ([#156748](https://github.com/pytorch/pytorch/pull/156748))
- [dcp_poc] Introduce a new simple rank local checkpointer ([#156142](https://github.com/pytorch/pytorch/pull/156142))
- bug fix for losing shape on wrapper tensor for DTensor ([#156774](https://github.com/pytorch/pytorch/pull/156774))
- [PG/nccl] Simplify uniqueHash management ([#156790](https://github.com/pytorch/pytorch/pull/156790))
- [SymmMem] Remove redundant dist.barrier in Triton NVSHMEM tests & add device‐side signal_op support ([#156684](https://github.com/pytorch/pytorch/pull/156684))
- [SymmMem] Allow selection of allocation backend ([#156661](https://github.com/pytorch/pytorch/pull/156661))
- [BE] Remove SymmMem allocator destruct log ([#157020](https://github.com/pytorch/pytorch/pull/157020))
- Add pg transport and tests ([#154653](https://github.com/pytorch/pytorch/pull/154653))
- [sym_mem] Further Fix NCCL symm mem unit test ([#157156](https://github.com/pytorch/pytorch/pull/157156))
- Create a base Checkpointer and SyncCheckpointer and add dist barrier impl and  ([#156926](https://github.com/pytorch/pytorch/pull/156926))
- Fix FSDP offload pin_memory bug ([#157147](https://github.com/pytorch/pytorch/pull/157147))
- [DCP] OSS Zero Overhead Checkpointing Implementation ([#156207](https://github.com/pytorch/pytorch/pull/156207))
- [dtensor] relax device_mesh argument constraint in local_map ([#157049](https://github.com/pytorch/pytorch/pull/157049))
- [tp] improve parallelize_module API to support more cases ([#157182](https://github.com/pytorch/pytorch/pull/157182))
- Script for consolidation of sharded safetensor files ([#154743](https://github.com/pytorch/pytorch/pull/154743))
- HF - consolidate shards of safetensors files to full tensors in finish step ([#156705](https://github.com/pytorch/pytorch/pull/156705))
- [ROCm][SymmetricMemory] Performance improvements for two-shot allreduce ([#156746](https://github.com/pytorch/pytorch/pull/156746))
- [ROCm] Remove use of `warpsize` on host-side compilation ([#156979](https://github.com/pytorch/pytorch/pull/156979))
- [SymmMem] Add NVSHMEM_CHECK macro ([#157174](https://github.com/pytorch/pytorch/pull/157174))
- [PT] support custom all_gather and reduce_scatter comms ([#155189](https://github.com/pytorch/pytorch/pull/155189))
- Fix typo: 'Intializes' → 'Initializes' in _distributed_c10d.pyi docst… ([#157455](https://github.com/pytorch/pytorch/pull/157455))
- Support complex numbers in DTensor redistribute ([#157329](https://github.com/pytorch/pytorch/pull/157329))
- [c10d][PGNCCL] Add waitcounter for watchdog and heartbeat monitoring thread ([#157480](https://github.com/pytorch/pytorch/pull/157480))
- [symm_mem] Create a one side get api for symm mem ([#157294](https://github.com/pytorch/pytorch/pull/157294))
- Fix unsafe collective reorder past wait ([#157489](https://github.com/pytorch/pytorch/pull/157489))
- [PT][FSDP] fail `set_allocate_memory_from_process_group` if used together with custom comm hooks ([#157487](https://github.com/pytorch/pytorch/pull/157487))
- [dtensor] Rework partial propagation in pointwise op and support mul ([#157340](https://github.com/pytorch/pytorch/pull/157340))
- Add async checkpointing impl to experimental checkpointer and add a builder API ([#156927](https://github.com/pytorch/pytorch/pull/156927))
- [SymmMem] Move code to where it is used ([#157611](https://github.com/pytorch/pytorch/pull/157611))
- [Refactor] Remove unused variables ([#157654](https://github.com/pytorch/pytorch/pull/157654))
- Fix: Ensure writeback handles NO_SHARD correctly by flattening tensors before copying ([#154369](https://github.com/pytorch/pytorch/pull/154369))
- [FSDP2] Fix issue with set_reduce_scatter_divide_factor errors and MixedPrecisionPolicy  ([#155964](https://github.com/pytorch/pytorch/pull/155964))
- [DeviceMesh] Use user set backend and pg option even for the global mesh ([#157501](https://github.com/pytorch/pytorch/pull/157501))
- [DeviceMesh] Add error when users try to slice non contiguous flattened dim submesh ([#157523](https://github.com/pytorch/pytorch/pull/157523))
- [oss] Add version to metadata ([#155343](https://github.com/pytorch/pytorch/pull/155343))
- [c10d] support dynamic shapes for all_to_all_single_autograd ([#157521](https://github.com/pytorch/pytorch/pull/157521))
- Work: block_current_stream API ([#156883](https://github.com/pytorch/pytorch/pull/156883))
- [FSDP2] Use reduceOpSum for world size 1 ([#157529](https://github.com/pytorch/pytorch/pull/157529))
- torch.distributed: add initial _dist2 prototype API ([#157841](https://github.com/pytorch/pytorch/pull/157841))
- [HF][DCP] Upload local consolidated files to remote storage if needed ([#157371](https://github.com/pytorch/pytorch/pull/157371))
- Updates to safetensors checkpoint consolidation script to be faster ([#157936](https://github.com/pytorch/pytorch/pull/157936))
- [c10d][PGNCCL] Cleanup unused params for nccl comm split ([#157978](https://github.com/pytorch/pytorch/pull/157978))
- dist2: add group context manager ([#157988](https://github.com/pytorch/pytorch/pull/157988))
- [DTensor] Rewrite doc of TupleStrategy ([#158132](https://github.com/pytorch/pytorch/pull/158132))
- [c10d] ProcessGroupGloo: support per operation timeouts ([#158128](https://github.com/pytorch/pytorch/pull/158128))
- dist2: cleanup non-option methods on PG (missing, timeouts) ([#158123](https://github.com/pytorch/pytorch/pull/158123))
- dist2: add support for passing custom configs directly to PG ([#158147](https://github.com/pytorch/pytorch/pull/158147))
- Allow dynamic shapes for DTensor slice ([#157953](https://github.com/pytorch/pytorch/pull/157953))
- [DTensor] Fix grouped_mm strategy for invalid stride cases ([#158245](https://github.com/pytorch/pytorch/pull/158245))
- [DTensor] implement histc ([#158298](https://github.com/pytorch/pytorch/pull/158298))
- add device generalization support for distributed tests ([#156796](https://github.com/pytorch/pytorch/pull/156796))
- [c10d]Prototype of remote_group_merge ([#158287](https://github.com/pytorch/pytorch/pull/158287))
- [PP] Add eval() API to schedule ([#157795](https://github.com/pytorch/pytorch/pull/157795))
- Always disable ShardingPropagation cache if compiling ([#156868](https://github.com/pytorch/pytorch/pull/156868))
- [oss][hf][bug fix] Remove buggy consolidation logic ([#158380](https://github.com/pytorch/pytorch/pull/158380))
- [DTensor] Document redistribute_costs ([#158495](https://github.com/pytorch/pytorch/pull/158495))
- Make torch.distributed.breakpoint() set a long timeout ([#158481](https://github.com/pytorch/pytorch/pull/158481))
- [c10d][ez] Fix error message to reflect the correct API name ([#158668](https://github.com/pytorch/pytorch/pull/158668))
- [cca] [c10d] Refactor CUDAEventCache into separate files ([#158616](https://github.com/pytorch/pytorch/pull/158616))
- DCP safetensors test fix ([#158685](https://github.com/pytorch/pytorch/pull/158685))
- [SymmMem] Add NVSHMEM barrier_all, my_pe, n_pes support into Triton ([#158511](https://github.com/pytorch/pytorch/pull/158511))
- [SymmMem] Add NVSHMEM sync_all support into Triton ([#158512](https://github.com/pytorch/pytorch/pull/158512))
- [SymmMem] Add NVSHMEM alltoall support into Triton ([#158513](https://github.com/pytorch/pytorch/pull/158513))
- Use more fine-grained locks in sym mem kernels ([#158523](https://github.com/pytorch/pytorch/pull/158523))
- [DCP] Add support for ShardedTensor to PgTransport ([#158573](https://github.com/pytorch/pytorch/pull/158573))
- [SymmMem] Add NVSHMEM broadcast support into Triton ([#158514](https://github.com/pytorch/pytorch/pull/158514))
- [c10d] block_current_stream: correctness fixes ([#158757](https://github.com/pytorch/pytorch/pull/158757))
- [PGNCCLx] Bring split and merge for PGNCCLx ([#158790](https://github.com/pytorch/pytorch/pull/158790))
- [DeviceMesh] Make the repr shorter when debug ENV not set ([#158822](https://github.com/pytorch/pytorch/pull/158822))
- [DeviceMesh] Enable slicing a submesh with warnings ([#158899](https://github.com/pytorch/pytorch/pull/158899))
- [FSDP][Replicate] added replicate function that uses FSDP instead of DDP ([#158207](https://github.com/pytorch/pytorch/pull/158207))
- [a2av] Split in_out_splits into in_splits and out_splits_offsets ([#156743](https://github.com/pytorch/pytorch/pull/156743))
- fix forced loglevel in pytorch oss code ([#158820](https://github.com/pytorch/pytorch/pull/158820))
- [doc] remove FSDP1 developer note ([#158991](https://github.com/pytorch/pytorch/pull/158991))
- [a2av] Add token combine operator ([#156881](https://github.com/pytorch/pytorch/pull/156881))
- Device agnostic for DCP ([#158337](https://github.com/pytorch/pytorch/pull/158337))
- [TCPStore] Allow ping to be retried ([#159165](https://github.com/pytorch/pytorch/pull/159165))
- NUMA binding integration with elastic agent and torchrun ([#149334](https://github.com/pytorch/pytorch/pull/149334))
- support scalar tensor for functional all_gather ([#149913](https://github.com/pytorch/pytorch/pull/149913))
- [PP] Allow intermediate nodes in ZB to have multiple grads ([#159084](https://github.com/pytorch/pytorch/pull/159084))
- [c10d] Cleanup split_group logic using the newly built splitGroup ([#158488](https://github.com/pytorch/pytorch/pull/158488))
- [PP] Fix eval step under no_grad() ([#159293](https://github.com/pytorch/pytorch/pull/159293))
- [DTensor] dispatch to sharding prop over decomps ([#159324](https://github.com/pytorch/pytorch/pull/159324))
- [a2av] not returning out tensor from ops ([#159435](https://github.com/pytorch/pytorch/pull/159435))
- [RPC][TensorPipe] Fix import torch if compiled without TensorPipe ([#159461](https://github.com/pytorch/pytorch/pull/159461))
- [DCP] Improve error handling for process based async checkpointing ([#159374](https://github.com/pytorch/pytorch/pull/159374))
- [PP] Fix zero bubble schedules for eval() ([#159475](https://github.com/pytorch/pytorch/pull/159475))
- [c10d] Fix setGroupName and setGroupDesc in `group_split` and `merge_remote_group` ([#159429](https://github.com/pytorch/pytorch/pull/159429))
- Fix a bug of distributed 'gather' with uncontiguous tensors on the Gloo backend ([#158903](https://github.com/pytorch/pytorch/pull/158903))
- support fabric handles with symmetric memory ([#159319](https://github.com/pytorch/pytorch/pull/159319))
- [AOTI] Explicitly delete wait_tensor returned tensor ([#159502](https://github.com/pytorch/pytorch/pull/159502))
- [cutlass upgrade] Ignore unused-but-set-variable for AsyncMM.cu ([#159578](https://github.com/pytorch/pytorch/pull/159578))
- Fix a bug of distributed 'gather' with noncontiguous tensors on the NCCL backend. ([#159549](https://github.com/pytorch/pytorch/pull/159549))
- [C10D] Document barrier interaction with device_id ([#159389](https://github.com/pytorch/pytorch/pull/159389))
- [PP] Support OVERLAP_F_B computation type ([#158978](https://github.com/pytorch/pytorch/pull/158978))
- Edit a test case to detect potential bugs in all-gathering noncontiguous inputs in the Gloo backend ([#159542](https://github.com/pytorch/pytorch/pull/159542))
- fix compilation on cuda < 12.3 ([#159657](https://github.com/pytorch/pytorch/pull/159657))
- [c10d][nvshmem] modify is_nvshmem_available runtime check to work with static-linked library (#159558) ([#159561](https://github.com/pytorch/pytorch/pull/159561))
- [c10d][nvshmem] fix missing override compilation error for nvshmem symmetric code ([#159557](https://github.com/pytorch/pytorch/pull/159557))
- Revert #156868: Bring back symint check for sharding propagation cache ([#159671](https://github.com/pytorch/pytorch/pull/159671))
- check driver to be >=12.4 to use fabric handles ([#159697](https://github.com/pytorch/pytorch/pull/159697))
- [C10D] fix slow init due to repeated dns resolution failure ([#159596](https://github.com/pytorch/pytorch/pull/159596))
- Allow controlling PG backend and options via init_device_mesh ([#159371](https://github.com/pytorch/pytorch/pull/159371))
- [DCP][Prototype] Checkpoint replication via PGTransport (#157963) ([#159801](https://github.com/pytorch/pytorch/pull/159801))
- [AOTI] Fix memory leak from all_reduce ([#159818](https://github.com/pytorch/pytorch/pull/159818))
- HF component update to not use fsspec components ([#159405](https://github.com/pytorch/pytorch/pull/159405))
- DCP HF reader: use safe_open instead of reading the bytes ([#159406](https://github.com/pytorch/pytorch/pull/159406))
- Use only safetensors APIs in HFStorageReader ([#159681](https://github.com/pytorch/pytorch/pull/159681))
- [DTensor] Support user-supplied Generator for random ops ([#159933](https://github.com/pytorch/pytorch/pull/159933))
- [SymmMem] Add NVSHMEM Reduction support (sum, min, max) into Triton ([#158515](https://github.com/pytorch/pytorch/pull/158515))
- [SymmMem]  Standardize NVSHMEM Triton wrappers on byte-based APIs + improve code clarity ([#159136](https://github.com/pytorch/pytorch/pull/159136))
- [SymmMem] Add Triton 3.4 support to NVSHMEM Triton and fix CI tests (make device library discoverable + fix peer calculation bug)  ([#159701](https://github.com/pytorch/pytorch/pull/159701))
- [SymmMem] Initialize NVSHMEM module only for kernels that have nvshmem in their name ([#159734](https://github.com/pytorch/pytorch/pull/159734))
- [SymmMem] Refactor NVSHMEM Reduction API to be more ergonomic with automatic dtype‐based dispatch ([#159755](https://github.com/pytorch/pytorch/pull/159755))
- [SymmMem] Add helpful docstrings for all NVSHMEM APIs  ([#159756](https://github.com/pytorch/pytorch/pull/159756))
- [SymmMem] Send tensors with unerased type information to NVSHMEM Triton kernels ([#159788](https://github.com/pytorch/pytorch/pull/159788))
- [replicate][be] improved readability and cleaned up remaining DDP code ([#160133](https://github.com/pytorch/pytorch/pull/160133))
- [dcp][hf] Improve HF consolidation algorithm ([#158648](https://github.com/pytorch/pytorch/pull/158648))
- Fix requires_cuda to requires_cuda_and_triton ([#160222](https://github.com/pytorch/pytorch/pull/160222))
- Fix clang builds by adding headers ([#160252](https://github.com/pytorch/pytorch/pull/160252))
- [PP] Initialize P2P communicators on first step ([#160210](https://github.com/pytorch/pytorch/pull/160210))
- fix retaining multimem in symmetric memory ([#160343](https://github.com/pytorch/pytorch/pull/160343))
- move thread-local capture mode guard to include work.isStarted ([#160398](https://github.com/pytorch/pytorch/pull/160398))
- Support NUMA Binding for Callable Entrypoints ([#160163](https://github.com/pytorch/pytorch/pull/160163))
- Remove usage of fsspec in HF consolidation script ([#159392](https://github.com/pytorch/pytorch/pull/159392))
- [FSDP][Collectives] skipping allgather when world size is 1 ([#160135](https://github.com/pytorch/pytorch/pull/160135))
- [c10d] Error out the case when registering symmetric memory without eager init ([#160145](https://github.com/pytorch/pytorch/pull/160145))
- Add new function consolidate_safetensors_files_on_every_rank for HF consolidation ([#159393](https://github.com/pytorch/pytorch/pull/159393))
- Write full tensors out at once in HF consolidation script ([#159394](https://github.com/pytorch/pytorch/pull/159394))
- port 3 distributed test to Intel GPU and unified some common functions ([#158533](https://github.com/pytorch/pytorch/pull/158533))
- Support ddp zero hook XCCL path ([#159240](https://github.com/pytorch/pytorch/pull/159240))
- guard cuMulticastUnbind call ([#160499](https://github.com/pytorch/pytorch/pull/160499))
- [DCP][OSS] Rank local checkpointing in DCP without collectives ([#147758](https://github.com/pytorch/pytorch/pull/147758))
- [C10D] Add check_rng_sync util ([#160283](https://github.com/pytorch/pytorch/pull/160283))
- Fix wrong log file name in the docs of `torch.distributed.elastic.multiprocessing.start_processes()` ([#160396](https://github.com/pytorch/pytorch/pull/160396))
- [PP] Add DualPipeV schedule ([#159591](https://github.com/pytorch/pytorch/pull/159591))
- [SymmMem] Check return of nvshmem_malloc ([#160603](https://github.com/pytorch/pytorch/pull/160603))
- [PP] Rename _load_actions and validate ([#160558](https://github.com/pytorch/pytorch/pull/160558))
- [cuda][cupy] Improve cupy device placement when device is provided with explicit index ([#158529](https://github.com/pytorch/pytorch/pull/158529))
- harden fabric checks for symmetric memory ([#160790](https://github.com/pytorch/pytorch/pull/160790))
- [FSDP] Use post_reduce_stream.record_event() on hsdp+cpuoffload ([#160481](https://github.com/pytorch/pytorch/pull/160481))
- [dcp_poc] Fix parameter order in distributed checkpoint API to use path-first for consistency ([#160986](https://github.com/pytorch/pytorch/pull/160986))
- Fix error message for fsdp_pre_all_gather ([#160817](https://github.com/pytorch/pytorch/pull/160817))
- [dist] expose unsafe_get_ptr for dist.ProcessGroupNCCL.NCCLConfig ([#161136](https://github.com/pytorch/pytorch/pull/161136))
- [dcp][hf] Fix multi-rank consolidation for no files to process case ([#160660](https://github.com/pytorch/pytorch/pull/160660))
- [DCP][HF] Add option to parallelize reads in HF Storage Reader ([#160205](https://github.com/pytorch/pytorch/pull/160205))
- [fr] [xpu] Add FlightRecorder support for ProcessGroupXCCL ([#158568](https://github.com/pytorch/pytorch/pull/158568))
- Support NUMA Binding for Callable Entrypoints, Take 2 ([#161183](https://github.com/pytorch/pytorch/pull/161183))
- [nccl symm mem] don't use arg for mempool, correctly use symmetric registration in hooks ([#161238](https://github.com/pytorch/pytorch/pull/161238))
- [dtensor] Add propagate_tensor_meta function that skips cache if _are_we_tracing ([#161334](https://github.com/pytorch/pytorch/pull/161334))
- fix-unpin-memory-tensor-param ([#160992](https://github.com/pytorch/pytorch/pull/160992))
- Add early_stop kwarg to torch.utils.checkpoint ([#160781](https://github.com/pytorch/pytorch/pull/160781))
- [1/N][SymmMem] Add offset to handle, cache on base address ([#161470](https://github.com/pytorch/pytorch/pull/161470))
- [dtensor] support local_map as a decorator ([#161353](https://github.com/pytorch/pytorch/pull/161353))
- [DTensor] Wrap sharding prop error with contextual exception ([#161574](https://github.com/pytorch/pytorch/pull/161574))
- use host+device_id to make sure devices are unique in rendezvous request ([#161756](https://github.com/pytorch/pytorch/pull/161756))
- Add pg argument to consolidate_safetensors_files_on_every_rank ([#161421](https://github.com/pytorch/pytorch/pull/161421))
- Add batch option for send/recv_object_list ([#160342](https://github.com/pytorch/pytorch/pull/160342))
- [c10d][nvshmem] add nvshmem build rules and dependency for libtorch_cuda ([#159562](https://github.com/pytorch/pytorch/pull/159562))
- Make error message descriptive (#150627) ([#159423](https://github.com/pytorch/pytorch/pull/159423))
- [SymmMem] Add device guard before alloc ([#161214](https://github.com/pytorch/pytorch/pull/161214))
- [SymmMem] Make sure CUDA runtime is initialized before NVSHMEM init ([#161232](https://github.com/pytorch/pytorch/pull/161232))
- [SymmMem] Increase minimum nthreads to cover sync needs in NVL72 ([#161983](https://github.com/pytorch/pytorch/pull/161983))
- [SymmMem] Use non-blocking version of getmem ([#162006](https://github.com/pytorch/pytorch/pull/162006))
- [c10d] Lessen density of barrier warning ([#162015](https://github.com/pytorch/pytorch/pull/162015))
- [ROCm/Windows] Fix build failures and support some BLAS calls ([#161981](https://github.com/pytorch/pytorch/pull/161981))
- [Symmetric memory] set handle type for ROCm ([#161741](https://github.com/pytorch/pytorch/pull/161741))
- [PP] Add profiling to schedule execution ([#160753](https://github.com/pytorch/pytorch/pull/160753))
- [DCP][HuggingFace] Add Support for dequantization of SafeTensors checkpoints ([#160682](https://github.com/pytorch/pytorch/pull/160682))
- Don't require FakeStore to be passed into fake backend ([#162164](https://github.com/pytorch/pytorch/pull/162164))
- [SymmMem] Add a helper API to distinguish intra- and inter- node ([#161984](https://github.com/pytorch/pytorch/pull/161984))
- [SymmMem] Increase signal pad size for NVL72 ([#162026](https://github.com/pytorch/pytorch/pull/162026))
- [SymmMem] Feed tensor.data_ptr instead of handle.buffer_ptr into kernels ([#162193](https://github.com/pytorch/pytorch/pull/162193))
- fix incorrect interaction between DDPOptimizer and donated buffers ([#160745](https://github.com/pytorch/pytorch/pull/160745))
- [C10d][Gloo] Enable complex datatype support in ProcessGroupGloo ([#156633](https://github.com/pytorch/pytorch/pull/156633))
- Making batching rule for F.embedding DTensor-aware ([#162117](https://github.com/pytorch/pytorch/pull/162117))
- Fixed comment to match logic in distributed_c10d.py ([#162158](https://github.com/pytorch/pytorch/pull/162158))
- [AsyncTP] Fixes AsyncMM ([#162040](https://github.com/pytorch/pytorch/pull/162040))
- [DTensor] fix F.one_hot ([#162307](https://github.com/pytorch/pytorch/pull/162307))
- [DTensor] Check if tracing for sharding propagation to handle unhashable keys ([#160798](https://github.com/pytorch/pytorch/pull/160798))
- [SymmMem] Add team pool to hold duplicated teams for the same rank group ([#162320](https://github.com/pytorch/pytorch/pull/162320))
- [SymmMem] Better tuning of A2AV based on accurate node boundary ([#162003](https://github.com/pytorch/pytorch/pull/162003))
- Add missing fstream include to fix std::ofstream compilation error ([#162421](https://github.com/pytorch/pytorch/pull/162421))
- [SymmMEM] Allow to import _SymmetricMemory when NVSHMEM is not available ([#162142](https://github.com/pytorch/pytorch/pull/162142))
### not user facing
- [torch][test] skip `test_transformer_backend_inductor_fullgraph_True` ([#156763](https://github.com/pytorch/pytorch/pull/156763))
- [1/n] refactor the ring attention implementation ([#155441](https://github.com/pytorch/pytorch/pull/155441))
- [2/n] rewrite load balancing and sharding in context parallel ([#155442](https://github.com/pytorch/pytorch/pull/155442))
- Minor error message fix in device_mesh.py ([#157096](https://github.com/pytorch/pytorch/pull/157096))
- Fix `aten::index_put` args Dtensor type mismatch and add a propagation strategy ([#156240](https://github.com/pytorch/pytorch/pull/156240))
- XCCL changes for DDP ([#155497](https://github.com/pytorch/pytorch/pull/155497))
- all_gather_bucketing fx pass ([#157396](https://github.com/pytorch/pytorch/pull/157396))
- Introduce sync_cross_rank_decision ([#156287](https://github.com/pytorch/pytorch/pull/156287))
- Fixing misspelling in documentation ([#157565](https://github.com/pytorch/pytorch/pull/157565))
- Fix index_put propagate strategy arg unpack error ([#157671](https://github.com/pytorch/pytorch/pull/157671))
- [inductor_collectives] Make reorder_collectives_preserve_peak pass grouping nodes ([#157706](https://github.com/pytorch/pytorch/pull/157706))
- Fix slice op redistribute_cost compute ([#157178](https://github.com/pytorch/pytorch/pull/157178))
- [inductor collectives] sink waits iterative ([#157708](https://github.com/pytorch/pytorch/pull/157708))
- Added philox based RNG context for HPU device in Dtensor scenarios ([#156581](https://github.com/pytorch/pytorch/pull/156581))
- Add stack trace of exception to MultiProcContinousTest ([#157589](https://github.com/pytorch/pytorch/pull/157589))
- Fix einsum strategy shard dim > ndim ([#157593](https://github.com/pytorch/pytorch/pull/157593))
- Fixes typo in nccl_window_registration test ([#157293](https://github.com/pytorch/pytorch/pull/157293))
- [BE] fix typo in torch/distributed/tensor/: childs -> children ([#156609](https://github.com/pytorch/pytorch/pull/156609))
- [Easy] Fix the compilation warning ([#157889](https://github.com/pytorch/pytorch/pull/157889))
- [doc] Document an invariant in OpSpec ([#157805](https://github.com/pytorch/pytorch/pull/157805))
- [doc] DeviceMesh invariant on DTensorSpec ([#157806](https://github.com/pytorch/pytorch/pull/157806))
- [BE] Remove stale pyre-fixme ([#157816](https://github.com/pytorch/pytorch/pull/157816))
- [simple_fsdp] Port fx pass to bucket reduce_scatters ([#157780](https://github.com/pytorch/pytorch/pull/157780))
- Tests for #158030 ([#158033](https://github.com/pytorch/pytorch/pull/158033))
- [1/N] cost coverage improvment ([#157504](https://github.com/pytorch/pytorch/pull/157504))
- [2/N] cost coverage improvment  ([#157738](https://github.com/pytorch/pytorch/pull/157738))
- Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead ([#156165](https://github.com/pytorch/pytorch/pull/156165))
- [DTensor] support split op on Partial placement ([#157991](https://github.com/pytorch/pytorch/pull/157991))
- [Reducer] Remove custom handling of view tensors for MTIA ([#157882](https://github.com/pytorch/pytorch/pull/157882))
- fix MPCT destroy_pg call ([#157952](https://github.com/pytorch/pytorch/pull/157952))
- [BE][2/16] fix typos in torch/ (torch/_*/) ([#156312](https://github.com/pytorch/pytorch/pull/156312))
- [test][distributed][vllm] stabilize the p2p sharing through ipc ([#158089](https://github.com/pytorch/pytorch/pull/158089))
- [BE][2/16] fix typos in torch/ (torch/_*/) ([#156312](https://github.com/pytorch/pytorch/pull/156312))
- [c10d] Prototype of `group_split` for dist2 work ([#157716](https://github.com/pytorch/pytorch/pull/157716))
- [DTensor] have split_strategy return OpStrategy instead of TupleStrategy ([#158051](https://github.com/pytorch/pytorch/pull/158051))
- [DTensor][BE] improve DTensor ops correctness check utils ([#158112](https://github.com/pytorch/pytorch/pull/158112))
- [SymmMem] Fix NCCL Hang in NVSHMEM Triton Wait Until Test ([#158167](https://github.com/pytorch/pytorch/pull/158167))
- Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead ([#156165](https://github.com/pytorch/pytorch/pull/156165))
- [DTensor][BE] add document to ShardingPropagator.register_op_strategy ([#158362](https://github.com/pytorch/pytorch/pull/158362))
- [BE][15/16] fix typos in torch/ (torch/distributed/tensor/) ([#156605](https://github.com/pytorch/pytorch/pull/156605))
- Fix clamp(min/max) strategy ([#158619](https://github.com/pytorch/pytorch/pull/158619))
- Use init_device_mesh API for select tests where possible ([#158675](https://github.com/pytorch/pytorch/pull/158675))
- Using torch.accelerator in comm_mode_features_example.py and visualize_sharding_example.py ([#157317](https://github.com/pytorch/pytorch/pull/157317))
- Making input dynamically adjust. ([#157324](https://github.com/pytorch/pytorch/pull/157324))
- [doc] Updates to distributed.md for XCCL backend ([#155834](https://github.com/pytorch/pytorch/pull/155834))
- [1/N] support of replication fallback strategy ([#158046](https://github.com/pytorch/pytorch/pull/158046))
- Support `sort` and `scatter_add` strategy ([#159022](https://github.com/pytorch/pytorch/pull/159022))
- add a util function _make_all_gather_out_tensor to reduce code duplication ([#149912](https://github.com/pytorch/pytorch/pull/149912))
- [bucketing] Rewrite all_gather, reduce_scatter passes via tracing merge_fn ([#158663](https://github.com/pytorch/pytorch/pull/158663))
- add softmax_backward_strategy missing field ([#159167](https://github.com/pytorch/pytorch/pull/159167))
- Fix SDPA sharding when `return_debug_mask` is False ([#159205](https://github.com/pytorch/pytorch/pull/159205))
- fix torch/distributed contributing doc ([#158934](https://github.com/pytorch/pytorch/pull/158934))
- Fix redistribution costs for slice_scatter ([#159223](https://github.com/pytorch/pytorch/pull/159223))
- Add a title to distributed._dist2.md ([#159385](https://github.com/pytorch/pytorch/pull/159385))
- Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead ([#156165](https://github.com/pytorch/pytorch/pull/156165))
- [BE] Fix global config leak in test_c10d_functional_native.py ([#159476](https://github.com/pytorch/pytorch/pull/159476))
- [BE] Fix buf name mismatch in test_c10d_functional_native.py ([#159487](https://github.com/pytorch/pytorch/pull/159487))
- [DTensor] Improve `sort` strategy ([#159189](https://github.com/pytorch/pytorch/pull/159189))
- [PP] Refactor test_schedule_multiproc ([#158780](https://github.com/pytorch/pytorch/pull/158780))
- Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead ([#156165](https://github.com/pytorch/pytorch/pull/156165))
- Fix inductor memory estimation when a single buf has multiple mutations. Add runtime verification of mem tracking ([#159569](https://github.com/pytorch/pytorch/pull/159569))
- [Distributed] Fix `@parametrize` on unordered iterable in distributed test ([#159793](https://github.com/pytorch/pytorch/pull/159793))
- [DTensor] Set up DTensorContinuousTestBase ([#159885](https://github.com/pytorch/pytorch/pull/159885))
- [async-TP] Make scaled-mm + reduce-scatter preserve alignment of scales ([#159957](https://github.com/pytorch/pytorch/pull/159957))
- [DTensor] support _StridedShard in view op ([#159656](https://github.com/pytorch/pytorch/pull/159656))
- [replicate][be] improved readability of test case description ([#160128](https://github.com/pytorch/pytorch/pull/160128))
- [SymmMem] Fix flaky wait_until test ([#159215](https://github.com/pytorch/pytorch/pull/159215))
- improve gather and scatter_add strategy ([#160140](https://github.com/pytorch/pytorch/pull/160140))
- rename-HAS_CUDA-to-HAS_CUDA_AND_TRITON ([#159883](https://github.com/pytorch/pytorch/pull/159883))
- Add type assert for tensor_meta, based on real bug in autoparallel. ([#157927](https://github.com/pytorch/pytorch/pull/157927))
- [FR] Add stack_id and an optional print of stack_id to stack_trace mapping ([#160119](https://github.com/pytorch/pytorch/pull/160119))
- [bucketing] Bucket only adjacent collectives to prevent reordering ([#159983](https://github.com/pytorch/pytorch/pull/159983))
- Ensure outer aliasing on DTensor matches inner aliasing ([#158954](https://github.com/pytorch/pytorch/pull/158954))
- [DTensor] Registers sharding rule for rms_norm ([#159692](https://github.com/pytorch/pytorch/pull/159692))
- [claude-code] Add top-level module doc for torch/distributed/tensor/_op_schema.py ([#157804](https://github.com/pytorch/pytorch/pull/157804))
- [1/N]Port 3  distributed/_tools test cases to Intel GPU ([#159543](https://github.com/pytorch/pytorch/pull/159543))
- [c10d] Fix test test_nccl_user_buffer_registration ([#160497](https://github.com/pytorch/pytorch/pull/160497))
- measure dispatch overhead ([#160504](https://github.com/pytorch/pytorch/pull/160504))
- [PP] Allow larger world_size schedule tests ([#160559](https://github.com/pytorch/pytorch/pull/160559))
- [DTensor] add op support: aten.squeeze_.dim ([#159532](https://github.com/pytorch/pytorch/pull/159532))
- typing distributed.py ([#160365](https://github.com/pytorch/pytorch/pull/160365))
- port 2 distributed pipeline test files for Intel GPU ([#159140](https://github.com/pytorch/pytorch/pull/159140))
- [BE][CUDA][Distributed] Add require_exact_world_size() and a few distributed unit test fixes   ([#160803](https://github.com/pytorch/pytorch/pull/160803))
- [ez] Only use default numa bindings if nproc == cuda device count ([#160848](https://github.com/pytorch/pytorch/pull/160848))
- [C10D] Fix spelling of MultiProcContinuousTest ([#160892](https://github.com/pytorch/pytorch/pull/160892))
- [C10D] Make MultiProcContinuousTest less spammy ([#160821](https://github.com/pytorch/pytorch/pull/160821))
- [ContextParallel] add Document Masking test ([#160700](https://github.com/pytorch/pytorch/pull/160700))
- Fix bucketing introducing cycles ([#160967](https://github.com/pytorch/pytorch/pull/160967))
- [bucketing] allow convert_element_type after fsdp reduce_scatter ([#161159](https://github.com/pytorch/pytorch/pull/161159))
- Use comparison key in OpSchema to avoid duplicate work between `__hash__` and `__eq__` ([#161234](https://github.com/pytorch/pytorch/pull/161234))
- Minor cleanup of DeviceMesh.__eq__ ([#161235](https://github.com/pytorch/pytorch/pull/161235))
- Typo correction in variable name inital_grad of Class TestFullyShardG… ([#161501](https://github.com/pytorch/pytorch/pull/161501))
- [C10D] add _summarize_ranks util ([#160284](https://github.com/pytorch/pytorch/pull/160284))
- [SymmMem] Isolate set_device tests to avoid hang ([#161668](https://github.com/pytorch/pytorch/pull/161668))
- [BE][SymmMEM] Change Optional to the shorthand expression for symmetric memory modules ([#161676](https://github.com/pytorch/pytorch/pull/161676))
- [DTensor] fix DTensorTestCase.destroy_pg() when device_type is "cpu" but CUDA device is available ([#161015](https://github.com/pytorch/pytorch/pull/161015))
- [SymmMEM] Fix test_empty_strided_p2p_persistent ([#161677](https://github.com/pytorch/pytorch/pull/161677))
- [SymmMEM] Move AsyncTP tests to a seperate test class ([#161820](https://github.com/pytorch/pytorch/pull/161820))
- Pass shared_ptr by value ([#161834](https://github.com/pytorch/pytorch/pull/161834))
- [SymmMem][CI] Make sure group names are consistent ([#162035](https://github.com/pytorch/pytorch/pull/162035))
- Add torch.Tensor._make_dtensor to accelerate DTensor.__new__ further ([#161590](https://github.com/pytorch/pytorch/pull/161590))
- Fix `DeviceMesh._flatten` docstring example ([#162277](https://github.com/pytorch/pytorch/pull/162277))
- [3/N] Enable 6 fsdp test on Intel GPU ([#161601](https://github.com/pytorch/pytorch/pull/161601))
- [AsyncTP] Use assertEqual instead of allClose for bf16 tests ([#162041](https://github.com/pytorch/pytorch/pull/162041))
- [bucketing] custom_ops mode to hide inductor copies overhead ([#161499](https://github.com/pytorch/pytorch/pull/161499))
### security
