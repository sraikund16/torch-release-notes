
# Release Notes worksheet skip

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please clean up and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## skip
### bc breaking
### deprecation
### new features
### improvements
### bug fixes
### performance
### docs
### devs
### Untopiced
- Revert "Simplify nvtx3 CMake handling, always use nvtx3 (19f851ce10b)
- Revert "[dynamo] Graph break on `torch.Tensor.data` assignment with mismatched dtype (1dc1eedd436)
- Enable C++ dynamic shape guards by default ([#140756](https://github.com/pytorch/pytorch/pull/140756))
- [Quant][CPU] fix fake_quantize_per_tensor_affine of inf values ([#155109](https://github.com/pytorch/pytorch/pull/155109))
- Revert "[Quant][CPU] fix fake_quantize_per_tensor_affine of inf values (029e2b05c22)
- [Quant][CPU] fix fake_quantize_per_tensor_affine of inf values ([#155109](https://github.com/pytorch/pytorch/pull/155109))
- refine fp32 precision api ([#125888](https://github.com/pytorch/pytorch/pull/125888))
- Fix silent incorrectness arising from incorrect alias information ([#152011](https://github.com/pytorch/pytorch/pull/152011))
- Revert "[cond] support gen_schema for cond (21990fbad97)
- Revert "[dynamo] fix segfault due to dangling CacheEntry backend pointer (9fe2d156a9f)
- [dynamo] add set_fullgraph decorator/context manager ([#154289](https://github.com/pytorch/pytorch/pull/154289))
- Revert "Fix silent incorrectness arising from incorrect alias information (e3977e843de)
- Revert "[dynamo] Improve error message for cond aliasing (6215e90b7b9)
- Rename torch::standalone to headeronly ([#156964](https://github.com/pytorch/pytorch/pull/156964))
- Revert "[dynamo] Better error for invalid @contextlib.contextmanager usage (56c69bedcc7)
- Revert "Rename torch::standalone to headeronly (e290a4c645e)
- Use std::string_view in torchgen ([#157050](https://github.com/pytorch/pytorch/pull/157050))
- Revert "Fix reinplace pass handling of view input + mutable custom op (4a80ddfbe70)
- [dynamo] Fix issue with tensors passed as view() shapes ([#156928](https://github.com/pytorch/pytorch/pull/156928))
- Fix silent incorrectness arising from incorrect alias information ([#152011](https://github.com/pytorch/pytorch/pull/152011))
- Revert "python definitely_contiguous-> is_contiguous_or_false (75a7d9e8684)
- Revert "Fixes for CPython int/float tests (0decd966af9)
- Revert "[dynamo] fix _torchdynamo_orig_callable naming issues (1e4c5b666af)
- Revert "[schema_upgrader] add C++ upgrader for json based upgrading (f810480dbef)
- Revert "[BE] parse CMake version from `cmake -E capabilities` instead of `cmake --version` (2eb744c08d6)
- Revert "[BE] use `pathlib.Path` instead of `os.path.*` in `setup.py` (29f76ec0f3e)
- Revert "Fixes for CPython int/float tests (da1f337bc43)
- Revert "Inductor logging + analysis of torch.profile (c038719731a)
- HF loads dcp - don't do a full deserialize on every file ([#155942](https://github.com/pytorch/pytorch/pull/155942))
- Revert "[dynamo] Fix issue with tensors passed as view() shapes (efbf07e7ea4)
- Revert "Use std::string_view in torchgen (d5e6f420942)
- Switch to standard pep517 sdist generation ([#152098](https://github.com/pytorch/pytorch/pull/152098))
- Revert "[cutlass backend][BE][ez] Make matmul layouts be row x column (d3efd732348)
- Revert "Fixes for CPython int/float tests (c202a7329ad)
- Revert "[dynamo] Add fx_graph_runnable test coverage (0bce3902691)
- Revert "HF loads dcp - don't do a full deserialize on every file (13bf2655c14)
- Revert "[xla hash update] update the pinned xla hash (534c454e77a)
- Revert "Compute contiguity symbolically to avoid dde, and introduce c++ sym_is_contiguous (1586521461c)
- Revert "Switch to standard pep517 sdist generation (023887fc5af)
- Fused RMSNorm implementation ([#153666](https://github.com/pytorch/pytorch/pull/153666))
- Revert "ci: Add ability to test images for build-triton-wheel (3a5677a380c)
- Revert "Fused RMSNorm implementation (6401d1d53d1)
- Fix full_like decomposition to preserve strides ([#144765](https://github.com/pytorch/pytorch/pull/144765))
- Revert "Inductor logging + analysis of torch.profile (6ef70edd9a9)
- Revert "[do not revert] Compute contiguity symbolically to avoid dde, and introduce c++ sym_is_contiguous (c6a27bae365)
- Revert "[dynamo][fsdp] Consistent behavior of int attributes (8c0df6fe176)
- Revert "[dynamo] Add fx_graph_runnable test coverage (d5a89178b05)
- Revert "Fix full_like decomposition to preserve strides (c553c55be76)
- Revert "[MPS] Add `shifted_chebyshev_polynomial_[tuvw]` (b6276a425f5)
- Revert "[BE] Unskip special ops (c9174a20f75)
- Revert "[build] modernize build-backend: `setuptools.build_meta:__legacy__` -> `setuptools.build_meta` (2e64e45b0b2)
- Revert "Fix is_unaligned usage of statically_known_true (f2e712ca14d)
- Revert "[WIP] Automatically load and save dynamo entries via caching_precompile (ae1094b72b7)
- Revert "[CI][MacOS] Add `VENV_PATH` to search path (30a1cc11a47)
- Deprecate DataLoader pin_memory_device param ([#146821](https://github.com/pytorch/pytorch/pull/146821))
- Revert "Cleanup leftover miniconda brew installation (76fe88fa56c)
- Revert "[DTensor][FSDP2] necessary changes to FSDP and TP to unblock EP (2e140690811)
- Revert "Introduce AcceleratorAllocatorConfig as the common class (86251eff406)
- Revert "Deprecate DataLoader pin_memory_device param (b83d8827bcd)
- [BE][Ez]: Fully type nn.utils.clip_grad ([#154801](https://github.com/pytorch/pytorch/pull/154801))
- Revert "[PT2][memory] mutation size correctness (6defd5084e3)
- Revert "[BE] always use `uv pip` if possible in `pip_init.py` for `lintrunner init` (cb711c8fa04)
- Revert "[Inductor] Fix epilogue fusion decision with 1 Triton caller as choice (dfa2649434f)
- [BE] Replace `std::runtime_error` with `TORCH_CHECK` [2/N] ([#152080](https://github.com/pytorch/pytorch/pull/152080))
- [CUDA] Use runtime driver API for cuStreamWriteValue32 ([#156097](https://github.com/pytorch/pytorch/pull/156097))
- [BE]: Reduce binary size 40% using aggressive fatbin compression. ([#157791](https://github.com/pytorch/pytorch/pull/157791))
- Revert "[BE]: Reduce binary size 40% using aggressive fatbin compression. (493bd625e25)
- Revert "[dynamo][fsdp] Consistent behavior of int attributes (e517066f413)
- Fix logdet returning finite values for singular matrices on CUDA  ([#157910](https://github.com/pytorch/pytorch/pull/157910))
- Revert "[BE] Replace `std::runtime_error` with `TORCH_CHECK` [2/N] (ecd73c58eea)
- Revert "[DTensor][FSDP2] necessary changes to FSDP and TP to unblock EP (92ee5bd9f6b)
- Revert "[CUDA] Use runtime driver API for cuStreamWriteValue32 (702a304b078)
- Revert "Fix logdet returning finite values for singular matrices on CUDA  (7444debaca7)
- [DCP][HF] [ez]Change where sharded tensors are saved ([#158069](https://github.com/pytorch/pytorch/pull/158069))
- Revert "multi-kernel matmuls based on varying hint sizes (9c189ed29a2)
- Revert "[BE][2/16] fix typos in torch/ (torch/_*/) (e15f4248ad2)
- Revert "[PT2][fusion] ban fusions with large accumulated reads (e90148c91d3)
- [Inductor] Set the default value of min_chunk_size to 512 ([#150762](https://github.com/pytorch/pytorch/pull/150762))
- Revert "Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (e8cca7bac75)
- Revert "Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (6fe7456aa1a)
- Revert "[Inductor] Set the default value of min_chunk_size to 512 (6ea91f06725)
- Revert "[simple_fsdp][inductor_collectives] rewrite reorder_collectives, sink_waits_iterative (4f36743f5ee)
- Revert "[PT2][fusion] ban fusions with large accumulated reads (26807dcf277)
- Revert "[CI] Fixes CI for CUDA Version > 12.9 (b26da7741be)
- Revert "Enable AcceleratorAllocatorConfig key check (f2ecf6145fd)
- Revert "Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (ea5f88dca62)
- Revert "Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (41971335c98)
- Revert "Introduce AcceleratorAllocatorConfig as the common class (46915b13614)
- Revert "[ROCm] logsumexp on ROCm needs scaling back to natural base. (03852ddc223)
- [cuda][cupy] Improve cupy device placement when device is provided ([#158320](https://github.com/pytorch/pytorch/pull/158320))
- Revert "Support DeepSeek-style blockwise scaling scaled-mm for fp8 on Hopper+ (9513b9d03fa)
- recovering node source from dict ([#158373](https://github.com/pytorch/pytorch/pull/158373))
- Revert "[cuda][cupy] Improve cupy device placement when device is provided (944a140e903)
- Revert "recovering node source from dict (14ecc033618)
- DDE-Free select with unbacked index. ([#157605](https://github.com/pytorch/pytorch/pull/157605))
- [BE] remove torch deploy - conditionals ([#158288](https://github.com/pytorch/pytorch/pull/158288))
- [BE] Remove __reduce_deploy__ ([#158291](https://github.com/pytorch/pytorch/pull/158291))
- Revert "[Docker builds] Move from Miniconda to Miniforge (9f37cce6933)
- Revert "Move off of deprecated API in 2.9 (288bf54a23a)
- Revert "DDE-Free select with unbacked index. (23550ab735e)
- Revert "[cuDNN][SDPA] cuDNN SDPA refactor/cleanup, nested tensor backward, test priority bump for `sm90`, `sm100` (bfe5674e229)
- Revert "Cleanup old caffe2 scripts (ced5cf042de)
- [DTensor] Fix default_strategy and rename for clarity ([#158490](https://github.com/pytorch/pytorch/pull/158490))
- Revert "Add torch compile force disable caches alias (9a7c2f1f64b)
- Revert "Forward-fix unused variables warning/error (be896d6b41f)
- Revert "Support DeepSeek-style blockwise scaling scaled-mm for fp8 on Hopper+ (32aade9d8d3)
- [DTensor] fix copy_ strategy ([#158538](https://github.com/pytorch/pytorch/pull/158538))
- Revert "[DTensor] fix copy_ strategy (50f33a6fca8)
- Revert "[DTensor] Fix default_strategy and rename for clarity (bf4aa782790)
- Revert "[ROCm][CI] update fbgemm_gpu hash used by inductor tests (86675af3f02)
- Revert "[DCP][HF] [ez]Change where sharded tensors are saved (e3351b3ddff)
- Revert "Fix test consolidate hf safetensors (3bb729df97e)
- Fused RMSNorm implementation ([#153666](https://github.com/pytorch/pytorch/pull/153666))
- [DTensor] Fix default_strategy and rename for clarity ([#158490](https://github.com/pytorch/pytorch/pull/158490))
- [DTensor] fix copy_ strategy ([#158538](https://github.com/pytorch/pytorch/pull/158538))
- Revert "[Easy] Show some clear error when torch.ops.load_library fails. (c2c88846a9c)
- Revert "[build] pin `setuptools>=77` to enable PEP 639 (5e1232871b6)
- [Inductor] Set the default value of min_chunk_size to 512 ([#150762](https://github.com/pytorch/pytorch/pull/150762))
- Revert "[CI] update flake8 and mypy lint dependencies (393377d2156)
- Revert "Fused RMSNorm implementation (35f1b4ad9ef)
- [AOTI] Convert C-struct zip handling to RAII container ([#158687](https://github.com/pytorch/pytorch/pull/158687))
- Revert "[DTensor] Assert DTensorSpec has valid placements (72db0a98a34)
- Revert "Still run TritonBundler with BundledAOTAutogradCache, save autotune results (bc379aebe2e)
- removed zero dim cpu logic from fake_tensor.py ([#147501](https://github.com/pytorch/pytorch/pull/147501))
- Revert "[AOTI] Convert C-struct zip handling to RAII container (97d7dc197f9)
- Revert "[AOTI] normalize path and process model files. (e8af168ee09)
- Revert "[AOTI] fix extract file failed on Windows. (5a56e6a72b8)
- Revert "[AOTI] windows package load dev (734826d88e5)
- Setup TorchBench in Docker ([#158613](https://github.com/pytorch/pytorch/pull/158613))
- Revert "[build] pin `setuptools>=77` to enable PEP 639 (feaa02f9add)
- Revert "[BE] Make PyObjectSlot use a global PyInterpreter and remove (15a50dcf1c9)
- Revert "[BE] Modify PyObjectSlot the assume only a single interpreter is in use (99cc3633f69)
- Revert "[BE] Remove __reduce_deploy__ (920f26c7617)
- Revert "[BE] Remove torch deploy | remove torch deploy specific files (4c18e85300e)
- Revert "[BE] remove torch deploy - conditionals (ee5a434f8ce)
- Revert "Setup TorchBench in Docker (9281625a9b5)
- Revert "Remove is_arvr_mode() from xnnpack.buck.bzl (0142d5f4e26)
- Revert "removed zero dim cpu logic from fake_tensor.py (9a28e23d979)
- [AOTI] Convert C-struct zip handling to RAII container ([#158687](https://github.com/pytorch/pytorch/pull/158687))
- Revert "[AOTI] Add more default options to compile_standalone (7d6f3402380)
- Fused RMSNorm implementation ([#153666](https://github.com/pytorch/pytorch/pull/153666))
- Revert "[Inductor] Expose decomposeK knobs as envvars (badfebf29e4)
- Fix decorators skipping NCCL tests  ([#158846](https://github.com/pytorch/pytorch/pull/158846))
- Revert "Fix Triton GEMM templates with k=1 (41b6cdaf761)
- Revert "Fix decorators skipping NCCL tests  (30b0ad5c683)
- Revert "[Precompile] Various small bugfixes, add CachingPrecompile to torchbench (76be282e3a4)
- [BE] remove torch deploy - conditionals ([#158288](https://github.com/pytorch/pytorch/pull/158288))
- [BE] Remove __reduce_deploy__ ([#158291](https://github.com/pytorch/pytorch/pull/158291))
- Revert "[Torch Native] Add test for packaging weight  (d34cee4cf3b)
- removed zero dim cpu logic from fake_tensor.py ([#147501](https://github.com/pytorch/pytorch/pull/147501))
- Revert "[CI][testing] Use 3 processes for testing on sm89 and sm90 jobs (11ea3736ddd)
- DDE-Free select with unbacked index. ([#157605](https://github.com/pytorch/pytorch/pull/157605))
- Move some of vec into headeronly in preparation for Half.h ([#158976](https://github.com/pytorch/pytorch/pull/158976))
- Revert "Remove tensorexpr tests (13398dab799)
- Graph break with error message ([#158800](https://github.com/pytorch/pytorch/pull/158800))
- Revert "[Profiler] Fix lost C call events problem in Python 3.12.0-3.12.4 (b533f121200)
- Revert "Move some of vec into headeronly in preparation for Half.h (751285cb22e)
- Revert "Graph break with error message (8d2a1d6e18a)
- Revert "[inductor] consolidate common GEMM triton param retrieval (a53db90e21f)
- Revert "Remove tensorexpr tests (9535995bbc0)
- Revert "[BE] Modify PyObjectSlot the assume only a single interpreter is in use (5620e617c9e)
- Revert "[BE] Remove __reduce_deploy__ (a9f6770edd8)
- Revert "[BE] Remove torch deploy | remove torch deploy specific files (c8316d0e79c)
- Revert "[BE] remove torch deploy - conditionals (f8fafdc7a6d)
- Revert "enable compiled autograd on CPU windows (3fb78501f07)
- Revert "[NativeRT] Apply Device placement once when loading the graph (3db8623dcba)
- Revert "Remove tensorexpr tests (f62772f3659)
- Revert "[Inductor] Support native Inductor as backend for MTIA (fe0ff12dab7)
- Revert "[Profiler] Fix lost C call events problem in Python 3.12.0-3.12.4 (1cffb217ef5)
- Revert "Setup TorchBench in Docker (d26ab281d29)
- [dynamo] [guard] Add caching for inside torch.compile.disable function to avoid unnecessary recompilation. ([#157566](https://github.com/pytorch/pytorch/pull/157566))
- Revert "[dynamo][fsdp] Consistent behavior of int attributes (14d67eec05c)
- Partitioner: Fix to align partition node order with original graph ([#157892](https://github.com/pytorch/pytorch/pull/157892))
- Revert "[CI] update flake8 and mypy lint dependencies (59e261bbd86)
- [CPU] fix _weight_int8pack_mm with large output shape ([#158341](https://github.com/pytorch/pytorch/pull/158341))
- Move some of vec into headeronly in preparation for Half.h ([#158976](https://github.com/pytorch/pytorch/pull/158976))
- Revert "[CPU] fix _weight_int8pack_mm with large output shape (61aa2ae20ff)
- [inductor] Fix mm decomposition evaluating symints ([#158998](https://github.com/pytorch/pytorch/pull/158998))
- Graph break with error message ([#158800](https://github.com/pytorch/pytorch/pull/158800))
- [BE] remove torch deploy - conditionals ([#158288](https://github.com/pytorch/pytorch/pull/158288))
- Revert "[inductor] Fix mm decomposition evaluating symints (e557b3d5e5f)
- Revert "[HOP, map] Rework of map autograd to the new interface (5d93127c87f)
- Revert "[Dynamo][Better Engineering] Add typing annotations to guard and source (d987a6f7f00)
- Revert "Partitioner: Fix to align partition node order with original graph (c0c24b61ff4)
- Revert "Remove tensorexpr tests (e288c258f7d)
- [BE] Remove __reduce_deploy__ ([#158291](https://github.com/pytorch/pytorch/pull/158291))
- Move Half to headeronly ([#159172](https://github.com/pytorch/pytorch/pull/159172))
- Revert "Move Half to headeronly (eaadd1282c8)
- Move Half to headeronly ([#159172](https://github.com/pytorch/pytorch/pull/159172))
- [inductor] Fix mm decomposition evaluating symints ([#158998](https://github.com/pytorch/pytorch/pull/158998))
- Revert "Fix max_width computation in _tensor_str._Formatter (6c6e11c206b)
- Revert "Fix rand_like decomposition to preserve strides (7ac70ac4cd7)
- [ContextParallel][FlexAttention] Prototype of supporting FlexAttention in Context Parallel ([#158692](https://github.com/pytorch/pytorch/pull/158692))
- Fix ep deepcopy when there is python builitin name ([#159478](https://github.com/pytorch/pytorch/pull/159478))
- Revert "Fix large_tensor_test skipping cpu (3e5e0946152)
- Revert "fix strategy hashing arg mismatch (c07bb277a0f)
- Revert "[ContextParallel][FlexAttention] Prototype of supporting FlexAttention in Context Parallel (8a233d6000b)
- Revert "Fix ep deepcopy when there is python builitin name (b1fb552974a)
- Revert "[inductor] Add logging for distributed collective ops for multi‑rank diagnostics (490cb3f1a4b)
- Revert "[dynamo] [guard] Add caching for inside torch.compile.disable function to avoid unnecessary recompilation. (cb4f41e125a)
- [CPU] fix _weight_int8pack_mm with large output shape ([#158341](https://github.com/pytorch/pytorch/pull/158341))
- Revert "Generalize torch._C._set_allocator_settings to be generic (cb9b74872b8)
- Revert "Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (90f13f3b2a1)
- Revert "Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (5cc6a0abc13)
- [ContextParallel][FlexAttention] Prototype of supporting FlexAttention in Context Parallel ([#158692](https://github.com/pytorch/pytorch/pull/158692))
- Migrate ScalarType to headeronly ([#159416](https://github.com/pytorch/pytorch/pull/159416))
- Revert "Fix rand_like decomposition to preserve strides (c687446374e)
- Revert "[inductor] consolidate common GEMM triton param retrieval (acad8085457)
- Revert "[dynamo] Be consistent with storing func source for UserMethodVariable (6e8d705a22a)
- Revert "[dynamo][guards] Make class members go through obj.__class__.__dict__ (805a102beb9)
- Revert "Migrate ScalarType to headeronly (7e8197e34d5)
- Revert "Stop parsing command line arguments every time common_utils is imported. (356ac3103af)
- [inductor] allocate non-blocking copy destinations in pinned memory (#155121) ([#158758](https://github.com/pytorch/pytorch/pull/158758))
- [inductor] move all cpu scalars using pinned memory for graph partition (#155360) ([#158983](https://github.com/pytorch/pytorch/pull/158983))
- Revert "[inductor] move all cpu scalars using pinned memory for graph partition (#155360) (1fad16aacb3)
- Revert "[inductor] allocate non-blocking copy destinations in pinned memory (#155121) (83ba3f11017)
- Revert "[mps] Turn on inductor dynamic shapes tests (fb8f32ef52e)
- Revert "[Inductor][Triton] Support TMA before strict 3.4 cutoff (410812763bd)
- Revert "Set PYTHONHOME for inductor subprocesses using torch (2457e62c90a)
- Replace C array with std::array in formatSockAddr ([#159812](https://github.com/pytorch/pytorch/pull/159812))
- Revert "Replace C array with std::array in formatSockAddr (2855688a1db)
- Revert "[dynamo] Be consistent with storing func source for UserMethodVariable (ba37f589d49)
- Partitioner: Fix to align partition node order with original graph ([#157892](https://github.com/pytorch/pytorch/pull/157892))
- Revert "[pytorch] Moving torch.compile worker process logs to a dedicated rank based log directory (cb4b29b754b)
- Revert "unskipped mobilenet_v3 quantization and mobilenet_v2 quantization plus tests from https://github.com/pytorch/pytorch/issues/125438 (a53d14d5f84)
- Revert "Add UT for torch.accelerator memory-related API (c4e64467b5a)
- [inductor] allocate non-blocking copy destinations in pinned memory (#155121) ([#158758](https://github.com/pytorch/pytorch/pull/158758))
- [inductor] move all cpu scalars using pinned memory for graph partition (#155360) ([#158983](https://github.com/pytorch/pytorch/pull/158983))
- Revert "dynamo: Remove passing or deleted dynamo_expected_failures (195b5c2e27e)
- Revert "[pytorch][dynamo_compile] Log stack_trace to dynamo_compile (206c1eef657)
- Make user defined Triton kernels serializable for fx_graph_runnable ([#160002](https://github.com/pytorch/pytorch/pull/160002))
- Revert "Make user defined Triton kernels serializable for fx_graph_runnable (2f4c2226175)
- Revert "Fix get_free_symbol_uses for several nodes. (d3d359dbafa)
- Revert "[inductor] turn on windows inductor UTs (7ae0629d64b)
- Revert "[inductor] turn on windows inductor UTs (b602ea9cab7)
- [triton_heuristics] Optimize the triton launcher in pt2 ([#160000](https://github.com/pytorch/pytorch/pull/160000))
- Revert "[Graph Partition] Pass all OSS unit tests (09381f5dacd)
- Revert "port distributed pipeline test files for Intel GPU (b149c7204c2)
- Make user defined Triton kernels serializable for fx_graph_runnable ([#160002](https://github.com/pytorch/pytorch/pull/160002))
- Add `label_smoothing` param in `nn.BCELoss` and `nn.BCEWithLogitsLoss` ([#150282](https://github.com/pytorch/pytorch/pull/150282))
- Revert "Update triton xpu commit to support python 3.14 (b219ca2a00a)
- Revert "[triton_heuristics] Optimize the triton launcher in pt2 (f7b2f3314cf)
- [ROCm] Support large inputs for coalesceValuesKernel ([#158281](https://github.com/pytorch/pytorch/pull/158281))
- Revert "[ROCm] Support large inputs for coalesceValuesKernel (f341077ce47)
- [cuDNN][64-bit indexing] update conv depthwise 64bit indexing dispatch condition to match native kernel ([#156140](https://github.com/pytorch/pytorch/pull/156140))
- Enable output padding when only outermost dim is dynamic ([#159404](https://github.com/pytorch/pytorch/pull/159404))
- Revert "[Fix XPU CI][Inductor UT] Fix test cases broken by community. (652a6f5954d)
- Revert "Add `label_smoothing` param in `nn.BCELoss` and `nn.BCEWithLogitsLoss` (641ee747815)
- Revert "Factor out the strings to templates for better editor integration (c6563341208)
- Revert "[BE][Dynamo] Type improvements in `_dynamo/utils` to generics (33d94018668)
- [cutlass backend] re-add pip cutlass path ([#160180](https://github.com/pytorch/pytorch/pull/160180))
- [ROCm] Support large inputs for coalesceValuesKernel ([#158281](https://github.com/pytorch/pytorch/pull/158281))
- Revert "appending the pythonpath (eac2d9d695a)
- [triton_heuristics] Optimize the triton launcher in pt2 ([#160000](https://github.com/pytorch/pytorch/pull/160000))
- Revert "[cutlass] fix dictionary iteration error (3650989e6e1)
- Revert "[Inductor] addmm + activation function fusion (846963fa9bd)
- Revert "[inductor] dont reuse buffers if it affects peak (#145883) (9df07ecfbeb)
- Revert "[cutlass backend] re-add pip cutlass path (30d2f98daa9)
- Revert "[BE] Update nvshem dependency to 3.3.20 (c015e53d37b)
- Remove guard_size_oblivious from default contiguity python check, and add aten.sym_is_contiguous. ([#159197](https://github.com/pytorch/pytorch/pull/159197))
- Revert "[inductor] TLParse tensor metadata logging + test (26297c27e25)
- Revert "[BE] [Inductor] Re-Land Support TMA before strict 3.4 cutoff (04c7be903d0)
- Revert "Use numpy 1.26.2 for Python 3.9 and 3.10 (3ced4f1e6cb)
- Revert "Remove guard_size_oblivious from default contiguity python check, and add aten.sym_is_contiguous. (b82aa3df20b)
- Revert "[ONNX] Default to dynamo export (82c7a1eb4b7)
- [dynamic shapes] unbacked-safe slicing ([#157944](https://github.com/pytorch/pytorch/pull/157944))
- Revert "[dynamic shapes] unbacked-safe slicing (5e98d9f9bab)
- [FSDP][Collectives] skipping reduce_scatter when world size is 1 ([#160136](https://github.com/pytorch/pytorch/pull/160136))
- [dynamo] [guard] Add caching for inside torch.compile.disable function to avoid unnecessary recompilation. ([#160934](https://github.com/pytorch/pytorch/pull/160934))
- Revert "Use numpy 1.26.2 for Python 3.9 and 3.10 (e3ebf364e6d)
- Revert "Recheck Autotune cache on Precompile serialization to prune compilation results (eddaaa6c2a6)
- [dynamic shapes] unbacked-safe slicing ([#157944](https://github.com/pytorch/pytorch/pull/157944))
- [SymmMem] Support rendezvous on slice of a tensor ([#160825](https://github.com/pytorch/pytorch/pull/160825))
- Revert "[WIP] Merge Test (eba20d2d748)
- handling special case for pow(3) for GPU ([#157537](https://github.com/pytorch/pytorch/pull/157537))
- [dynamic shapes] use prims_common contiguity in create_example_tensors ([#160933](https://github.com/pytorch/pytorch/pull/160933))
- Revert "handling special case for pow(3) for GPU (e83825f91cb)
- [SymmMem] Support rendezvous on view of a tensor ([#160925](https://github.com/pytorch/pytorch/pull/160925))
- [rfc] add hint_override kwarg to mark_dynamic ([#161007](https://github.com/pytorch/pytorch/pull/161007))
- Revert "[dynamic shapes] unbacked-safe slicing (6ea4be1e2ec)
- Revert "[rfc] add hint_override kwarg to mark_dynamic (90ea9ccefe3)
- [ATen][CPU][Sparse] Use Third-Party Eigen for sparse add and addmm ([#155357](https://github.com/pytorch/pytorch/pull/155357))
- Close some sources of fake tensor leakages  ([#159923](https://github.com/pytorch/pytorch/pull/159923))
- [dynamic shapes] unbacked-safe slicing ([#157944](https://github.com/pytorch/pytorch/pull/157944))
- Revert "[FSDP][Replicate] replicate tests for param registration and input device movements (6b5be1f4a0a)
- Revert "[FSDP][Collectives] skipping reduce_scatter when world size is 1 (f9875166a95)
- [rfc] add hint_override kwarg to mark_dynamic ([#161007](https://github.com/pytorch/pytorch/pull/161007))
- Revert "[inductor] Estimate peak memory allocfree and applying to reordering collectives (bd5857a1d6d)
- Revert "Fix torchaudio build when TORCH_CUDA_ARCH_LIST is not set (acb00d3ccf5)
- [ROCm] SDPA fix mem fault when dropout is enabled ([#154864](https://github.com/pytorch/pytorch/pull/154864))
- Revert "[inductor] Estimate peak memory allocfree and applying to reordering collectives (7006fd0c887)
- Revert "flip the list-as-tuple behavior for short lists (a6401cb5aa5)
- [DTensor] Make default RNG semantics match user-passed generator ([#160482](https://github.com/pytorch/pytorch/pull/160482))
- Revert "[ATen][CPU][Sparse] Use Third-Party Eigen for sparse add and addmm (fc0683b1e75)
- Revert "cd: Add no-cache for test binaries (639b8cc51dd)
- Revert "[DTensor] Make default RNG semantics match user-passed generator (c7a77470c54)
- Revert "[Inductor] Update Outer Reduction Heuristic (1d458e29475)
- Revert "[SymmMem] Support rendezvous on view of a tensor (eba1ad09e47)
- Revert "[BE][inductor] tl.dot(..., allow_tf32=...) -> tl.dot(..., input_precision=...) (2c0650a00a0)
- Revert "Close some sources of fake tensor leakages  (981ac533c6e)
- Revert "[dynamic shapes] unbacked-safe slicing (3f1a97a99ca)
- Revert "[SymmMem] Support rendezvous on slice of a tensor (47d267364ca)
- Revert "Enable output padding when only outermost dim is dynamic (710514a2a51)
- Revert "Move non inductor workflows to Python 3.9 -> 3.10 (f912c93344c)
- [ATen][CPU][Sparse] Use Third-Party Eigen for sparse add and addmm ([#155357](https://github.com/pytorch/pytorch/pull/155357))
- [DTensor] Make default RNG semantics match user-passed generator ([#160482](https://github.com/pytorch/pytorch/pull/160482))
- Revert "[BE] Remove intel-openmp dependency in setup.py (1eccfb157ab)
- Revert "[inductor] Windows inductor use intel-openmp. (ab7787fb82d)
- Revert "[AMD] Fix AMD User Defined Kernel Autotune (40c0e700a48)
- Revert "Fix conv exhaustive autotuning and expand Exhaustive test coverage (df571ae7ad7)
- Revert "[dynamo] Refactor convert_frame.compile_frame to be self contained function. [5/n] (3e210f90c2c)
- Update pybind11 submodule to 3.0.1 ([#160754](https://github.com/pytorch/pytorch/pull/160754))
- [cpp_wrapper] Swap to new PyBind11 simple GIL header ([#161063](https://github.com/pytorch/pytorch/pull/161063))
- Revert "Increase timeout value when pushing to ghcr.io (908b0ccb1f7)
- Revert "[Inductor] Prune configs that require more shared memory than the hardware limit (92ab1848245)
- Revert "[inductor] structured-log graph execution order + test (4a1aca11c20)
- Revert "[dynamo] Refactor convert_frame.compile_frame to be self contained function. [5/n] (e795450a35b)
- Revert "Ensure large tensor int32 -> int64 indexing is enabled (818ba434c7d)
- Revert "[dynamo, nested graph breaks] prevent excessive recompilations (46576f5a164)
- Revert "[dynamo, nested graph breaks] clean up comments and codegen (caf98fde0d5)
- Revert "[ROCm] SDPA fix mem fault when dropout is enabled (9f6e1b8730d)
- Revert "[dynamo, nested graph breaks] support nested closures (a7aa480e55e)
- Revert "[dynamo, nested graph breaks] support nested graph breaks x context managers (6afd7664016)
- Revert "[dynamo, nested graph breaks] support very simple nested graph breaks (a4fb65701b1)
- Revert "[dynamo, nested graph breaks] add nested graph break tests (6686974ddd7)
- [Inductor] Add DeviceAssert op to enable device-side assertion in torch.compile ([#160677](https://github.com/pytorch/pytorch/pull/160677))
- Revert "[Inductor] Update Outer Reduction Heuristic (4e630f0629d)
- Revert "[cpp_wrapper] Swap to new PyBind11 simple GIL header (1ce423274dd)
- Revert "Update pybind11 submodule to 3.0.1 (1b34e044853)
- [2/N][SymmMem] Add MemPool allocator and tests ([#161471](https://github.com/pytorch/pytorch/pull/161471))
- [3/N][SymmMem] Expose offset field from handle ([#161532](https://github.com/pytorch/pytorch/pull/161532))
- Revert "[Inductor] Add DeviceAssert op to enable device-side assertion in torch.compile (de585058904)
- [Inductor] Add DeviceAssert op to enable device-side assertion in torch.compile ([#160677](https://github.com/pytorch/pytorch/pull/160677))
- Revert "Fix index_add for int64 input + zerodim index (28af843ee0e)
- Revert "Back out "Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (#150312)" (69c7b16e6fd)
- Update pybind11 submodule to 3.0.1 ([#160754](https://github.com/pytorch/pytorch/pull/160754))
- [cpp_wrapper] Swap to new PyBind11 simple GIL header ([#161063](https://github.com/pytorch/pytorch/pull/161063))
- Revert "Updates to CuTe DSL template renderer (38ed57d4465)
- Revert "Add inductor backend to device interface; make minifier_tests more device agnostic (014b98dd09b)
- Revert "[3/N][SymmMem] Expose offset field from handle (8fc2467fe5e)
- Revert "[2/N][SymmMem] Add MemPool allocator and tests (903181bb6f1)
- Revert "[Inductor] Add DeviceAssert op to enable device-side assertion in torch.compile (c55bdb26e19)
- Support Triton kernels in SAC region ([#161541](https://github.com/pytorch/pytorch/pull/161541))
- [2/N][SymmMem] Add MemPool allocator and tests ([#161471](https://github.com/pytorch/pytorch/pull/161471))
- [3/N][SymmMem] Expose offset field from handle ([#161532](https://github.com/pytorch/pytorch/pull/161532))
- [4/N][SymmMem] Add `get_remote_tensor` + move up `get_buffer` and `get_signal_pad` ([#161533](https://github.com/pytorch/pytorch/pull/161533))
- Revert "[dynamic shapes] use prims_common contiguity in create_example_tensors (fa762566030)
- kill allow_complex_guards_as_runtime_asserts ([#160198](https://github.com/pytorch/pytorch/pull/160198))
- [RELAND] Close some sources of fake tensor leakage ([#161589](https://github.com/pytorch/pytorch/pull/161589))
- Revert "Support Triton kernels in SAC region (e9975f501cf)
- Revert "Remove test since it ooms on CI (5432966253c)
- Revert "Ensure large tensor int32 -> int64 indexing is enabled (ef0483d74c2)
- Revert "Add test coverage to tf32 in max autotune mm configs (05d0f11dbdb)
- Revert "kill allow_complex_guards_as_runtime_asserts (a8270dd1248)
- Revert "[dynamo] [guard] Add caching for inside torch.compile.disable function to avoid unnecessary recompilation. (049c08eda8b)
- [Inductor] Add DeviceAssert op to enable device-side assertion in torch.compile ([#160677](https://github.com/pytorch/pytorch/pull/160677))
- kill allow_complex_guards_as_runtime_asserts ([#160198](https://github.com/pytorch/pytorch/pull/160198))
- Use vectorized stores for all dtypes ([#161649](https://github.com/pytorch/pytorch/pull/161649))
- Revert "Add ciflow/vllm to vLLM commit hash update PR(s) (f46e4bcf43e)
- Support Triton kernels in SAC region ([#161541](https://github.com/pytorch/pytorch/pull/161541))
- Revert "kill allow_complex_guards_as_runtime_asserts (47742081c99)
- Revert "[RELAND] Close some sources of fake tensor leakage (9b67d8e3449)
- Revert "Cleanup stale submodule directories in checkout action (823a329984c)
- [MPS] sparse add unary funcs + add for sparse tensors ([#160839](https://github.com/pytorch/pytorch/pull/160839))
- Revert "[MPS] sparse add unary funcs + add for sparse tensors (f6368e934e6)
- Revert "[CI] Migrate XPU build and test to python 3.10 (6e548c1a879)
- Revert "Cleanup stale submodule directories after checkout (6db872fa2c7)
- [MPS] sparse add unary funcs + add for sparse tensors ([#160839](https://github.com/pytorch/pytorch/pull/160839))
- Revert "Use vectorized stores for all dtypes (e015de19695)
- [Fix XPU CI][Inductor UT] Fix test cases broken by community. ([#161142](https://github.com/pytorch/pytorch/pull/161142))
- Revert "[4/N][SymmMem] Add `get_remote_tensor` + move up `get_buffer` and `get_signal_pad` (684ae48c160)
- Revert "[3/N][SymmMem] Expose offset field from handle (2e1345a0f84)
- Revert "[2/N][SymmMem] Add MemPool allocator and tests (fb2d5ea697a)
- Use vectorized stores for all dtypes in cat ([#161649](https://github.com/pytorch/pytorch/pull/161649))
- [2/N][SymmMem] Add MemPool allocator and tests ([#161471](https://github.com/pytorch/pytorch/pull/161471))
- [3/N][SymmMem] Expose offset field from handle ([#161532](https://github.com/pytorch/pytorch/pull/161532))
- Add __init__.pyi to torch/linalg ([#160750](https://github.com/pytorch/pytorch/pull/160750))
- [4/N][SymmMem] Add `get_remote_tensor` + move up `get_buffer` and `get_signal_pad` ([#161533](https://github.com/pytorch/pytorch/pull/161533))
- Revert "[cuBLASLt][FP8] `cuBLASLt` appears to support float8 rowwise-scaling on H100 (21fae99c180)
- Revert "[CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator (63a9c23fe99)
- Revert "[Fix XPU CI][Inductor UT] Fix test cases broken by community. (54e275e0d81)
- Revert "Fix conv exhaustive autotuning and expand Exhaustive test coverage (17fa8eec4a1)
- Revert "Defer loading hipify until it is needed (13b65196db4)
- Make distributed modules importable even when backend not built ([#159889](https://github.com/pytorch/pytorch/pull/159889))
- Revert "[BE] Update xpu driver repo for CD used almalinux 8.10 (e304ea4e69d)
- [CUDAGraph] add config to error on skipping cudagraph ([#161862](https://github.com/pytorch/pytorch/pull/161862))
- Revert "Add __init__.pyi to torch/linalg (d6b74568e2c)
- Add inductor provenance mapping for cpp extern kernel ([#161656](https://github.com/pytorch/pytorch/pull/161656))
- Revert "[HOTFIX] Disable DISTRIBUTED_C10D_DIRECT_ACCESS for now (82f63c8f6de)
- Revert "Make distributed modules importable even when backend not built (420c52ecf36)
- Revert "Always build USE_DISTRIBUTED. (4e42aa8ffc4)
- Revert "Add inductor provenance mapping for cpp extern kernel (15c77a8cfd3)
- Revert "Update Kineto submodule (4cdaf8265d8)
- Make distributed modules importable even when backend not built ([#159889](https://github.com/pytorch/pytorch/pull/159889))
- Revert "test: ensure editable cached wrapper is respected (0cd6c56bdfa)
- Revert "[CUDAGraph] add config to error on skipping cudagraph (f27985b7e79)
- Revert "[inductor][ez] add hook for heuristics to adjust kernel input nodes (bb950284c7e)
- [SymmMem] Add root argument to broadcast op ([#161090](https://github.com/pytorch/pytorch/pull/161090))
- Rename propagate_tensor_meta to make private again ([#161744](https://github.com/pytorch/pytorch/pull/161744))
- Revert "Contiguous subgraph decomposition (aad96a20224)
- [2/N]Port several test files under test/distributed to Intel GPU ([#159473](https://github.com/pytorch/pytorch/pull/159473))
- Revert "Make distributed modules importable even when backend not built (34aa78274d6)
- Revert "Always build USE_DISTRIBUTED. (b7dad7dd494)
- [CUDAGraph] add config to error on skipping cudagraph ([#161862](https://github.com/pytorch/pytorch/pull/161862))
- Revert "[ROCm] Use MI325 (gfx942) runners for binary smoke testing (6b8b3ac4403)
- Fix usage of forwarding references ([#161094](https://github.com/pytorch/pytorch/pull/161094))
- [ROCm] [CK] Composable Kernel integration for inductor backend ([#158747](https://github.com/pytorch/pytorch/pull/158747))
- Revert "[MPS] enable cat op for sparse (9e5247f51d8)
- Revert "[BE] Cleanup stale comments/copy from `gemm`  (afa6e5604d7)
- Revert "[BLAS] Avoid downcasts for fp16fp16->fp32 BLAS (c3d54dea9fe)
- Revert "[nativert] triton runtime implementation (95ee0bfea99)
- Make distributed modules importable even when backend not built ([#159889](https://github.com/pytorch/pytorch/pull/159889))
- Revert "Fix usage of forwarding references (48bedd753da)
- Revert "[SymmMem] Add root argument to broadcast op (d5b38410b5b)
- [SymmMem] Add root argument to broadcast op ([#161090](https://github.com/pytorch/pytorch/pull/161090))
- Revert "Fix Arm64 OSS pytorch build with FBGEMM (1ec2c15914d)
- Revert "Rename propagate_tensor_meta to make private again (f3cebec39eb)
- Revert "[ROCm] [CK] Composable Kernel integration for inductor backend (d711f27845a)
- Revert "Make distributed modules importable even when backend not built (70f865ac9bc)
- Revert "Always build USE_DISTRIBUTED. (adae7f66aac)
- Make distributed modules importable even when backend not built ([#159889](https://github.com/pytorch/pytorch/pull/159889))
- [dynamo] Graph break on on user-defined class in compiled region ([#161670](https://github.com/pytorch/pytorch/pull/161670))
- Revert "Resize to 0 if not going to be used (a3e54660027)
- Revert "[dynamo] Graph break on on user-defined class in compiled region (0ff8eabf138)
- Revert "[inductor] fuse for scalar shared data (eac3d6f04cf)
- Revert "Add return-max-scores to flex-attention (104f2680e03)
- Revert "[inductor] pdl inductor option (disabled by default) (ada43ed39c8)
- Revert " [while_loop][autograd] support autograd_key of while_loop (7a83cf430e9)
- Revert "[inductor][ez] V.choices.get_mm_configs returns list of ChoiceCallers (4348db0b92c)
- Revert "[inductor] add kernel template choice (ktc) (093ab5f4775)
- Revert "[BE] Cleanup stale comments/copy from `gemm`  (df59c217680)
- Revert "[1/N] Port 5 _composable/fsdp distributed test cases to Intel GPU (e246a85b768)
- Revert "[ROCm] Enabling several UTs (8235c4f65d8)
- Revert "[2/N]Port several test files under test/distributed to Intel GPU (ff2de5d5223)
- Revert "Make distributed modules importable even when backend not built (29e09a65450)
- Revert "Always build USE_DISTRIBUTED. (1e0656f063c)
- Revert "[audio hash update] update the pinned audio hash (53297f6ad08)
- Revert "Use vectorized stores for all dtypes in cat (a92773eeb1f)
- Make distributed modules importable even when backend not built ([#159889](https://github.com/pytorch/pytorch/pull/159889))
- Revert "[associative_scan] Autograd separated (5d819f3fafe)
- Revert "Modify ROCm MI2xx-based workflows to run on cron schedule (dd44faa9d91)
- Revert "[dynamo] Constant fold torch.autograd._profiler_enabled (ed77e23b680)
- Revert "Add BundledAOTAutogradSerializableCallable (1641606aa4d)
- Revert "testing infra and some fixes (60d009267e5)
### not user facing
- Add CPython generator/contextlib tests ([#150796](https://github.com/pytorch/pytorch/pull/150796))
- [nativert] Move call_torchbind_kernel ([#156571](https://github.com/pytorch/pytorch/pull/156571))
- Fix non-bitwise type annotations for Tensor operators (see #145838) ([#146845](https://github.com/pytorch/pytorch/pull/146845))
- Skip FSDP tests if device  count is less then requested world_size value ([#155836](https://github.com/pytorch/pytorch/pull/155836))
- Add CPython exception tests ([#150789](https://github.com/pytorch/pytorch/pull/150789))
- [sigmoid] layout planner alias analyzer ([#156676](https://github.com/pytorch/pytorch/pull/156676))
- [dynamo] Graph break on `torch.Tensor.data` assignment with mismatched dtype ([#156623](https://github.com/pytorch/pytorch/pull/156623))
- [nativert] Move PrimKernelRegistry to PyTorch core ([#156506](https://github.com/pytorch/pytorch/pull/156506))
- Remove unnecessary use of c10::SmallVector from moments_utils ([#156714](https://github.com/pytorch/pytorch/pull/156714))
- [Inductor] Fix epilogue fusion decision with 1 Triton caller as choice ([#156500](https://github.com/pytorch/pytorch/pull/156500))
- [pt2][pr_time_benchmarks] Refresh instructions count after disabled test ([#156738](https://github.com/pytorch/pytorch/pull/156738))
- Move use of c10::string_view to std::string_view ([#152509](https://github.com/pytorch/pytorch/pull/152509))
- [dynamo] Graph break on `torch.Tensor.data` assignment with mismatched dtype ([#156623](https://github.com/pytorch/pytorch/pull/156623))
- Update docs for torch.device ([#156686](https://github.com/pytorch/pytorch/pull/156686))
- use more efficient implementation for broadcasted indexing in determi… ([#156744](https://github.com/pytorch/pytorch/pull/156744))
- [invoke_subgraph] make collect_meta_analysis fake prop cachable ([#156347](https://github.com/pytorch/pytorch/pull/156347))
- Update the UT of test_decompose_mm_cpu ([#154100](https://github.com/pytorch/pytorch/pull/154100))
- [nativert] Move ParallelGraphExecutor to PyTorch core ([#156751](https://github.com/pytorch/pytorch/pull/156751))
- [BE][Easy][setup] wrap over long error messages and redirect them to `stderr` in `setup.py` ([#156043](https://github.com/pytorch/pytorch/pull/156043))
- [cond] support gen_schema for cond ([#154193](https://github.com/pytorch/pytorch/pull/154193))
- [sigmoid] add layout planner to executor ([#156852](https://github.com/pytorch/pytorch/pull/156852))
- implement SR's storage group planning algorithm ([#156715](https://github.com/pytorch/pytorch/pull/156715))
- Mark TorchServe as all emeritus ([#156865](https://github.com/pytorch/pytorch/pull/156865))
- refresh expected numbers ([#156877](https://github.com/pytorch/pytorch/pull/156877))
- [BE] Add missing type for storage dict ([#156831](https://github.com/pytorch/pytorch/pull/156831))
- [OpenReg][1/N] Migrate cpp_extensions_open_device_registration to OpenReg ([#156588](https://github.com/pytorch/pytorch/pull/156588))
- [OpenReg][2/N] Migrate cpp_extensions_open_device_registration to OpenReg ([#156589](https://github.com/pytorch/pytorch/pull/156589))
- [OpenReg] Remove the unit.skip for test_serialization ([#156804](https://github.com/pytorch/pytorch/pull/156804))
- remove gso from set_storage_meta__symint ([#156525](https://github.com/pytorch/pytorch/pull/156525))
- [nativert] clean up some migration side-effects ([#156919](https://github.com/pytorch/pytorch/pull/156919))
- Convert to markdown: jit_python_reference.rst, jit_unsupported.rst, jit_utils.rst, library.rst ([#155404](https://github.com/pytorch/pytorch/pull/155404))
- [nativert] move constantfolder to libtorch ([#156918](https://github.com/pytorch/pytorch/pull/156918))
- [dynamo] control one_graph behavior additionally through config ([#154283](https://github.com/pytorch/pytorch/pull/154283))
- [dynamo] fix set_fullgraph for nested calls ([#154782](https://github.com/pytorch/pytorch/pull/154782))
- [dynamo] handle fullgraph toggle using nested torch.compile ([#155166](https://github.com/pytorch/pytorch/pull/155166))
- [dynamo] raise hard error if error is encountered while tracing resume function prologue ([#154564](https://github.com/pytorch/pytorch/pull/154564))
- update expected results ([#157010](https://github.com/pytorch/pytorch/pull/157010))
- [dynamo] Improve error message for cond aliasing ([#156963](https://github.com/pytorch/pytorch/pull/156963))
- [dynamo] fix _torchdynamo_orig_callable naming issues ([#156901](https://github.com/pytorch/pytorch/pull/156901))
- [dynamo] Better error for invalid @contextlib.contextmanager usage ([#156924](https://github.com/pytorch/pytorch/pull/156924))
- [core] Dispatch to `at::nansum_out` rather than `at::native::nansum_out` ([#156642](https://github.com/pytorch/pytorch/pull/156642))
- complex.pow(2) on GPU by replacing with complex * complex to avoid numerical instability ([#152373](https://github.com/pytorch/pytorch/pull/152373))
- [nativert] alias analyzer + layout planner/manager to pytorch core ([#156897](https://github.com/pytorch/pytorch/pull/156897))
- Fix reinplace pass handling of view input + mutable custom op ([#156729](https://github.com/pytorch/pytorch/pull/156729))
- Compute contiguity symbolically to avoid dde, and introduce c++ sym_is_contiguous ([#155590](https://github.com/pytorch/pytorch/pull/155590))
- [dynamo] Improve error message for cond aliasing ([#156963](https://github.com/pytorch/pytorch/pull/156963))
- Port three dynamo test to Intel GPU ([#156575](https://github.com/pytorch/pytorch/pull/156575))
- remove gso from Linear.cpp ([#156899](https://github.com/pytorch/pytorch/pull/156899))
- [ROCm][Windows] Fixing undefined symbol linker error after exposing MIOpen symbols ([#156479](https://github.com/pytorch/pytorch/pull/156479))
- [Kineto] Add MTIA_INSIGHT to kineto_shim ([#156853](https://github.com/pytorch/pytorch/pull/156853))
- [dynamo] Better error for invalid @contextlib.contextmanager usage ([#156924](https://github.com/pytorch/pytorch/pull/156924))
- Update nightly PyTorch version to 2.8.0->2.9.0 ([#156965](https://github.com/pytorch/pytorch/pull/156965))
- Fixes for CPython int/float tests ([#155978](https://github.com/pytorch/pytorch/pull/155978))
- [cutlass backend][BE][ez] Make matmul layouts be row x column ([#156656](https://github.com/pytorch/pytorch/pull/156656))
- Remove mentioning of TorchScript in Export doc ([#156969](https://github.com/pytorch/pytorch/pull/156969))
- [nit] fix xavier init doc ([#157100](https://github.com/pytorch/pytorch/pull/157100))
- Address richard's comments on libtorch_stable_abi note ([#156324](https://github.com/pytorch/pytorch/pull/156324))
- Fix Float16 CooperativeReduction Test Failure ([#154516](https://github.com/pytorch/pytorch/pull/154516))
- [MTIA ATen Backend] Add dispatch keys for ge.Tensor_out / ge.Scalar_out ([#156944](https://github.com/pytorch/pytorch/pull/156944))
- [MTIA ATen Backend] Add dispatch keys for  le.Tensor_out / le.Scalar_out ([#156945](https://github.com/pytorch/pytorch/pull/156945))
- [dynamo] Add fx_graph_runnable test coverage ([#157021](https://github.com/pytorch/pytorch/pull/157021))
- Stop skipping entire foreach tests, just skip the profiler portion ([#156871](https://github.com/pytorch/pytorch/pull/156871))
- [nativert] get rid of sigmoid naming ([#157134](https://github.com/pytorch/pytorch/pull/157134))
- [MTIA ATen Backend] Add dispatch keys for  ne.Tensor_out / ne.Scalar_out ([#156946](https://github.com/pytorch/pytorch/pull/156946))
- [MTIA ATen Backend] Add dispatch keys for  gt.Tensor_out / gt.Scalar_out ([#156947](https://github.com/pytorch/pytorch/pull/156947))
- [MTIA ATen Backend] Add dispatch keys for  mul.Scalar_out / mul.out ([#156948](https://github.com/pytorch/pytorch/pull/156948))
- [MTIA ATen Backend] Add dispatch key for div.out ([#156949](https://github.com/pytorch/pytorch/pull/156949))
- [MTIA ATen Backend] Add dispatch keys for fmod / abs.out / logical_not.out ([#156950](https://github.com/pytorch/pytorch/pull/156950))
- [MTIA ATen Backend] Add dispatch keys for rsub.Tensor / rsub.Scalar / sub.out ([#156951](https://github.com/pytorch/pytorch/pull/156951))
- [MTIA ATen Backend] Add dispatch keys for add.out ([#156952](https://github.com/pytorch/pytorch/pull/156952))
- [schema_upgrader] add C++ upgrader for json based upgrading ([#156761](https://github.com/pytorch/pytorch/pull/156761))
- ci: Add ability to test images for build-triton-wheel ([#156894](https://github.com/pytorch/pytorch/pull/156894))
- [nativert] Move KernelFactory to PyTorch core ([#156913](https://github.com/pytorch/pytorch/pull/156913))
- Display a warning when overwriting `CMAKE_CUDA_ARCHITECTURES` ([#156123](https://github.com/pytorch/pytorch/pull/156123))
- [BE] parse CMake version from `cmake -E capabilities` instead of `cmake --version` ([#157073](https://github.com/pytorch/pytorch/pull/157073))
- [schema_upgrader] add C++ upgrader for json based upgrading ([#156761](https://github.com/pytorch/pytorch/pull/156761))
- [ROCm] test_hip_device_count safely runs on 1 GPU systems ([#156398](https://github.com/pytorch/pytorch/pull/156398))
- [CUTLASS] [CUDA] SM100 GroupMM ([#156203](https://github.com/pytorch/pytorch/pull/156203))
- [BE] parse CMake version from `cmake -E capabilities` instead of `cmake --version` ([#157073](https://github.com/pytorch/pytorch/pull/157073))
- [BE] use `pathlib.Path` instead of `os.path.*` in `setup.py` ([#156742](https://github.com/pytorch/pytorch/pull/156742))
- Increase tolerance for test_corrcoef_cuda_int32 ([#157206](https://github.com/pytorch/pytorch/pull/157206))
- Update pr_time_benchmarks expected results ([#157214](https://github.com/pytorch/pytorch/pull/157214))
- [nativert] hook up memory planning to execution frame ([#157053](https://github.com/pytorch/pytorch/pull/157053))
- [caffe][executorch] rename to avoid shadow in irange ([#157107](https://github.com/pytorch/pytorch/pull/157107))
- [xla hash update] update the pinned xla hash ([#156584](https://github.com/pytorch/pytorch/pull/156584))
- Update slow tests ([#155448](https://github.com/pytorch/pytorch/pull/155448))
- Fixes for CPython int/float tests ([#155978](https://github.com/pytorch/pytorch/pull/155978))
- [SDPA] Fix `alloc_with_matching_layout` stride sorting ([#157145](https://github.com/pytorch/pytorch/pull/157145))
- Fixes for CPython int/float tests ([#155978](https://github.com/pytorch/pytorch/pull/155978))
- [dynamo][fsdp] Consistent behavior of int attributes ([#157262](https://github.com/pytorch/pytorch/pull/157262))
- Overload `mul_overflows` for `size_t` ([#155736](https://github.com/pytorch/pytorch/pull/155736))
- [Testing] Remove duplicate MPSInductor tests ([#157328](https://github.com/pytorch/pytorch/pull/157328))
- [ROCm] Initial AITER Integration for mha_bwd asm kernels ([#152630](https://github.com/pytorch/pytorch/pull/152630))
- Remove super spammy log ([#157157](https://github.com/pytorch/pytorch/pull/157157))
- [BE] Remove unused variable from Pooling.metal ([#157332](https://github.com/pytorch/pytorch/pull/157332))
- [BE] always use `uv pip` if possible in `pip_init.py` for `lintrunner init` ([#157199](https://github.com/pytorch/pytorch/pull/157199))
- [BE] remove duplicates in generated `torch._VF.__all__` ([#157365](https://github.com/pytorch/pytorch/pull/157365))
- [BE] add type annotations and run `mypy` on `setup.py` ([#156741](https://github.com/pytorch/pytorch/pull/156741))
- [Inductor UT][XPU] Reduce the runtime of the test case test_comprehensive_nn_functional_max_pool2d_xpu. ([#157357](https://github.com/pytorch/pytorch/pull/157357))
- remove allow-untyped-defs from torch/backends/mps/__init__.py ([#157227](https://github.com/pytorch/pytorch/pull/157227))
- [BE] Decorate LargeTensorTest with serialTests ([#157382](https://github.com/pytorch/pytorch/pull/157382))
- [BE] Replace `checkcall("chmod")` with `os.chmod` ([#157373](https://github.com/pytorch/pytorch/pull/157373))
- [invoke_subgraph][partitioner] Add meta val on run_and_save_rng ops ([#157319](https://github.com/pytorch/pytorch/pull/157319))
- [do not revert] Compute contiguity symbolically to avoid dde, and introduce c++ sym_is_contiguous ([#155590](https://github.com/pytorch/pytorch/pull/155590))
- [BE][Easy][setup] use `super().method(...)` in command subclasses in `setup.py` ([#156044](https://github.com/pytorch/pytorch/pull/156044))
- [dynamo] Add fx_graph_runnable test coverage ([#157021](https://github.com/pytorch/pytorch/pull/157021))
- allow to use bf16 as  fp32 internal precision for mkldnn conv ([#126050](https://github.com/pytorch/pytorch/pull/126050))
- allow to use bf16 as fp32 internal precision for mkldnn conv backward ([#126054](https://github.com/pytorch/pytorch/pull/126054))
- [nativert] start to move generated static dispatch kernels ([#157403](https://github.com/pytorch/pytorch/pull/157403))
- [dynamo] fix _torchdynamo_orig_callable naming issues ([#156901](https://github.com/pytorch/pytorch/pull/156901))
- Fix a div_mod bug in generic_math.h ([#157383](https://github.com/pytorch/pytorch/pull/157383))
- [BE] use `pathlib.Path` instead of `os.path.*` in `setup.py` ([#156742](https://github.com/pytorch/pytorch/pull/156742))
- Fixes for CPython int/float tests ([#155978](https://github.com/pytorch/pytorch/pull/155978))
- [nativert] create persistent value helper ([#157286](https://github.com/pytorch/pytorch/pull/157286))
- fixed a tiny typo in torch.compiler.md ([#157462](https://github.com/pytorch/pytorch/pull/157462))
- [BE] `@serialTest` decorator must be called ([#157388](https://github.com/pytorch/pytorch/pull/157388))
- [nativert] continue to move generated static dispatch kernels ([#157460](https://github.com/pytorch/pytorch/pull/157460))
- Add Release 2.8 CUDA matrix. Update Release schedule for 2.7.1 and 2.9 ([#157482](https://github.com/pytorch/pytorch/pull/157482))
- [BE] Remove extra semicolon ([#157486](https://github.com/pytorch/pytorch/pull/157486))
- [ez] Add super().setUp() in test_ops::TestFakeTensor ([#157475](https://github.com/pytorch/pytorch/pull/157475))
- [BE] Unskip special ops ([#157464](https://github.com/pytorch/pytorch/pull/157464))
- Fix CPU bitwise shifts for out-of-limit values in VSX-vec  ([#157463](https://github.com/pytorch/pytorch/pull/157463))
- Fix typo: 'intial_query_grad' → 'initial_query_grad' in test_transformers.py ([#157306](https://github.com/pytorch/pytorch/pull/157306))
- Fix typo: 'intialized' → 'initialized' in test_modules.py ([#157226](https://github.com/pytorch/pytorch/pull/157226))
- [dynamo] Add fx_graph_runnable test coverage ([#157021](https://github.com/pytorch/pytorch/pull/157021))
- [cutlass backend] fix CutlassTensor post-renaming ([#157408](https://github.com/pytorch/pytorch/pull/157408))
- Enable Half dtype for logcumsumexp_backward ([#157512](https://github.com/pytorch/pytorch/pull/157512))
- Fix typo: 'recieve' → 'receive' in comments ([#157544](https://github.com/pytorch/pytorch/pull/157544))
- Fix is_unaligned usage of statically_known_true ([#157400](https://github.com/pytorch/pytorch/pull/157400))
- [nativert] Move Executor to PyTorch core ([#157514](https://github.com/pytorch/pytorch/pull/157514))
- [torch] Add MTIA to the list of devices supporting foreach/fused kernels ([#157583](https://github.com/pytorch/pytorch/pull/157583))
- Add missing graph and memory related symbols to cuda_to_hip_mappings (#157435) ([#157573](https://github.com/pytorch/pytorch/pull/157573))
- Fix typo: 'initalized' → 'initialized' in alias analysis test ([#157628](https://github.com/pytorch/pytorch/pull/157628))
- [AOTI] Fix AOT inductor CMake build dependency order ([#157557](https://github.com/pytorch/pytorch/pull/157557))
- Fix #153942 ([#153943](https://github.com/pytorch/pytorch/pull/153943))
- Fix typo: 'paramters' → 'parameters' in ATen tunable README ([#157575](https://github.com/pytorch/pytorch/pull/157575))
- fix: correct sentence punctuation in cuDNN note ([#157623](https://github.com/pytorch/pytorch/pull/157623))
- Add isnan exit condition to special ops ([#157464](https://github.com/pytorch/pytorch/pull/157464))
- [pc] verify max autotune is in generated source code ([#157650](https://github.com/pytorch/pytorch/pull/157650))
- Fix typo: 'Intializing' → 'Initializing' in test_parametrization.py ([#157362](https://github.com/pytorch/pytorch/pull/157362))
- More testing of Python arithmetic operators between tensors and scalars (see 157266) ([#157632](https://github.com/pytorch/pytorch/pull/157632))
- [BE][4/6] fix typos in test/ (test/inductor/) ([#157638](https://github.com/pytorch/pytorch/pull/157638))
- [BE][5/6] fix typos in test/ (test/dynamo/) ([#157639](https://github.com/pytorch/pytorch/pull/157639))
- Support transpose and pack for bit8 ([#156065](https://github.com/pytorch/pytorch/pull/156065))
- [WIP] Automatically load and save dynamo entries via caching_precompile ([#155913](https://github.com/pytorch/pytorch/pull/155913))
- Update slow tests ([#157696](https://github.com/pytorch/pytorch/pull/157696))
- [xla hash update] update the pinned xla hash ([#156584](https://github.com/pytorch/pytorch/pull/156584))
- [submodule][cutlass] Update pin to b995f93 v4.0.0 ([#157376](https://github.com/pytorch/pytorch/pull/157376))
- correctly import torch.version ([#157584](https://github.com/pytorch/pytorch/pull/157584))
- Automatically load and save dynamo entries via caching_precompile ([#155913](https://github.com/pytorch/pytorch/pull/155913))
- remove allow-untyped-defs from torch/_classes.py ([#157231](https://github.com/pytorch/pytorch/pull/157231))
- remove allow-untyped-defs from torch/backends/cusparselt/__init__.py ([#157232](https://github.com/pytorch/pytorch/pull/157232))
- Fix unbound local when an error occurs before pool is initialized ([#156750](https://github.com/pytorch/pytorch/pull/156750))
- [CI][MacOS] Add `VENV_PATH` to search path ([#157749](https://github.com/pytorch/pytorch/pull/157749))
- [CPU] Fix memory access for sbgemm bf16 ([#156585](https://github.com/pytorch/pytorch/pull/156585))
- [1/N] Don't use CUDA.cmake module ([#157188](https://github.com/pytorch/pytorch/pull/157188))
- [BE][Easy] add `.editorconfig` setting for C/C++/CUDA/ObjC ([#157692](https://github.com/pytorch/pytorch/pull/157692))
- [BE][Easy] set end-of-line for `.bat` file to CRLF in `.editorconfig` ([#156032](https://github.com/pytorch/pytorch/pull/156032))
- fix storage use_count ([#157694](https://github.com/pytorch/pytorch/pull/157694))
- Enable target-determination (TD) for ROCm CI ([#156545](https://github.com/pytorch/pytorch/pull/156545))
- Mitigate some flaky tests in trunk ([#157756](https://github.com/pytorch/pytorch/pull/157756))
- [BE] add a minimal linter to check `pyproject.toml` consistency ([#156017](https://github.com/pytorch/pytorch/pull/156017))
- S390x update test marks ([#157541](https://github.com/pytorch/pytorch/pull/157541))
- Use CMake wholearchive group ([#156393](https://github.com/pytorch/pytorch/pull/156393))
- [BE] Update xpu driver repo for CD used almalinux 8.10 ([#157356](https://github.com/pytorch/pytorch/pull/157356))
- [PT2][memory] mutation size correctness ([#157562](https://github.com/pytorch/pytorch/pull/157562))
- [CI][MacOS] Add `VENV_PATH` to search path ([#157749](https://github.com/pytorch/pytorch/pull/157749))
- Simplify the base classes of `_PyFutureMeta` ([#157757](https://github.com/pytorch/pytorch/pull/157757))
- [cutlass backend][BE][ez] Make matmul layouts be row x column ([#156656](https://github.com/pytorch/pytorch/pull/156656))
- [Intel GPU] Set higher tolerance for squeezenet1_1 with bf16 ([#156920](https://github.com/pytorch/pytorch/pull/156920))
- Add legacy note to autograd.profiler doc. ([#157459](https://github.com/pytorch/pytorch/pull/157459))
- [dynamo][fsdp] Consistent behavior of int attributes ([#157262](https://github.com/pytorch/pytorch/pull/157262))
- [Break XPU][Inductor UT] Align tolerance of newly added case with cuda. ([#157702](https://github.com/pytorch/pytorch/pull/157702))
- Suppress warning when using native arch for jit loading cuda extensions. ([#156923](https://github.com/pytorch/pytorch/pull/156923))
- [BE] add a linter to check consistency for cmake minimum version in requirements ([#156961](https://github.com/pytorch/pytorch/pull/156961))
- [BE] remove commented out code in c10/ovrsource_defs.bzl ([#157856](https://github.com/pytorch/pytorch/pull/157856))
- [BE][Ez]: Update mimalloc submodule to 2.2.4 ([#157794](https://github.com/pytorch/pytorch/pull/157794))
- [ez][CI][testing] Set upload artifacts while running to default true if in CI ([#157868](https://github.com/pytorch/pytorch/pull/157868))
- [CPU Generator] Remove the unused CPUGeneratorImplStateLegacy in set_state ([#153934](https://github.com/pytorch/pytorch/pull/153934))
- Uninstall brew miniconda while running MacOS testing ([#156898](https://github.com/pytorch/pytorch/pull/156898))
- Update `is_sparse` doc to mention that it is sparse_coo specific ([#157378](https://github.com/pytorch/pytorch/pull/157378))
- [BE][Ez]: Autotype torch/profiler with ruff ANN ([#157923](https://github.com/pytorch/pytorch/pull/157923))
- [PT2][memory] mutation size correctness ([#157562](https://github.com/pytorch/pytorch/pull/157562))
- Add check `nested_tensor_from_jagged` param `jagged_dim >= 1` ([#157770](https://github.com/pytorch/pytorch/pull/157770))
- Make the name assert actually do something, and reserve some more names ([#157342](https://github.com/pytorch/pytorch/pull/157342))
- Deprecated pkg_resources and use distributions instead ([#151915](https://github.com/pytorch/pytorch/pull/151915))
- Fix MKL error: Inconsistent configuration parameters ([#154585](https://github.com/pytorch/pytorch/pull/154585))
- Implement fast exp for AVX2 and AVX512 for the flash attention  ([#151441](https://github.com/pytorch/pytorch/pull/151441))
- [Easy] Make the error message shown by THPUtils_unpackLong to be clearer ([#157886](https://github.com/pytorch/pytorch/pull/157886))
- Enable set SDPA backend by torch.nn.attention.sdpa_kernel on XPU ([#156669](https://github.com/pytorch/pytorch/pull/156669))
- RPC tutorial audit ([#157938](https://github.com/pytorch/pytorch/pull/157938))
- Fix UB in BFloat16 round_to_nearest_even ([#157942](https://github.com/pytorch/pytorch/pull/157942))
- Add deprecation hint for accelerator APIs ([#158013](https://github.com/pytorch/pytorch/pull/158013))
- [cuDNN][SDPA] Bump cuDNN frontend submodule version to 1.12.1 ([#158044](https://github.com/pytorch/pytorch/pull/158044))
- Add CPython test `test_bool` ([#157799](https://github.com/pytorch/pytorch/pull/157799))
- Add CPython test `test_operator` ([#157800](https://github.com/pytorch/pytorch/pull/157800))
- Add CPython test `test_numeric_tower` ([#157801](https://github.com/pytorch/pytorch/pull/157801))
- Add CPython test `test_with` ([#157802](https://github.com/pytorch/pytorch/pull/157802))
- Add CPython test `test_itertools` ([#156981](https://github.com/pytorch/pytorch/pull/156981))
- Enable AcceleratorAllocatorConfig key check ([#157908](https://github.com/pytorch/pytorch/pull/157908))
- Fix XPU broken CI ([#158092](https://github.com/pytorch/pytorch/pull/158092))
- port 4 dynamo test files to Intel GPU ([#157779](https://github.com/pytorch/pytorch/pull/157779))
- [cuDNN][SDPA] cuDNN SDPA refactor/cleanup, nested tensor backward, test priority bump for `sm90`, `sm100` ([#149282](https://github.com/pytorch/pytorch/pull/149282))
- Use new cuBLAS row-wise fp8 matmul for scaled-mm ([#157905](https://github.com/pytorch/pytorch/pull/157905))
- Restore fake device ([#157972](https://github.com/pytorch/pytorch/pull/157972))
- [PyTorch] Deprecate numpy serialization for MTIA ([#157884](https://github.com/pytorch/pytorch/pull/157884))
- [PyTorch Core] MTIA supports arbitrary strides ([#157883](https://github.com/pytorch/pytorch/pull/157883))
- multi-kernel matmuls based on varying hint sizes ([#156628](https://github.com/pytorch/pytorch/pull/156628))
- [CUDA] Support family-conditional compute capabilies in `TORCH_CUDA_ARCH_LIST` ([#157999](https://github.com/pytorch/pytorch/pull/157999))
- [CUDA] Allow cuDNN or flash attn in `test_activation_checkpointing` pattern match check ([#153272](https://github.com/pytorch/pytorch/pull/153272))
- Fix diagnostic message for CUDA version mismatch in cuda.cmake ([#157370](https://github.com/pytorch/pytorch/pull/157370))
- [PT2][fusion] ban fusions with large accumulated reads ([#157563](https://github.com/pytorch/pytorch/pull/157563))
- [MTIA Aten Backend] Change relu / relu_ back to use relu kernel ([#158101](https://github.com/pytorch/pytorch/pull/158101))
- #IS157973/numpy version issue ([#158036](https://github.com/pytorch/pytorch/pull/158036))
- Fix torch._numpy advanced indexing to match NumPy when indices are separated ([#157676](https://github.com/pytorch/pytorch/pull/157676))
- adding arg values and arg types to Strobelight USDT ([#155185](https://github.com/pytorch/pytorch/pull/155185))
- multi-kernel matmuls based on varying hint sizes ([#156628](https://github.com/pytorch/pytorch/pull/156628))
- remove allow-untyped-defs from torch/_higher_order_ops/run_const_graph.py ([#157847](https://github.com/pytorch/pytorch/pull/157847))
- [ROCm] logsumexp on ROCm needs scaling back to natural base. ([#156903](https://github.com/pytorch/pytorch/pull/156903))
- Issue warning with reference to user code rather than torch ([#155112](https://github.com/pytorch/pytorch/pull/155112))
- Tag CPython test files with the commit or tag they were copied from. ([#158038](https://github.com/pytorch/pytorch/pull/158038))
- Normalize placeholder names in AOTAutogradCache ([#157916](https://github.com/pytorch/pytorch/pull/157916))
- [nativert] add memory overlap debug assertion ([#157290](https://github.com/pytorch/pytorch/pull/157290))
- Fix land race typos from #157290 ([#158272](https://github.com/pytorch/pytorch/pull/158272))
- skip inductor/test_torchinductor_opinfo in windows ([#158225](https://github.com/pytorch/pytorch/pull/158225))
- [PT2][fusion] ban fusions with large accumulated reads ([#157563](https://github.com/pytorch/pytorch/pull/157563))
- [CI] Fixes CI for CUDA Version > 12.9 ([#157385](https://github.com/pytorch/pytorch/pull/157385))
- Refactor and Improve the OpenReg Module ([#158090](https://github.com/pytorch/pytorch/pull/158090))
- [cutlass backend][BE] remove force disable cache in tests ([#158053](https://github.com/pytorch/pytorch/pull/158053))
- [BE] Remove CUDA 11.8 artifacts ([#158303](https://github.com/pytorch/pytorch/pull/158303))
- [simple_fsdp][inductor_collectives] rewrite reorder_collectives, sink_waits_iterative ([#158062](https://github.com/pytorch/pytorch/pull/158062))
- Expose opt_einsum in torch.backends ([#157740](https://github.com/pytorch/pytorch/pull/157740))
- Update CODEOWNERS for dataloading ([#158348](https://github.com/pytorch/pytorch/pull/158348))
- Add test for user-managed weights with load_state_dict ([#157496](https://github.com/pytorch/pytorch/pull/157496))
- [BE][S538760] get rid of TORCH_CHECK_.* and CHECK macros ([#158269](https://github.com/pytorch/pytorch/pull/158269))
- [PT2][fusion] ban fusions with large accumulated reads ([#157563](https://github.com/pytorch/pytorch/pull/157563))
- Update warning of TF32 ([#158209](https://github.com/pytorch/pytorch/pull/158209))
- [Easy] Show some clear error when torch.ops.load_library fails. ([#157524](https://github.com/pytorch/pytorch/pull/157524))
- Fix torch._numpy to match NumPy when empty ellipsis causes advanced indexing separation ([#158297](https://github.com/pytorch/pytorch/pull/158297))
- disable multi kernel rocm ([#158299](https://github.com/pytorch/pytorch/pull/158299))
- [nativert] libtorch kernel registry ([#157150](https://github.com/pytorch/pytorch/pull/157150))
- Support DeepSeek-style blockwise scaling scaled-mm for fp8 on Hopper+ ([#158037](https://github.com/pytorch/pytorch/pull/158037))
- Fix sha256 for aotriton ROCm7.0 tarball ([#158420](https://github.com/pytorch/pytorch/pull/158420))
- [ROCm][Windows] Fix finding ROCm/HIP version ([#156486](https://github.com/pytorch/pytorch/pull/156486))
- [BE][testing] Skip test_triton_interpret internally ([#158260](https://github.com/pytorch/pytorch/pull/158260))
- [BE][testing] Skip test_repeated_masked_load internally ([#158355](https://github.com/pytorch/pytorch/pull/158355))
- [BE][testing] disable test_custom_op_square internally ([#158367](https://github.com/pytorch/pytorch/pull/158367))
- [ROCm] check stream graph capture status in memcpy_and_sync inline function ([#158165](https://github.com/pytorch/pytorch/pull/158165))
- enable compiled autograd on CPU windows ([#158432](https://github.com/pytorch/pytorch/pull/158432))
- [hop] add supports_higher_order_operators flag to TorchDispatchMode ([#158077](https://github.com/pytorch/pytorch/pull/158077))
- [cond] add _FlopCounterMode support for cond  ([#158067](https://github.com/pytorch/pytorch/pull/158067))
- Don't need to handle PyTrace_EXCEPTION in pyProfileFn ([#154392](https://github.com/pytorch/pytorch/pull/154392))
- Fix indexing with multi-dimensional boolean mask ([#158369](https://github.com/pytorch/pytorch/pull/158369))
- Move the rest of c10/macros/Export.h ([#158358](https://github.com/pytorch/pytorch/pull/158358))
- [ez][lint] Add pr_time_benchmarks to merge conflictless csv linter ([#158353](https://github.com/pytorch/pytorch/pull/158353))
- [BE][testing] Disable test_static_cuda_launcher:test_floats internally ([#158296](https://github.com/pytorch/pytorch/pull/158296))
- update expeced results ([#158497](https://github.com/pytorch/pytorch/pull/158497))
- [dynamo] Constant fold torch.autograd._profiler_enabled ([#158482](https://github.com/pytorch/pytorch/pull/158482))
- [easy][guards] Add developer comment for posterity ([#158471](https://github.com/pytorch/pytorch/pull/158471))
- [Device] Add support for PrivateUse1 device type in parse_type function ([#157609](https://github.com/pytorch/pytorch/pull/157609))
- [DTensor] Assert DTensorSpec has valid placements ([#158133](https://github.com/pytorch/pytorch/pull/158133))
- [Fix] Rework CUDA error explanation framework to be less destructive … ([#158484](https://github.com/pytorch/pytorch/pull/158484))
- Fix mask construction when dispatching index_put to masked_fill ([#158472](https://github.com/pytorch/pytorch/pull/158472))
- [BE] Modify PyObjectSlot the assume only a single interpreter is in use ([#158407](https://github.com/pytorch/pytorch/pull/158407))
- Move off of deprecated API in 2.9 ([#158527](https://github.com/pytorch/pytorch/pull/158527))
- [Docker builds] Move from Miniconda to Miniforge ([#158370](https://github.com/pytorch/pytorch/pull/158370))
- [PT2]: Skip AOTI Weight Loading during Init ([#158416](https://github.com/pytorch/pytorch/pull/158416))
- [NativeRT] Remove normalizeDevice ([#158489](https://github.com/pytorch/pytorch/pull/158489))
- Support DeepSeek-style blockwise scaling scaled-mm for fp8 on Hopper+ ([#158037](https://github.com/pytorch/pytorch/pull/158037))
- Forward-fix unused variables warning/error ([#158549](https://github.com/pytorch/pytorch/pull/158549))
- Add torch compile force disable caches alias ([#158072](https://github.com/pytorch/pytorch/pull/158072))
- [BE][testing] fix test/dynamo/test_repros:test_longtensor_list ([#158458](https://github.com/pytorch/pytorch/pull/158458))
- check if USE_ROCM is defined ([#158571](https://github.com/pytorch/pytorch/pull/158571))
- Make Inductor imports TYPE_CHECKING only ([#158524](https://github.com/pytorch/pytorch/pull/158524))
- [simple_fsdp][inductor_collectives] rewrite reorder_collectives, sink_waits_iterative ([#158062](https://github.com/pytorch/pytorch/pull/158062))
- [ca] reset between tests ([#158418](https://github.com/pytorch/pytorch/pull/158418))
- [CI][TD] Enable TD on all test configs ([#158163](https://github.com/pytorch/pytorch/pull/158163))
- Suppress volatile type error ([#158435](https://github.com/pytorch/pytorch/pull/158435))
- Add missing <vector> in c10/util/WaitCounter.h ([#158354](https://github.com/pytorch/pytorch/pull/158354))
- Avoid globally modifying torch.testing._internal.common_methods_invocations.wrapper_set_seed ([#158548](https://github.com/pytorch/pytorch/pull/158548))
- Fix test linalg for MKL upgrading ([#158312](https://github.com/pytorch/pytorch/pull/158312))
- [NativeRT] Remove makeProxyExecutor from ModelRunner interface ([#158587](https://github.com/pytorch/pytorch/pull/158587))
- [ROCm][CI] update fbgemm_gpu hash used by inductor tests ([#158602](https://github.com/pytorch/pytorch/pull/158602))
- [BE] Make PyObjectSlot use a global PyInterpreter and remove ([#158427](https://github.com/pytorch/pytorch/pull/158427))
- Add a way to disable compile for debugging flex-attention ([#158534](https://github.com/pytorch/pytorch/pull/158534))
- [hop] set capture_scalar_outputs=True by default for compiled hops ([#158480](https://github.com/pytorch/pytorch/pull/158480))
- [ATen][CUDA][SDPA] Flash Attention: Refactor sm version checks ([#158558](https://github.com/pytorch/pytorch/pull/158558))
- [iter] Update some of the tests to not call pickle ([#156369](https://github.com/pytorch/pytorch/pull/156369))
- [aot] fix greater_than_max build fail on Windows. ([#158479](https://github.com/pytorch/pytorch/pull/158479))
- [BE][testing] Fix internal test failures in test/dynamo/test_unspec ([#158485](https://github.com/pytorch/pytorch/pull/158485))
- Add lower bounds for fsspec and networkx dependencies ([#158565](https://github.com/pytorch/pytorch/pull/158565))
- [BE] Add pre-push hook for lintrunner to the PyTorch repo ([#158389](https://github.com/pytorch/pytorch/pull/158389))
- [BE][testing] Fix test_cudacodecache.py ([#158259](https://github.com/pytorch/pytorch/pull/158259))
- only fail regressions>10% on pr_time benchmarks ([#158577](https://github.com/pytorch/pytorch/pull/158577))
- GenAI Layer Benchmark ([#158536](https://github.com/pytorch/pytorch/pull/158536))
- [CI] Fixes CI for CUDA Version > 12.9 ([#157385](https://github.com/pytorch/pytorch/pull/157385))
- [nativert] benchmark util ([#158678](https://github.com/pytorch/pytorch/pull/158678))
- [DLPack] add NumPy exchange tests. ([#150216](https://github.com/pytorch/pytorch/pull/150216))
- Fix DLPack stream logic. ([#150217](https://github.com/pytorch/pytorch/pull/150217))
- don't set CUDA_MODULE_LOADING ([#158712](https://github.com/pytorch/pytorch/pull/158712))
- [CI] update flake8 and mypy lint dependencies ([#158720](https://github.com/pytorch/pytorch/pull/158720))
- De-abstract premature generalization with InductorWrapper ([#158528](https://github.com/pytorch/pytorch/pull/158528))
- Still run TritonBundler with BundledAOTAutogradCache, save autotune results ([#158048](https://github.com/pytorch/pytorch/pull/158048))
- [BE] Always use python 3.9 for pre-push hook's lintrunner ([#158693](https://github.com/pytorch/pytorch/pull/158693))
- [CMake] Move xpu flag to xpu.cmake ([#158542](https://github.com/pytorch/pytorch/pull/158542))
- [Easy] Show some clear error when torch.ops.load_library fails. ([#157524](https://github.com/pytorch/pytorch/pull/157524))
- [BE] always use `uv pip` if possible in `pip_init.py` for `lintrunner init` ([#157199](https://github.com/pytorch/pytorch/pull/157199))
- [BE][testing] fix test_cat_max_autotune_triton ([#158589](https://github.com/pytorch/pytorch/pull/158589))
- Bump requests from 2.32.2 to 2.32.4 in /tools/build/bazel ([#158006](https://github.com/pytorch/pytorch/pull/158006))
- Fix `MaskedTensor` to device ignored mask ([#151205](https://github.com/pytorch/pytorch/pull/151205))
- Remove is_arvr_mode() from xnnpack.buck.bzl ([#158682](https://github.com/pytorch/pytorch/pull/158682))
- Use cuda error code instead of error text in get_cuda_error_help ([#158688](https://github.com/pytorch/pytorch/pull/158688))
- [AOTI] fix load_pt2 split wrong model name on Windows ([#158711](https://github.com/pytorch/pytorch/pull/158711))
- [Inductor] Expose decomposeK knobs as envvars ([#158745](https://github.com/pytorch/pytorch/pull/158745))
- Remove duplicated installation for python dependencies. ([#158339](https://github.com/pytorch/pytorch/pull/158339))
- [benchmark] allow default mode for compile  ([#158792](https://github.com/pytorch/pytorch/pull/158792))
- [ROCm][CI] update fbgemm_gpu hash used by inductor tests ([#158602](https://github.com/pytorch/pytorch/pull/158602))
- [dynamo][fsdp] Consistent behavior of int attributes ([#157262](https://github.com/pytorch/pytorch/pull/157262))
- Still run TritonBundler with BundledAOTAutogradCache, save autotune results ([#158048](https://github.com/pytorch/pytorch/pull/158048))
- [hop] allow non fake inputs when check input alias and mutation ([#158798](https://github.com/pytorch/pytorch/pull/158798))
- [FP8][CUTLASS] xFail `honor_sm_carveout` on `sm100` ([#152378](https://github.com/pytorch/pytorch/pull/152378))
- Revert "We do support 3.14" ([#158856](https://github.com/pytorch/pytorch/pull/158856))
- [ROCm] logsumexp on ROCm needs scaling back to natural base. ([#156903](https://github.com/pytorch/pytorch/pull/156903))
- [tests] Reduce sizes of unnecessarily large tensors to reduce OOM flakes ([#158456](https://github.com/pytorch/pytorch/pull/158456))
- Remove top limit for cpython version and fix lint appropriately. ([#158853](https://github.com/pytorch/pytorch/pull/158853))
- Fix Triton GEMM templates with k=1 ([#158650](https://github.com/pytorch/pytorch/pull/158650))
- [MTIA Aten Backend] Migrate addcdiv.out / addcmul.out / eq.Tensor_out / eq.Scalar_out ([#158748](https://github.com/pytorch/pytorch/pull/158748))
- [MTIA Aten Backend] Migrate addmm.out / baddbmm.out / bmm.out ([#158749](https://github.com/pytorch/pytorch/pull/158749))
- [Precompile] Various small bugfixes, add CachingPrecompile to torchbench ([#158847](https://github.com/pytorch/pytorch/pull/158847))
- [Inductor] Expose decomposeK knobs as envvars ([#158745](https://github.com/pytorch/pytorch/pull/158745))
- [Torch Native] Add test for packaging weight  ([#158750](https://github.com/pytorch/pytorch/pull/158750))
- Fix Triton GEMM templates with k=1 ([#158650](https://github.com/pytorch/pytorch/pull/158650))
- Add kernel options to flex docs ([#158875](https://github.com/pytorch/pytorch/pull/158875))
- [ROCm] UT verifies a runtime error is raised if tensor.item() is captured in a cudagraph ([#158878](https://github.com/pytorch/pytorch/pull/158878))
- [Profiler] Fix lost C call events problem in Python 3.12.0-3.12.4 ([#155446](https://github.com/pytorch/pytorch/pull/155446))
- [BE] Modify PyObjectSlot the assume only a single interpreter is in use ([#158407](https://github.com/pytorch/pytorch/pull/158407))
- [cutlass backend] Change default inst level mm config number ([#158901](https://github.com/pytorch/pytorch/pull/158901))
- [BE][Easy] add missing Python 3.14 PyPI classifier ([#158904](https://github.com/pytorch/pytorch/pull/158904))
- Update distributed maintainers ([#158900](https://github.com/pytorch/pytorch/pull/158900))
- fix xnnpack build on mac ([#158881](https://github.com/pytorch/pytorch/pull/158881))
- Skip slow tests for aarch64-inductor-benchmarks ([#158842](https://github.com/pytorch/pytorch/pull/158842))
- [CI][testing] Use 3 processes for testing on sm89 and sm90 jobs ([#158691](https://github.com/pytorch/pytorch/pull/158691))
- [2/N] Remove FindPackageHandleStandardArgs.cmake ([#156559](https://github.com/pytorch/pytorch/pull/156559))
- [doc] add weifengpy to torch distributed pocs ([#158989](https://github.com/pytorch/pytorch/pull/158989))
- Remove CUDA 11 CMake code ([#156795](https://github.com/pytorch/pytorch/pull/156795))
- [NativeRT] Remove device_ member from OpKernel base class ([#158944](https://github.com/pytorch/pytorch/pull/158944))
- better testing for subclasses + compile ([#158742](https://github.com/pytorch/pytorch/pull/158742))
- [docker release] Remove build layer as not used ([#158988](https://github.com/pytorch/pytorch/pull/158988))
- Fix inductor non-stable argsort/sort test ([#146622](https://github.com/pytorch/pytorch/pull/146622))
- [Precompile] Various small bugfixes, add CachingPrecompile to torchbench ([#158847](https://github.com/pytorch/pytorch/pull/158847))
- Remove tensorexpr tests ([#158928](https://github.com/pytorch/pytorch/pull/158928))
- [Dynamo][Better Engineering] Add typing annotations to guard and source ([#158397](https://github.com/pytorch/pytorch/pull/158397))
- Add `zerotensor` design description in code ([#158837](https://github.com/pytorch/pytorch/pull/158837))
- Added Emscripten __assert_fail declaration to Macros.h ([#158580](https://github.com/pytorch/pytorch/pull/158580))
- [inductor] consolidate common GEMM triton param retrieval ([#158015](https://github.com/pytorch/pytorch/pull/158015))
- Support DeepSeek-style blockwise scaling scaled-mm for fp8 on Hopper+ ([#158037](https://github.com/pytorch/pytorch/pull/158037))
- [ROCm] Update jit_utils.cpp for compatibility with ROCm7.0 ([#158868](https://github.com/pytorch/pytorch/pull/158868))
- [AOTI] skip crashed case on Windows temporary. ([#158929](https://github.com/pytorch/pytorch/pull/158929))
- Add tests for torch.ops.load_library ([#158838](https://github.com/pytorch/pytorch/pull/158838))
- Using the latest torch.library.register_fake API instead of torch.library.impl_abstract ([#158839](https://github.com/pytorch/pytorch/pull/158839))
- [OpenReg] Improve README.md and optimize some codes for OpenReg ([#158415](https://github.com/pytorch/pytorch/pull/158415))
- [OpenReg] add pyproject.toml for openreg ([#158440](https://github.com/pytorch/pytorch/pull/158440))
- Add missing optional for tensor ops ([#159028](https://github.com/pytorch/pytorch/pull/159028))
- only call re-plan if historic max's were updated. ([#159016](https://github.com/pytorch/pytorch/pull/159016))
- Aten vector default constructors set to 0, add fnmadd and fnmsub ([#158508](https://github.com/pytorch/pytorch/pull/158508))
- Remove tensorexpr tests ([#158928](https://github.com/pytorch/pytorch/pull/158928))
- [CI][testing] Use 3 processes for testing on sm89 and sm90 jobs ([#158691](https://github.com/pytorch/pytorch/pull/158691))
- Use simple_wraps instead of functools.wraps in AOTAutograd ([#158734](https://github.com/pytorch/pytorch/pull/158734))
- Remove torch.functional entries from the doc ignore list ([#158581](https://github.com/pytorch/pytorch/pull/158581))
- Add host protoc script back ([#159157](https://github.com/pytorch/pytorch/pull/159157))
- Fix: Use memory_order_relaxed instead of memory_order_relaxed ([#159105](https://github.com/pytorch/pytorch/pull/159105))
- Add torchcheck for replication_pad3d_backward ([#151986](https://github.com/pytorch/pytorch/pull/151986))
- [NativeRT] Apply Device placement once when loading the graph ([#158996](https://github.com/pytorch/pytorch/pull/158996))
- [nativert] make per-node benchmark work with memory planning ([#159117](https://github.com/pytorch/pytorch/pull/159117))
- [torchbind] support register_autocast for  torchbind custom op  ([#158583](https://github.com/pytorch/pytorch/pull/158583))
- [test][torchbind] turn on inductor backend for compile torchbind tests ([#158606](https://github.com/pytorch/pytorch/pull/158606))
- [torchbind] fix fakifying a staitc tensor returns dynamic accidentally ([#158607](https://github.com/pytorch/pytorch/pull/158607))
- [test][torchbind] don't allow set torchbind attr at runtime ([#158608](https://github.com/pytorch/pytorch/pull/158608))
- [HOP, map] Rework of map autograd to the new interface ([#153343](https://github.com/pytorch/pytorch/pull/153343))
- [Profiler] Fix lost C call events problem in Python 3.12.0-3.12.4 ([#155446](https://github.com/pytorch/pytorch/pull/155446))
- [BE] Use .md instead of .rst for nn.aliases doc ([#158666](https://github.com/pytorch/pytorch/pull/158666))
- Add aot_autograd.fx_utils ([#159005](https://github.com/pytorch/pytorch/pull/159005))
- Scaled MM Fix NVfp4 ([#159170](https://github.com/pytorch/pytorch/pull/159170))
- Throw invalid_argument instead of RuntimeError when parameters exceed… ([#158267](https://github.com/pytorch/pytorch/pull/158267))
- [conv][cuDNN][64-bit indexing] reduce memory usage of depthwise conv 64-bit indexing test ([#158981](https://github.com/pytorch/pytorch/pull/158981))
- Remove tensorexpr tests ([#158928](https://github.com/pytorch/pytorch/pull/158928))
- [Inductor] Support native Inductor as backend for MTIA ([#158526](https://github.com/pytorch/pytorch/pull/158526))
- [CI] update flake8 and mypy lint dependencies ([#158720](https://github.com/pytorch/pytorch/pull/158720))
- Remove tensorexpr tests ([#158928](https://github.com/pytorch/pytorch/pull/158928))
- Fix atleast_{1,2,3}d() with no arguments description ([#156042](https://github.com/pytorch/pytorch/pull/156042))
- Reland D78841818 ([#159216](https://github.com/pytorch/pytorch/pull/159216))
- [nativert][ez] Remove unused dist collectives ops. ([#159220](https://github.com/pytorch/pytorch/pull/159220))
- [nativert] ensure planner once flag is class-local, not static. ([#159116](https://github.com/pytorch/pytorch/pull/159116))
- [benchmarks] Set model name early to keep warmup and main model same ([#159231](https://github.com/pytorch/pytorch/pull/159231))
- Add CPython tests for `collections` module ([#158950](https://github.com/pytorch/pytorch/pull/158950))
- [NativeRT] Strengthen matcher check for StaticDispatch kernel ([#159187](https://github.com/pytorch/pytorch/pull/159187))
- Fix flaky test_inductor_multiple_specializations ([#159264](https://github.com/pytorch/pytorch/pull/159264))
- unbacked handling for view_copy ([#159244](https://github.com/pytorch/pytorch/pull/159244))
- [CI] update flake8 and mypy lint dependencies ([#158720](https://github.com/pytorch/pytorch/pull/158720))
- Help fix numpy detection in cross compiled layouts ([#137084](https://github.com/pytorch/pytorch/pull/137084))
- [NativeRT] Clean up use of TargetDevice in KernelFactory ([#159298](https://github.com/pytorch/pytorch/pull/159298))
- Fix rand_like decomposition to preserve strides ([#159294](https://github.com/pytorch/pytorch/pull/159294))
- [BE] Move _freeze.py to torch/fb/utils ([#159307](https://github.com/pytorch/pytorch/pull/159307))
- [2/N][CI] Remove MacOS-13 workarounds from tests ([#159304](https://github.com/pytorch/pytorch/pull/159304))
- [CUDA][Convolution] Add `tf32_on_and_off` decorator to `test_deconv_freezing_cuda` ([#159280](https://github.com/pytorch/pytorch/pull/159280))
- [BE] Modify PyObjectSlot the assume only a single interpreter is in use ([#158407](https://github.com/pytorch/pytorch/pull/158407))
- Remove unused paramter on CUDA AllocParams ([#159159](https://github.com/pytorch/pytorch/pull/159159))
- [AOTI] disable crashed AOTI UTs on Windows. ([#159427](https://github.com/pytorch/pytorch/pull/159427))
- Enable AcceleratorAllocatorConfig key check ([#157908](https://github.com/pytorch/pytorch/pull/157908))
- Generalize torch._C._set_allocator_settings to be generic ([#156175](https://github.com/pytorch/pytorch/pull/156175))
- [Profiler] Fix lost C call events problem in Python 3.12.0-3.12.4 ([#155446](https://github.com/pytorch/pytorch/pull/155446))
- [BE] Make PyObjectSlot use a global PyInterpreter and remove ([#158427](https://github.com/pytorch/pytorch/pull/158427))
- fixed typo error ([#159451](https://github.com/pytorch/pytorch/pull/159451))
- [BE]: Update CUTLASS submodule to 4.1.0 ([#158854](https://github.com/pytorch/pytorch/pull/158854))
- Add CPython test for heapq ([#159370](https://github.com/pytorch/pytorch/pull/159370))
- Fix large_tensor_test skipping cpu ([#158617](https://github.com/pytorch/pytorch/pull/158617))
- Update CODEOWNERS for ONNX ([#159390](https://github.com/pytorch/pytorch/pull/159390))
- Fix TestAutogradFallback flaky tests under Dynamo: migrate to lib._destroy() ([#159443](https://github.com/pytorch/pytorch/pull/159443))
- [ATen][CUDA][cuFFT] Guard against deprecated error codes ([#159466](https://github.com/pytorch/pytorch/pull/159466))
- fix strategy hashing arg mismatch ([#159506](https://github.com/pytorch/pytorch/pull/159506))
- [dynamo, easy] add comment on skipping sys.monitoring frames ([#159493](https://github.com/pytorch/pytorch/pull/159493))
- [CUDA][CUDA Graphs] Move cuda graphs test to subprocess to avoid polluting mempool tests ([#159305](https://github.com/pytorch/pytorch/pull/159305))
- Fix rand_like decomposition to preserve strides ([#159294](https://github.com/pytorch/pytorch/pull/159294))
- [Refactor] Fix Compile Warning: `possibly dangling reference to a temporary` ([#159517](https://github.com/pytorch/pytorch/pull/159517))
- disable execution frame cleanup ([#159531](https://github.com/pytorch/pytorch/pull/159531))
- [Break XPU][Indutor UT] Fix failures introduced by community. ([#159463](https://github.com/pytorch/pytorch/pull/159463))
- update the baseline for nightly max_autotune tests ([#154973](https://github.com/pytorch/pytorch/pull/154973))
- set MSVC debug information only on debug builds ([#159533](https://github.com/pytorch/pytorch/pull/159533))
- [inductor] consolidate common GEMM triton param retrieval ([#159383](https://github.com/pytorch/pytorch/pull/159383))
- [CUDA] Add `serialTest` decorator to `largeTensorTest` in `test_cuda.py` ([#159271](https://github.com/pytorch/pytorch/pull/159271))
- [cutlass backend] Fix EVT tests post buf name change ([#159541](https://github.com/pytorch/pytorch/pull/159541))
- [NativeRT] Make VariadicOpConverter and FuseListUnpackConverter for cpu nodes only ([#159519](https://github.com/pytorch/pytorch/pull/159519))
- Fix typo in link to torch memory_viz tool ([#159214](https://github.com/pytorch/pytorch/pull/159214))
- [ez] get rid of unused var ([#159571](https://github.com/pytorch/pytorch/pull/159571))
- [inductor] Add logging for distributed collective ops for multi‑rank diagnostics ([#159190](https://github.com/pytorch/pytorch/pull/159190))
- Add myself as a reviewer for when someone touches headeronly or stable ([#159583](https://github.com/pytorch/pytorch/pull/159583))
- [Doc fix] fix spelling of enough ([#159587](https://github.com/pytorch/pytorch/pull/159587))
- Fix autocast context manager when there is exception ([#159565](https://github.com/pytorch/pytorch/pull/159565))
- fix strategy hashing arg mismatch ([#159506](https://github.com/pytorch/pytorch/pull/159506))
- [ROCm][Windows] Switch __builtin_clz ifdef from WIN32 to MSC_VER. ([#159273](https://github.com/pytorch/pytorch/pull/159273))
- Remove unused input parameter in ExpandableSegment ([#159356](https://github.com/pytorch/pytorch/pull/159356))
- Actually test STD_TORCH_CHECK, add testfile to CMake ([#159603](https://github.com/pytorch/pytorch/pull/159603))
- [BE][Easy] respect `os.environ` in subprocess calls in tools/nightly.py ([#159572](https://github.com/pytorch/pytorch/pull/159572))
- [inductor] Add logging for distributed collective ops for multi‑rank diagnostics ([#159190](https://github.com/pytorch/pytorch/pull/159190))
- [MTIA Aten Backend] Migrate all foreach ops ([#159098](https://github.com/pytorch/pytorch/pull/159098))
- Update CK Kernel generation and update ck submodule ([#157964](https://github.com/pytorch/pytorch/pull/157964))
- [NativeRT] Turn on enableStaticCPUKernels by default ([#159422](https://github.com/pytorch/pytorch/pull/159422))
- Back out "[ez] get rid of unused var" ([#159677](https://github.com/pytorch/pytorch/pull/159677))
- Fix rand_like decomposition to preserve strides ([#159294](https://github.com/pytorch/pytorch/pull/159294))
- Stop parsing command line arguments every time common_utils is imported. ([#156703](https://github.com/pytorch/pytorch/pull/156703))
- [dynamo][guards] Make class members go through obj.__class__.__dict__ ([#159534](https://github.com/pytorch/pytorch/pull/159534))
- gc before warming up benchmarking ([#159670](https://github.com/pytorch/pytorch/pull/159670))
- [dynamo] Be consistent with storing func source for UserMethodVariable ([#159696](https://github.com/pytorch/pytorch/pull/159696))
- Add torch compile force disable caches alias ([#158072](https://github.com/pytorch/pytorch/pull/158072))
- [MTIA Aten Backend] Migrate all.out ([#159539](https://github.com/pytorch/pytorch/pull/159539))
- [BE] Remove macos-13 guard from bench_mps_ops ([#159732](https://github.com/pytorch/pytorch/pull/159732))
- refresh expected results ([#159727](https://github.com/pytorch/pytorch/pull/159727))
- [dynamo][guards] Make class members go through obj.__class__.__dict__ ([#159534](https://github.com/pytorch/pytorch/pull/159534))
- [dynamo] Be consistent with storing func source for UserMethodVariable ([#159696](https://github.com/pytorch/pytorch/pull/159696))
- Fix perf downgrad by reverting template use in use_mkldnn_matmul ([#159024](https://github.com/pytorch/pytorch/pull/159024))
- [MTIA Aten Backend] Migrate arange.start_out ([#159540](https://github.com/pytorch/pytorch/pull/159540))
- [CUDA] Add some more missing `@serialTest` decorators ([#159672](https://github.com/pytorch/pytorch/pull/159672))
- Fix conversion of values in libtorch agnostic tests ([#155115](https://github.com/pytorch/pytorch/pull/155115))
- fix test_verbose_logs_dynamic_shapes with MSVC ([#159573](https://github.com/pytorch/pytorch/pull/159573))
- [export] Allow comparing device w/o index with device w/ index ([#159665](https://github.com/pytorch/pytorch/pull/159665))
- [mps] Turn on inductor dynamic shapes tests ([#159456](https://github.com/pytorch/pytorch/pull/159456))
- [cutlass backend][test] Expand FP8 tests to FP16 ([#159538](https://github.com/pytorch/pytorch/pull/159538))
- Fix failing test ([#159800](https://github.com/pytorch/pytorch/pull/159800))
- Update torch-xpu-ops commit pin ([#159621](https://github.com/pytorch/pytorch/pull/159621))
- Use uppercase OR when checking for system XNNPACK ([#159527](https://github.com/pytorch/pytorch/pull/159527))
- [nativert] force resize to zero. ([#159683](https://github.com/pytorch/pytorch/pull/159683))
- Remove pin on libuv from instructions ([#159504](https://github.com/pytorch/pytorch/pull/159504))
- [Inductor][Triton] Support TMA before strict 3.4 cutoff ([#159777](https://github.com/pytorch/pytorch/pull/159777))
- Generalize torch._C._set_allocator_settings to be generic ([#156175](https://github.com/pytorch/pytorch/pull/156175))
- [inductor] consolidate common GEMM triton param retrieval ([#159383](https://github.com/pytorch/pytorch/pull/159383))
- tools: Add mode to find python automatically ([#159820](https://github.com/pytorch/pytorch/pull/159820))
- [mps] Turn on inductor dynamic shapes tests ([#159456](https://github.com/pytorch/pytorch/pull/159456))
- Set PYTHONHOME for inductor subprocesses using torch ([#159382](https://github.com/pytorch/pytorch/pull/159382))
- fix logging setup issue for Windows.. ([#159887](https://github.com/pytorch/pytorch/pull/159887))
- [export] Fix generated schema for C++20/23 ([#159871](https://github.com/pytorch/pytorch/pull/159871))
- [BE] Fix type hint in AOTIRunnerUtil ([#159577](https://github.com/pytorch/pytorch/pull/159577))
- Add UT for torch.accelerator memory-related API ([#155200](https://github.com/pytorch/pytorch/pull/155200))
- Enable fr_trace to read local traces from multiple hosts. ([#159490](https://github.com/pytorch/pytorch/pull/159490))
- [CUDA 13] CMake/Dependencies: no need to call find_package(CUB) ([#159854](https://github.com/pytorch/pytorch/pull/159854))
- [OpenReg] Disable automatic inclusion of data files ([#159845](https://github.com/pytorch/pytorch/pull/159845))
- Fix execution frame cleanup logic ([#158717](https://github.com/pytorch/pytorch/pull/158717))
- [Inductor UT][Fix XPU CI] Fix case failures introduced by community. ([#159759](https://github.com/pytorch/pytorch/pull/159759))
- Enable XNNPACK aarch64 builds ([#159762](https://github.com/pytorch/pytorch/pull/159762))
- Improve `extract_test_fn` ([#158637](https://github.com/pytorch/pytorch/pull/158637))
- [CPUBLAS] add macros for brgemm APIs for versioning ([#158629](https://github.com/pytorch/pytorch/pull/158629))
- [pytorch] Moving torch.compile worker process logs to a dedicated rank based log directory ([#159874](https://github.com/pytorch/pytorch/pull/159874))
- [HOP, map] Rework of map autograd to the new interface ([#153343](https://github.com/pytorch/pytorch/pull/153343))
- update expected results ([#159867](https://github.com/pytorch/pytorch/pull/159867))
- Add a USE_NIGHTLY option to setup.py ([#159965](https://github.com/pytorch/pytorch/pull/159965))
- Relax unclaimed successes in dtype op tests when running under TEST_WITH_DYNAMO/TEST_WITH_INDUCTOR ([#159976](https://github.com/pytorch/pytorch/pull/159976))
- dataclass pytree fix ([#159916](https://github.com/pytorch/pytorch/pull/159916))
- Renaming HAS_XPU to HAS_XPU_AND_TRITON ([#159908](https://github.com/pytorch/pytorch/pull/159908))
- Actually run the einops tests in CI ([#159776](https://github.com/pytorch/pytorch/pull/159776))
- Further fix failing tests in test/inductor/test_analysis.py ([#160070](https://github.com/pytorch/pytorch/pull/160070))
- dynamo: Remove passing or deleted dynamo_expected_failures ([#159691](https://github.com/pytorch/pytorch/pull/159691))
- [MTIA] Allow users who know what they are doing to ignore all device mismatches in tracing and take a preferred device. ([#159931](https://github.com/pytorch/pytorch/pull/159931))
- turn on executon frame clenaup by default ([#160110](https://github.com/pytorch/pytorch/pull/160110))
- [Linter] Improve device-bias linter by adding detection for `with torch.device("cuda")`. ([#159926](https://github.com/pytorch/pytorch/pull/159926))
- [benchmarks] Add nativert benchmark ([#159922](https://github.com/pytorch/pytorch/pull/159922))
- [BE][PYFMT] remove `black`: finish `black -> ruff format` migration ([#144557](https://github.com/pytorch/pytorch/pull/144557))
- [Test][Easy] Use float16 dtype in test_sort_large ([#159939](https://github.com/pytorch/pytorch/pull/159939))
- CMake build: preserve PYTHONPATH ([#160144](https://github.com/pytorch/pytorch/pull/160144))
- Don't build nccl when distributed is disabled ([#160086](https://github.com/pytorch/pytorch/pull/160086))
- Add UT for torch.accelerator memory-related API ([#155200](https://github.com/pytorch/pytorch/pull/155200))
- [torch.export] Fix test_export_api_with_dynamic_shapes ([#160164](https://github.com/pytorch/pytorch/pull/160164))
- [pytorch][dynamo_compile] Log stack_trace to dynamo_compile ([#159655](https://github.com/pytorch/pytorch/pull/159655))
- Documents tuning NVLink performance on H100/H200 ([#159792](https://github.com/pytorch/pytorch/pull/159792))
- [cuDNN][SDPA] cuDNN SDPA refactor/cleanup, nested tensor backward, test priority bump for `sm90`, `sm100` ([#149282](https://github.com/pytorch/pytorch/pull/149282))
- [Torch Native] Add test for packaging weight  ([#158750](https://github.com/pytorch/pytorch/pull/158750))
- Remove tensorexpr tests ([#158928](https://github.com/pytorch/pytorch/pull/158928))
- Add Sherlock and Zhengxu as codeowner for schema.py ([#160233](https://github.com/pytorch/pytorch/pull/160233))
- [Linter] Expanding the scope of detecting device-bias code. ([#159949](https://github.com/pytorch/pytorch/pull/159949))
- Remove outdated CMAKE_CUDA_COMPILER_VERSION branch ([#160075](https://github.com/pytorch/pytorch/pull/160075))
- Fix get_free_symbol_uses for several nodes. ([#160134](https://github.com/pytorch/pytorch/pull/160134))
- [inductor] turn on windows inductor UTs ([#160161](https://github.com/pytorch/pytorch/pull/160161))
- Delete Python reference implementation from torchdim, as it is untested ([#160115](https://github.com/pytorch/pytorch/pull/160115))
- Add Alban and Piotr to list of maintainers ([#160187](https://github.com/pytorch/pytorch/pull/160187))
- [inductor] turn on windows inductor UTs ([#160161](https://github.com/pytorch/pytorch/pull/160161))
- [CUDA] Remove the uncessary CUDA_GUARD ([#160249](https://github.com/pytorch/pytorch/pull/160249))
- Fix typo in README.md ([#160160](https://github.com/pytorch/pytorch/pull/160160))
- Remove unnecessary CMake checks for glog  ([#158185](https://github.com/pytorch/pytorch/pull/158185))
- Update slow tests ([#158222](https://github.com/pytorch/pytorch/pull/158222))
- [submodule] Bump fbgemm to latest ([#158210](https://github.com/pytorch/pytorch/pull/158210))
- [Graph Partition] Pass all OSS unit tests ([#154667](https://github.com/pytorch/pytorch/pull/154667))
- 159965 is merged, no need to patch it in ([#160275](https://github.com/pytorch/pytorch/pull/160275))
- [MTIA] Implement isAvailable() for MTIA hooks ([#160304](https://github.com/pytorch/pytorch/pull/160304))
- [ROCm][Windows] Fix LoadHIP handling of environment variable paths on Windows. ([#159080](https://github.com/pytorch/pytorch/pull/159080))
- [ROCm][Windows] Enable USE_ROCM, disable USE_RCCL on Windows. ([#159079](https://github.com/pytorch/pytorch/pull/159079))
- [BE][cutlass backend] Fix subproc addmm tests ([#160295](https://github.com/pytorch/pytorch/pull/160295))
- [BE] Isolate pre-push hook dependencies in dedicated virtual environment ([#160048](https://github.com/pytorch/pytorch/pull/160048))
- [ROCm] Add torch/_rocm_init.py to .gitignore. ([#159806](https://github.com/pytorch/pytorch/pull/159806))
- [ROCm][Windows] Revert copying hipblaslt and rocblas dirs. ([#159083](https://github.com/pytorch/pytorch/pull/159083))
- [Graph Partition] Pass all OSS unit tests ([#154667](https://github.com/pytorch/pytorch/pull/154667))
- extract shape in _view_has_unbacked_input ([#160255](https://github.com/pytorch/pytorch/pull/160255))
- Dynamo Deep Dive Documentation Fix ([#158860](https://github.com/pytorch/pytorch/pull/158860))
- Update triton xpu commit to support python 3.14 ([#160183](https://github.com/pytorch/pytorch/pull/160183))
- Account for triton kernel source code hidden in custom ops properly in AOTAutogradCache ([#160120](https://github.com/pytorch/pytorch/pull/160120))
- Fix Tensor illustration, use permalinks for image embedding in Readme.md ([#160416](https://github.com/pytorch/pytorch/pull/160416))
- [Fix XPU CI][Inductor UT] Fix test cases broken by community. ([#160403](https://github.com/pytorch/pytorch/pull/160403))
- Factor out the strings to templates for better editor integration ([#160357](https://github.com/pytorch/pytorch/pull/160357))
- [EZ] Replace `pytorch-labs` with `meta-pytorch` ([#160459](https://github.com/pytorch/pytorch/pull/160459))
- Avoid potential deadlocks in host allocator ([#159352](https://github.com/pytorch/pytorch/pull/159352))
- [inductor] Windows inductor use intel-openmp. ([#160258](https://github.com/pytorch/pytorch/pull/160258))
- Wrap class definitions in `set_fullgraph(False)` in `test_set` ([#160216](https://github.com/pytorch/pytorch/pull/160216))
- Wrap class definitions in `set_fullgraph(False)` in `test_operator` ([#160217](https://github.com/pytorch/pytorch/pull/160217))
- Wrap class definitions in `set_fullgraph(False)` in `test_int`/`bool`/`float`/`complex` ([#160276](https://github.com/pytorch/pytorch/pull/160276))
- Add `compile_id: Optional[CompileID]` to `torch._logging._internal.trace_structured_artifact` ([#160440](https://github.com/pytorch/pytorch/pull/160440))
- Replace TORCH_INTERNAL_ASSERT with TORCH_CHECK ([#160411](https://github.com/pytorch/pytorch/pull/160411))
- Enable XPU for test_autograd_function.py ([#160309](https://github.com/pytorch/pytorch/pull/160309))
- Add placeholder for the User Guide ([#159379](https://github.com/pytorch/pytorch/pull/159379))
- [ROCm] remove extra transposes in NHWC convolutions on MIOpen ([#160435](https://github.com/pytorch/pytorch/pull/160435))
- Fix the Doc of `pivot` in `torch.lu` ([#159617](https://github.com/pytorch/pytorch/pull/159617))
- [BE][CI] Adjust `error_inputs` for cat and complex ([#160378](https://github.com/pytorch/pytorch/pull/160378))
- [BE][Dynamo] Type improvements in `_dynamo/utils` to generics ([#159824](https://github.com/pytorch/pytorch/pull/159824))
- Fix unit test test_equivalent_template_code ([#160432](https://github.com/pytorch/pytorch/pull/160432))
- refresh expected results ([#160537](https://github.com/pytorch/pytorch/pull/160537))
- Factor out the strings to templates for better editor integration ([#160357](https://github.com/pytorch/pytorch/pull/160357))
- finfo eps doc fix ([#160502](https://github.com/pytorch/pytorch/pull/160502))
- Wrap class definitions in `set_fullgraph(False)` in `test_iter` ([#160278](https://github.com/pytorch/pytorch/pull/160278))
- Wrap class definitions in `set_fullgraph(False)` in `test_math`/`cmath` ([#160330](https://github.com/pytorch/pytorch/pull/160330))
- Wrap class definitions in `set_fullgraph(False)` in `test_sort` ([#160331](https://github.com/pytorch/pytorch/pull/160331))
- Remove the dead code in setup.py ([#160515](https://github.com/pytorch/pytorch/pull/160515))
- [Intel GPU] Support SDPA backend selection and priority setting on XPU ([#159464](https://github.com/pytorch/pytorch/pull/159464))
- reduce threshold to suggest changes to expected results ([#160463](https://github.com/pytorch/pytorch/pull/160463))
- appending the pythonpath ([#160219](https://github.com/pytorch/pytorch/pull/160219))
- [BE][Dynamo] Type improvements in `_dynamo/utils` to generics ([#159824](https://github.com/pytorch/pytorch/pull/159824))
- Wrap class definitions in `set_fullgraph(False)` in `test_list`/`tuple` ([#160277](https://github.com/pytorch/pytorch/pull/160277))
- Allow torch.hub.load with unauthorized GITHUB_TOKEN ([#159896](https://github.com/pytorch/pytorch/pull/159896))
- Fix flight recorder for P2P ops ([#160097](https://github.com/pytorch/pytorch/pull/160097))
- [FR] Don't check incomplete ranks for printing ([#160195](https://github.com/pytorch/pytorch/pull/160195))
- [Inductor] addmm + activation function fusion ([#158137](https://github.com/pytorch/pytorch/pull/158137))
- Update checkpoint warning to target PyTorch 2.9  ([#160643](https://github.com/pytorch/pytorch/pull/160643))
- [Test Fix] Delete dynamo skipfile for OpenMP test_one_thread ([#160562](https://github.com/pytorch/pytorch/pull/160562))
- [inductor] dont reuse buffers if it affects peak (#145883) ([#159530](https://github.com/pytorch/pytorch/pull/159530))
- [doc] AOTI debugging guide ([#160430](https://github.com/pytorch/pytorch/pull/160430))
- [ROCm] hipify needs specific header mappings ([#160675](https://github.com/pytorch/pytorch/pull/160675))
- Test multiprocessing spawn timing fix ([#160672](https://github.com/pytorch/pytorch/pull/160672))
- [contextlib] Fixes for CPython contextlib tests ([#157148](https://github.com/pytorch/pytorch/pull/157148))
- [NVIDIA] Refactor Family Blackwell Support codegen ([#156176](https://github.com/pytorch/pytorch/pull/156176))
- Update triton xpu commit to support python 3.14 ([#160183](https://github.com/pytorch/pytorch/pull/160183))
- Introduce OpInfo test for testing export on fake device ([#160694](https://github.com/pytorch/pytorch/pull/160694))
- Update torch-xpu-ops commit pin ([#160062](https://github.com/pytorch/pytorch/pull/160062))
- Disable flaky cpp test RecordDebugHandles.Basic ([#160577](https://github.com/pytorch/pytorch/pull/160577))
- [ez] Make NUMA signpost parameters JSON serializable ([#160710](https://github.com/pytorch/pytorch/pull/160710))
- [cond] support gen_schema for cond ([#154193](https://github.com/pytorch/pytorch/pull/154193))
- [BE] create an empty shape_env for check_input_alias_and_mutation_return_outputs ([#158965](https://github.com/pytorch/pytorch/pull/158965))
- [while_loop] support gen_schema for while_loop ([#158863](https://github.com/pytorch/pytorch/pull/158863))
- [scan] support gen_schema for scan ([#158864](https://github.com/pytorch/pytorch/pull/158864))
- [associative_scan] support gen_schema for associative_scan ([#158883](https://github.com/pytorch/pytorch/pull/158883))
- [map] filter none gradients and add autograd inductor tests ([#160548](https://github.com/pytorch/pytorch/pull/160548))
- [cuDNN][SDPA] Introduce `TORCH_CUDNN_SDPA_AVOID_RECOMPILE=1` ([#155958](https://github.com/pytorch/pytorch/pull/155958))
- [BE] Update nvshem dependency to 3.3.20 ([#160458](https://github.com/pytorch/pytorch/pull/160458))
- [BE] Update nvshem dependency to 3.3.20 ([#160458](https://github.com/pytorch/pytorch/pull/160458))
- [ROCm][Windows] Add hipcc compatibility flags to cpp_extension.py. ([#159790](https://github.com/pytorch/pytorch/pull/159790))
- Fix mypy errors: PyTreeSpec inheritance ([#160652](https://github.com/pytorch/pytorch/pull/160652))
- [inductor] TLParse tensor metadata logging + test ([#160132](https://github.com/pytorch/pytorch/pull/160132))
- Wrap class definitions in `set_fullgraph(False)` in `test_collections` ([#159902](https://github.com/pytorch/pytorch/pull/159902))
- [BE] [Inductor] Re-Land Support TMA before strict 3.4 cutoff ([#160747](https://github.com/pytorch/pytorch/pull/160747))
- [inductor] TLParse tensor metadata logging + test ([#160132](https://github.com/pytorch/pytorch/pull/160132))
- Update TensorPipe submodule ([#160808](https://github.com/pytorch/pytorch/pull/160808))
- Use numpy 1.26.2 for Python 3.9 and 3.10 ([#160836](https://github.com/pytorch/pytorch/pull/160836))
- [nativert] oss subgraph rewriter ([#160780](https://github.com/pytorch/pytorch/pull/160780))
- Add build support for RISCV ([#160172](https://github.com/pytorch/pytorch/pull/160172))
- Update slow tests ([#160870](https://github.com/pytorch/pytorch/pull/160870))
- [ROCm] Add HIPConfig.h to .gitignore like CUDAConfig.h. ([#159805](https://github.com/pytorch/pytorch/pull/159805))
- Recheck Autotune cache on Precompile serialization to prune compilation results ([#158656](https://github.com/pytorch/pytorch/pull/158656))
- Remove unused test code ([#160823](https://github.com/pytorch/pytorch/pull/160823))
- [ROCm] Fix Sliding Window Attention in AOTriton integration code ([#159773](https://github.com/pytorch/pytorch/pull/159773))
- [nativert] oss pass graph pass registration ([#160859](https://github.com/pytorch/pytorch/pull/160859))
- Fix UndefinedGrad::apply ([#160572](https://github.com/pytorch/pytorch/pull/160572))
- [Fix XPU CI][Inductor UT] Fix test cases broken by community. ([#160403](https://github.com/pytorch/pytorch/pull/160403))
- fix mul.Scalar with strided tensor ([#160560](https://github.com/pytorch/pytorch/pull/160560))
- [BE] [Inductor] Re-Land Support TMA before strict 3.4 cutoff ([#160747](https://github.com/pytorch/pytorch/pull/160747))
- Use numpy 1.26.2 for Python 3.9 and 3.10 ([#160836](https://github.com/pytorch/pytorch/pull/160836))
- Update checkpoint warning to target PyTorch 2.9 ([#160725](https://github.com/pytorch/pytorch/pull/160725))
- forward fix #160747 ([#160981](https://github.com/pytorch/pytorch/pull/160981))
- [inductor] dont reuse buffers if it affects peak (#145883) ([#159530](https://github.com/pytorch/pytorch/pull/159530))
- [BE][Ez]: Update ruff to 0.12.9 ([#160896](https://github.com/pytorch/pytorch/pull/160896))
- [WIP] Merge Test ([#160998](https://github.com/pytorch/pytorch/pull/160998))
- [export] Relax FC requirement of serde.deserialize by allowing unknown fields. ([#160918](https://github.com/pytorch/pytorch/pull/160918))
- [FP8][cuBLAS][SM100] cuBLAS doesn't support rowwise-scaling on `sm100` ([#160693](https://github.com/pytorch/pytorch/pull/160693))
- fabric detection - fix build on an old toolkit ([#160984](https://github.com/pytorch/pytorch/pull/160984))
- [FSDP][Replicate] replicate tests for param registration and input device movements ([#160147](https://github.com/pytorch/pytorch/pull/160147))
- test_matmul_cuda: Refine MX test skipping ([#161009](https://github.com/pytorch/pytorch/pull/161009))
- [nativert] ensure that moveable outputs are set in other executionframe ctor ([#161005](https://github.com/pytorch/pytorch/pull/161005))
- Back out "Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (#156165)" ([#160999](https://github.com/pytorch/pytorch/pull/160999))
- [BE] Remove intel-openmp dependency in setup.py ([#160976](https://github.com/pytorch/pytorch/pull/160976))
- add static dispatch kernel registration to open source ([#160439](https://github.com/pytorch/pytorch/pull/160439))
- [CUDA] Bump tolerances for `test_baddmm` ([#159915](https://github.com/pytorch/pytorch/pull/159915))
- Try to fix Inductor CI periodic tests ([#160932](https://github.com/pytorch/pytorch/pull/160932))
- Decrease number of bytes used by uninitialized tokens_ in KernelFunction ([#160764](https://github.com/pytorch/pytorch/pull/160764))
- Replace _device_t with torch.types.Device in torch/cpu/__init__.py ([#161031](https://github.com/pytorch/pytorch/pull/161031))
- [BE] Move indexing tests to test_indexing ([#160994](https://github.com/pytorch/pytorch/pull/160994))
- don't try to set lazy module loading env var ([#161103](https://github.com/pytorch/pytorch/pull/161103))
- remove redundant installation ([#160634](https://github.com/pytorch/pytorch/pull/160634))
- Fix torchaudio build when TORCH_CUDA_ARCH_LIST is not set ([#161084](https://github.com/pytorch/pytorch/pull/161084))
- [nativert] oss static kernel test utils ([#161086](https://github.com/pytorch/pytorch/pull/161086))
- [inductor] Estimate peak memory allocfree and applying to reordering collectives ([#160113](https://github.com/pytorch/pytorch/pull/160113))
- [nativert] make runtime const folding aware of run_const_graph ([#160760](https://github.com/pytorch/pytorch/pull/160760))
- [inductor] Estimate peak memory allocfree and applying to reordering collectives ([#160113](https://github.com/pytorch/pytorch/pull/160113))
- Fix torchaudio build when TORCH_CUDA_ARCH_LIST is not set ([#161084](https://github.com/pytorch/pytorch/pull/161084))
- [nativert] oss static kernel tests ([#161087](https://github.com/pytorch/pytorch/pull/161087))
- [Inductor] Update Outer Reduction Heuristic ([#159093](https://github.com/pytorch/pytorch/pull/159093))
- [BE] Enable `test_index_put_accumulate_duplicate_indices` on MPS ([#161201](https://github.com/pytorch/pytorch/pull/161201))
- [nativert] oss layout planner tests ([#160942](https://github.com/pytorch/pytorch/pull/160942))
- Quick fix to headers in stable/tensor_inl.h ([#161168](https://github.com/pytorch/pytorch/pull/161168))
- [ROCm][Windows] Include native_transformers srcs to fix link errors. ([#160373](https://github.com/pytorch/pytorch/pull/160373))
- Refactoring TensorImpl by using constexpr and std::is_same_v ([#161043](https://github.com/pytorch/pytorch/pull/161043))
- [inductor] Estimate peak memory allocfree and applying to reordering collectives ([#160113](https://github.com/pytorch/pytorch/pull/160113))
- Enable `test/test_numpy_interop.py` config in mypy ([#158556](https://github.com/pytorch/pytorch/pull/158556))
- Move non inductor workflows to Python 3.9 -> 3.10 ([#161182](https://github.com/pytorch/pytorch/pull/161182))
- remove old while_loop_schema_gen test ([#161202](https://github.com/pytorch/pytorch/pull/161202))
- Fix conv exhaustive autotuning and expand Exhaustive test coverage ([#159387](https://github.com/pytorch/pytorch/pull/159387))
- [cuDNN] head dim > 128 works on H100 again in cuDNN SDPA?  ([#161210](https://github.com/pytorch/pytorch/pull/161210))
- [benchmark] Add torchscript jit.trace to benchmark option ([#161223](https://github.com/pytorch/pytorch/pull/161223))
- Add initial bc-linter configuration ([#161319](https://github.com/pytorch/pytorch/pull/161319))
- Update pyrefly config for better codenav ([#161200](https://github.com/pytorch/pytorch/pull/161200))
- [Test] Adding a testcase for constant_pad_nd ([#161259](https://github.com/pytorch/pytorch/pull/161259))
- [AMD] Fix AMD User Defined Kernel Autotune ([#160671](https://github.com/pytorch/pytorch/pull/160671))
- [OpenReg] Add OSX/Windows Support for OpenReg ([#159441](https://github.com/pytorch/pytorch/pull/159441))
- Fix typo: 'complext' ([#160335](https://github.com/pytorch/pytorch/pull/160335))
- [dynamo] Refactor convert_frame.compile_frame to be self contained function. [5/n] ([#160900](https://github.com/pytorch/pytorch/pull/160900))
- [cuDNN][convolution] remove redundant conv3d 64bit test ([#161177](https://github.com/pytorch/pytorch/pull/161177))
- docstring_linter: Fix #151692 and other issues ([#156596](https://github.com/pytorch/pytorch/pull/156596))
- Improve efficiency of _python_dispatch.return_and_correct_aliasing ([#161240](https://github.com/pytorch/pytorch/pull/161240))
- Remove unnecessary len() call in _correct_storage_aliasing.is_read_only_alias_match ([#161284](https://github.com/pytorch/pytorch/pull/161284))
- Inline is_read_only_alias_match in _correct_storage_aliasing ([#161285](https://github.com/pytorch/pytorch/pull/161285))
- [inductor] structured-log graph execution order + test ([#160448](https://github.com/pytorch/pytorch/pull/160448))
- [Cutlass] Fix regression from  f7ad69f ([#161398](https://github.com/pytorch/pytorch/pull/161398))
- Ensure large tensor int32 -> int64 indexing is enabled ([#157767](https://github.com/pytorch/pytorch/pull/157767))
- [Inductor] Prune configs that require more shared memory than the hardware limit ([#161040](https://github.com/pytorch/pytorch/pull/161040))
- [dynamo] Refactor convert_frame.compile_frame to be self contained function. [5/n] ([#160900](https://github.com/pytorch/pytorch/pull/160900))
- Increase timeout value when pushing to ghcr.io ([#161444](https://github.com/pytorch/pytorch/pull/161444))
- Get Inductor periodic CI green ([#161297](https://github.com/pytorch/pytorch/pull/161297))
- [dynamo, nested graph breaks] add nested graph break tests ([#144516](https://github.com/pytorch/pytorch/pull/144516))
- [dynamo, nested graph breaks] support very simple nested graph breaks ([#159329](https://github.com/pytorch/pytorch/pull/159329))
- [dynamo, nested graph breaks] support nested graph breaks x context managers ([#159678](https://github.com/pytorch/pytorch/pull/159678))
- [dynamo, nested graph breaks] support nested closures ([#159817](https://github.com/pytorch/pytorch/pull/159817))
- [dynamo, nested graph breaks] clean up comments and codegen ([#160138](https://github.com/pytorch/pytorch/pull/160138))
- [dynamo, nested graph breaks] prevent excessive recompilations ([#159786](https://github.com/pytorch/pytorch/pull/159786))
- Disable inductor/test_flex_attention.py ([#161450](https://github.com/pytorch/pytorch/pull/161450))
- [OpenReg] Refactor and Optimize the OpenReg for Preparation of Docs ([#159640](https://github.com/pytorch/pytorch/pull/159640))
- Increase timeout value when pushing to ghcr.io ([#161444](https://github.com/pytorch/pytorch/pull/161444))
- space added between type and checking for typechecking ([#161352](https://github.com/pytorch/pytorch/pull/161352))
- [easy][test] Add repeat_interleave opinfo that exercises binary search fusion ([#161445](https://github.com/pytorch/pytorch/pull/161445))
- SDPA skip logic for ROCm ([#160522](https://github.com/pytorch/pytorch/pull/160522))
- [Inductor] Update Outer Reduction Heuristic ([#159093](https://github.com/pytorch/pytorch/pull/159093))
- [hop] make materialize_as_graph disable pre-existing dispatch modes ([#161220](https://github.com/pytorch/pytorch/pull/161220))
- Back out "Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (#150312)" ([#161002](https://github.com/pytorch/pytorch/pull/161002))
- [ROCm][CI] restore test_flex_attention tests ([#161519](https://github.com/pytorch/pytorch/pull/161519))
- Add inductor backend to device interface; make minifier_tests more device agnostic ([#151314](https://github.com/pytorch/pytorch/pull/151314))
- [FP8][cuBLAS][SM100] cuBLAS doesn't support rowwise-scaling on `sm110` or `sm120` either ([#161236](https://github.com/pytorch/pytorch/pull/161236))
- Replace manual cache in _python_dispatch.get_alias_info with functools.cache ([#161286](https://github.com/pytorch/pytorch/pull/161286))
- Fix conv exhaustive autotuning and expand Exhaustive test coverage ([#159387](https://github.com/pytorch/pytorch/pull/159387))
- Move non inductor workflows to Python 3.9 -> 3.10 ([#161182](https://github.com/pytorch/pytorch/pull/161182))
- Ensure large tensor int32 -> int64 indexing is enabled ([#157767](https://github.com/pytorch/pytorch/pull/157767))
- [dynamo, nested graph breaks] add nested graph break tests ([#144516](https://github.com/pytorch/pytorch/pull/144516))
- [benchmarks] Skip mobilenetv3_large_100 in CI for accuracy ([#161570](https://github.com/pytorch/pytorch/pull/161570))
- Wrap class definitions in `set_fullgraph(False)` in `test_dict`/`test_ordered_dict` ([#160156](https://github.com/pytorch/pytorch/pull/160156))
- Set USE_NVSHMEM only if USE_DISTRIBUTED is set ([#161451](https://github.com/pytorch/pytorch/pull/161451))
- [inductor] structured-log graph execution order + test ([#160448](https://github.com/pytorch/pytorch/pull/160448))
- Updates to CuTe DSL template renderer ([#161117](https://github.com/pytorch/pytorch/pull/161117))
- Remove test since it ooms on CI ([#161644](https://github.com/pytorch/pytorch/pull/161644))
- Revert "Back out "Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (#156165)" (#160999)" ([#161625](https://github.com/pytorch/pytorch/pull/161625))
- Revert "Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (#156165)" ([#161627](https://github.com/pytorch/pytorch/pull/161627))
- Revert "Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (#150312)" ([#161628](https://github.com/pytorch/pytorch/pull/161628))
- [dynamo, nested graph breaks] support very simple nested graph breaks ([#159329](https://github.com/pytorch/pytorch/pull/159329))
- [dynamo, nested graph breaks] support nested graph breaks x context managers ([#159678](https://github.com/pytorch/pytorch/pull/159678))
- [dynamo, nested graph breaks] support nested closures ([#159817](https://github.com/pytorch/pytorch/pull/159817))
- [dynamo, nested graph breaks] clean up comments and codegen ([#160138](https://github.com/pytorch/pytorch/pull/160138))
- [dynamo, nested graph breaks] prevent excessive recompilations ([#159786](https://github.com/pytorch/pytorch/pull/159786))
- Updates to CuTe DSL template renderer ([#161117](https://github.com/pytorch/pytorch/pull/161117))
- Replace some calls to new with make_{unique,shared} ([#160581](https://github.com/pytorch/pytorch/pull/160581))
- Add ciflow/vllm to vLLM commit hash update PR(s) ([#161678](https://github.com/pytorch/pytorch/pull/161678))
- [export] Support AC HOP in pre-dispatch ([#161479](https://github.com/pytorch/pytorch/pull/161479))
- Add test coverage to tf32 in max autotune mm configs ([#161545](https://github.com/pytorch/pytorch/pull/161545))
- Fix Inductor Periodic ([#161617](https://github.com/pytorch/pytorch/pull/161617))
- Use std::apply for CPU code ([#152526](https://github.com/pytorch/pytorch/pull/152526))
- [Inductor UT] Re-enable test_torchinductor_opinfo.py on XPU. ([#161477](https://github.com/pytorch/pytorch/pull/161477))
- [cuDNN][TF32] Account for TF32 in `test_super_resolution_cuda`  ([#161662](https://github.com/pytorch/pytorch/pull/161662))
- [cuBLASLt][FP8] `cuBLASLt` appears to support float8 rowwise-scaling on H100 ([#161305](https://github.com/pytorch/pytorch/pull/161305))
- [CI] Migrate XPU build and test to python 3.10 ([#161708](https://github.com/pytorch/pytorch/pull/161708))
- [PT2] Add fastResizeToZero to all static dispatch kernels ([#161679](https://github.com/pytorch/pytorch/pull/161679))
- Ensure large tensor int32 -> int64 indexing is enabled ([#157767](https://github.com/pytorch/pytorch/pull/157767))
- Cleanup stale submodule directories in checkout action ([#161748](https://github.com/pytorch/pytorch/pull/161748))
- [Refactor][XPU] Refactor XPU quantization op and add header files. ([#157430](https://github.com/pytorch/pytorch/pull/157430))
- Cleanup stale submodule directories after checkout ([#161748](https://github.com/pytorch/pytorch/pull/161748))
- [Inductor] Update Outer Reduction Heuristic ([#159093](https://github.com/pytorch/pytorch/pull/159093))
- Add MTIA to floor_divide op ([#161575](https://github.com/pytorch/pytorch/pull/161575))
- [CI] Migrate XPU build and test to python 3.10 ([#161708](https://github.com/pytorch/pytorch/pull/161708))
- Cleanup stale submodule directories after checkout ([#161748](https://github.com/pytorch/pytorch/pull/161748))
- Stop accessing func._schema in _python_dispatch.correct_storage_aliasing ([#161292](https://github.com/pytorch/pytorch/pull/161292))
- Use `is`, not ==, to check exact type matches in _python_dispatch ([#161304](https://github.com/pytorch/pytorch/pull/161304))
- Optimize _python_dispatch.return_and_correct_aliasing.get_write_alias ([#161308](https://github.com/pytorch/pytorch/pull/161308))
- Fix pybind enum efficiency issue in return_and_correct_aliasing ([#161315](https://github.com/pytorch/pytorch/pull/161315))
- Improve assert perf in _python_dispatch._correct_storage_aliasing ([#161317](https://github.com/pytorch/pytorch/pull/161317))
- Avoid double hash lookup in torch._library.simple_registry ([#161328](https://github.com/pytorch/pytorch/pull/161328))
- Fix accidental copy in pushPyOutToStack ([#161329](https://github.com/pytorch/pytorch/pull/161329))
- Stop trying to intern arguments in PyObject_FastGetAttrString ([#161432](https://github.com/pytorch/pytorch/pull/161432))
- Update torch-xpu-ops commit pin ([#161152](https://github.com/pytorch/pytorch/pull/161152))
- Fix _scaled_grouped_mm not reported as unsupported on SM100. ([#161780](https://github.com/pytorch/pytorch/pull/161780))
- [OpenReg] Rename cpu_fallback_blacklist to cpu_fallback_blocklist ([#161603](https://github.com/pytorch/pytorch/pull/161603))
- [OpenReg] Add Event&Stream Support for OpenReg Backend ([#160099](https://github.com/pytorch/pytorch/pull/160099))
- [OpenReg] Add tests of device and memory for OpenReg ([#161773](https://github.com/pytorch/pytorch/pull/161773))
- [OpenReg] Integrate Event&Stream from OpenReg Backend into PyTorch ([#160100](https://github.com/pytorch/pytorch/pull/160100))
- [AOTI] fix ut, add extension file type for Windows. ([#161851](https://github.com/pytorch/pytorch/pull/161851))
- fix spelling of word - when  ([#160185](https://github.com/pytorch/pytorch/pull/160185))
- PythonArgs::toBool: order cheap mutually exclusive checks first ([#161455](https://github.com/pytorch/pytorch/pull/161455))
- Leak Python filenames so that we can give good dispatcher errors. ([#160418](https://github.com/pytorch/pytorch/pull/160418))
- Remove background thread UT on XPU to fix CI ([#161844](https://github.com/pytorch/pytorch/pull/161844))
- update supported OS for Intel client GPU ([#161699](https://github.com/pytorch/pytorch/pull/161699))
- [CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator ([#158352](https://github.com/pytorch/pytorch/pull/158352))
- Fix type checking for persistent loads in the weights-only unpickler ([#161661](https://github.com/pytorch/pytorch/pull/161661))
- Defer loading hipify until it is needed ([#160824](https://github.com/pytorch/pytorch/pull/160824))
- test: ensure editable cached wrapper is respected ([#160943](https://github.com/pytorch/pytorch/pull/160943))
- [BE][Easy] restore #157584 after #158288 ([#158541](https://github.com/pytorch/pytorch/pull/158541))
- Fixes broken memory_viz link in CUDA memory docs ([#161426](https://github.com/pytorch/pytorch/pull/161426))
- [HOTFIX] Disable DISTRIBUTED_C10D_DIRECT_ACCESS for now ([#161946](https://github.com/pytorch/pytorch/pull/161946))
- CUDA 13 -- sm_120 -- Nvidia 5090 -- ptxas warning : Value of threads … ([#161380](https://github.com/pytorch/pytorch/pull/161380))
- Defer loading hipify until it is needed ([#160824](https://github.com/pytorch/pytorch/pull/160824))
- ROCm: Enable overload tests from test_matmul_cuda ([#161540](https://github.com/pytorch/pytorch/pull/161540))
- Update Kineto submodule ([#161572](https://github.com/pytorch/pytorch/pull/161572))
- Add api info for torch._C._nn.pyi ([#161958](https://github.com/pytorch/pytorch/pull/161958))
- Log Const Folded Node ([#161827](https://github.com/pytorch/pytorch/pull/161827))
- Fix compiler errors in 3.14 stub definitions ([#161792](https://github.com/pytorch/pytorch/pull/161792))
- [inductor][ez] add hook for heuristics to adjust kernel input nodes ([#161339](https://github.com/pytorch/pytorch/pull/161339))
- build: Add fallback commands to setup.py ([#162009](https://github.com/pytorch/pytorch/pull/162009))
- [OpenReg] Migrate Accelerator Document from source/notes into source/accelerator ([#161845](https://github.com/pytorch/pytorch/pull/161845))
- Using pip3 install instead of python setup.py develop/install ([#161903](https://github.com/pytorch/pytorch/pull/161903))
- [Intel GPU] Fix XPU SDPA default priority_order UT fail ([#161690](https://github.com/pytorch/pytorch/pull/161690))
- Perf nitpicks on python_arg_parser's is_int_or_symint_list ([#161998](https://github.com/pytorch/pytorch/pull/161998))
- Using get_paths() to get correct installation path for PYTHONPATY ([#161947](https://github.com/pytorch/pytorch/pull/161947))
- Outline SymInt::maybe_as_int_slow_path ([#161466](https://github.com/pytorch/pytorch/pull/161466))
- Add inline fast paths for SymInt operators ([#161586](https://github.com/pytorch/pytorch/pull/161586))
- Add CPython test `test_range` ([#161799](https://github.com/pytorch/pytorch/pull/161799))
- Contiguous subgraph decomposition ([#161241](https://github.com/pytorch/pytorch/pull/161241))
- [inductor][ez] add hook for heuristics to adjust kernel input nodes ([#161339](https://github.com/pytorch/pytorch/pull/161339))
- [ROCm] Use MI325 (gfx942) runners for binary smoke testing ([#162044](https://github.com/pytorch/pytorch/pull/162044))
- [hipify] Replace cudaStreamCaptureStatusNone ([#161992](https://github.com/pytorch/pytorch/pull/161992))
- [ROCm] TunableOp should use HIP version, not ROCm version ([#162067](https://github.com/pytorch/pytorch/pull/162067))
- Modify ROCm MI2xx-based workflows to run on cron schedule ([#162103](https://github.com/pytorch/pytorch/pull/162103))
- [inductor] pdl inductor option (disabled by default) ([#160928](https://github.com/pytorch/pytorch/pull/160928))
- expose number of outputs in native runtime for unified runtime ([#161723](https://github.com/pytorch/pytorch/pull/161723))
- [pt] strip error messages in profile builds ([#162076](https://github.com/pytorch/pytorch/pull/162076))
- Contiguous subgraph decomposition ([#161241](https://github.com/pytorch/pytorch/pull/161241))
- [DLPACK] Optimize toDLPack Conversion Speed ([#162111](https://github.com/pytorch/pytorch/pull/162111))
- [ROCm] Enable USE_FBGEMM_GENAI ([#160676](https://github.com/pytorch/pytorch/pull/160676))
- Optimize AMP custom_backend_name error message ([#162037](https://github.com/pytorch/pytorch/pull/162037))
- [Intel GPU] Upgrade OneDNN XPU Tag to v3.9.1 ([#161932](https://github.com/pytorch/pytorch/pull/161932))
- Keep default `CMAKE_PREFIX_PATH` in test_aot_inductor_package ([#161907](https://github.com/pytorch/pytorch/pull/161907))
- Add dependency-groups.dev to pyproject.toml ([#161216](https://github.com/pytorch/pytorch/pull/161216))
- Update torch-xpu-ops commit pin ([#162062](https://github.com/pytorch/pytorch/pull/162062))
- [CUDA] Reuse blocks with record_stream during CUDA Graph capture in the CUDACachingAllocator ([#158352](https://github.com/pytorch/pytorch/pull/158352))
- create torch._grouped_mm fallback path with for loops / bmm ([#161407](https://github.com/pytorch/pytorch/pull/161407))
- move `_grouped_mm` fallback to composite explicit autograd ([#161717](https://github.com/pytorch/pytorch/pull/161717))
- enable float32 and float16 in `torch._grouped_mm` fallback ([#162059](https://github.com/pytorch/pytorch/pull/162059))
- [cuDNN][SDPA] Enable cuDNN SDPA by default for SM 9.0, SM 10.0 ([#162073](https://github.com/pytorch/pytorch/pull/162073))
- [BE]: Update cpp-httplib submodule to 0.26.0 ([#162181](https://github.com/pytorch/pytorch/pull/162181))
- [nativert] triton runtime implementation ([#161798](https://github.com/pytorch/pytorch/pull/161798))
- [ROCm] Enabling several UTs ([#161715](https://github.com/pytorch/pytorch/pull/161715))
- [B200][MXFP8] Fix regex in `test_blockwise_mxfp8_nvfp4_error_messages_recipe_mxfp8_cuda` ([#162180](https://github.com/pytorch/pytorch/pull/162180))
- [B200][NVFP4] Fix argument passing in `test_blockwise_mxfp8_nvfp4_mxfp4_numerics_` ([#162185](https://github.com/pytorch/pytorch/pull/162185))
- A basic CLAUDE.md based on bad things I see claude code doing ([#162163](https://github.com/pytorch/pytorch/pull/162163))
- [cuBLASLt][FP8] `cuBLASLt` appears to support float8 rowwise-scaling on H100 ([#161305](https://github.com/pytorch/pytorch/pull/161305))
- Add const to stable amax ([#162082](https://github.com/pytorch/pytorch/pull/162082))
- [inductor] add kernel template choice (ktc) ([#161347](https://github.com/pytorch/pytorch/pull/161347))
- [inductor][ez] V.choices.get_mm_configs returns list of ChoiceCallers ([#161348](https://github.com/pytorch/pytorch/pull/161348))
- [BE][pytree] cleanup parameterized pytree tests ([#160842](https://github.com/pytorch/pytorch/pull/160842))
- [cutlass backend] Add FP8 tests for multiple linears ([#160782](https://github.com/pytorch/pytorch/pull/160782))
- symbolic cpp channels_last_contiguous ([#160402](https://github.com/pytorch/pytorch/pull/160402))
- Add return-max-scores to flex-attention ([#161667](https://github.com/pytorch/pytorch/pull/161667))
- Resize to 0 if not going to be used ([#161730](https://github.com/pytorch/pytorch/pull/161730))
- re-land triton runtime implementation" ([#162217](https://github.com/pytorch/pytorch/pull/162217))
- Disable autocast when running joint graph passes ([#162304](https://github.com/pytorch/pytorch/pull/162304))
- codebase structure documentation to include torchgen  ([#162261](https://github.com/pytorch/pytorch/pull/162261))
- Docs on export joint with descriptors ([#159006](https://github.com/pytorch/pytorch/pull/159006))
- Add api info for torch._C._nn.pyi ([#162148](https://github.com/pytorch/pytorch/pull/162148))
- torch.zeros bound checks for symint ([#161976](https://github.com/pytorch/pytorch/pull/161976))
- Add -Wno-ctad-maybe-unsupported compiler flag ([#162223](https://github.com/pytorch/pytorch/pull/162223))
- [while_loop][autograd] support autograd_key of while_loop ([#160483](https://github.com/pytorch/pytorch/pull/160483))
- [inductor] fuse for scalar shared data ([#162311](https://github.com/pytorch/pytorch/pull/162311))
- [while_loop][autograd] support autograd_key of while_loop ([#160483](https://github.com/pytorch/pytorch/pull/160483))
- Add BundledAOTAutogradSerializableCallable ([#162170](https://github.com/pytorch/pytorch/pull/162170))
- [associative_scan] Autograd separated ([#139939](https://github.com/pytorch/pytorch/pull/139939))
- [audio hash update] update the pinned audio hash ([#162315](https://github.com/pytorch/pytorch/pull/162315))
- Update slow tests ([#161395](https://github.com/pytorch/pytorch/pull/161395))
- Don't call check_has_torch_dispatch in THPVariable_NewWithVar if we already know ([#161591](https://github.com/pytorch/pytorch/pull/161591))
- [Intel GPU] Integrate OneDNN SDPA training forward and backward ([#161058](https://github.com/pytorch/pytorch/pull/161058))
- [inductor] fuse for scalar shared data ([#162311](https://github.com/pytorch/pytorch/pull/162311))
- Avoid crash with release_available_cached_blocks ([#162269](https://github.com/pytorch/pytorch/pull/162269))
- [cuDNN][SDPA][Nested Tensor] add forward/backward caching support for cuDNN SDPA Nested tensor/varlen  ([#161434](https://github.com/pytorch/pytorch/pull/161434))
- [audio hash update] update the pinned audio hash ([#162315](https://github.com/pytorch/pytorch/pull/162315))
- Handle f([]) vs. f() in fake tensor caching ([#162284](https://github.com/pytorch/pytorch/pull/162284))
- [BE]: Update cudnn frontend submodule to 1.14.1 ([#162347](https://github.com/pytorch/pytorch/pull/162347))
- Add std::any_of to ConvParams struct ([#162334](https://github.com/pytorch/pytorch/pull/162334))
- Add fp16-overflow regression test ([#162401](https://github.com/pytorch/pytorch/pull/162401))
- Add return-max-scores to flex-attention ([#161667](https://github.com/pytorch/pytorch/pull/161667))
- [associative_scan] Autograd separated ([#139939](https://github.com/pytorch/pytorch/pull/139939))
- fastpath type Tensor in THPVariable_NewWithVar ([#161634](https://github.com/pytorch/pytorch/pull/161634))
- Call checkLong in is_int_or_symint, completing TODO ([#161692](https://github.com/pytorch/pytorch/pull/161692))
- Dynamo: set_eval_frame microoptimization ([#162220](https://github.com/pytorch/pytorch/pull/162220))
- [export] Update PT2 archive docs ([#162308](https://github.com/pytorch/pytorch/pull/162308))
- testing infra and some fixes ([#162183](https://github.com/pytorch/pytorch/pull/162183))
- Add BundledAOTAutogradSerializableCallable ([#162170](https://github.com/pytorch/pytorch/pull/162170))
### security
