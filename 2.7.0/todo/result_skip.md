
# Release Notes worksheet skip

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please cleanup, and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* Please use markdown format.
* Please use #PR_NUM to link to the PR, instead of `[#PR_NUM](https://github.com/pytorch/pytorch/pull/#PR_NUM)` to reduce the length of the release notes.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## skip
### bc breaking
### deprecation
### new features
### improvements
### bug fixes
### performance
### docs
### devs
### Untopiced
- [14/N] Fix extra warnings brought by clang-tidy-17 ([#141644](https://github.com/pytorch/pytorch/pull/141644))
- [Dynamo] only import einops if version is lower than 0.7.0 ([#142847](https://github.com/pytorch/pytorch/pull/142847))
- Revert "[Inductor] Use sleef implementation for CPP backend asinh codegen (cd1b5924d53)
- Revert "Hide torch_python symbols (cf538efd0c4)
- Remove unused Python variables in torch/[_-a]* ([#133492](https://github.com/pytorch/pytorch/pull/133492))
- Revert "[14/N] Fix extra warnings brought by clang-tidy-17 (2f0fe82f6d0)
- Revert "Tests Generelization for multiple accelerator devices (c85323c5e82)
- Revert "[Dynamo] only import einops if version is lower than 0.7.0 (d48b16a7251)
- Prologue Fusion ([#134532](https://github.com/pytorch/pytorch/pull/134532))
- [14/N] Fix extra warnings brought by clang-tidy-17 ([#141644](https://github.com/pytorch/pytorch/pull/141644))
- Revert "Migrate compiler config to Config (e87f07d3b8c)
- [Profiler] Add Optional Flag to turn off external correlations ([#142516](https://github.com/pytorch/pytorch/pull/142516))
- Revert "[ROCm] Prune old gfx archs gfx900/gfx906 from binaries (f406207af2b)
- Revert "[Profiler] Add Optional Flag to turn off external correlations (9ed045eae92)
- [RELAND] Add device-agnostic runtime Device/Stream C++ API ([#138677](https://github.com/pytorch/pytorch/pull/138677))
- Revert "[AMD] Turn on TF32 for aten::mm (7ab3177776a)
- Add support for other backends in get_preferred_device ([#132118](https://github.com/pytorch/pytorch/pull/132118))
- Revert "Update low prec codegen for div/mod (54ed13cdce0)
- Revert "[BE] Revert "Add conda to Manylinux Docker images (#139903)" (6356690b3d2)
- Kill capture_pre_autograd_graph API ([#143224](https://github.com/pytorch/pytorch/pull/143224))
- FileTimerClient: add retry logic on connect ([#143318](https://github.com/pytorch/pytorch/pull/143318))
- Revert "Kill capture_pre_autograd_graph API (519d858c31a)
- Revert "FileTimerClient: add retry logic on connect (533d63f83b4)
- Revert "[ROCm] CK Flash Attention Backend (969b07b96f4)
- Revert "[reland][dynamo][guards] Consider tensors as immutable for dict tag matches (e3d754419f7)
- [Dynamo] only import einops if version is lower than 0.7.0 ([#142847](https://github.com/pytorch/pytorch/pull/142847))
- FileTimerClient: add retry logic on connect ([#143318](https://github.com/pytorch/pytorch/pull/143318))
- [ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend ([#134124](https://github.com/pytorch/pytorch/pull/134124))
- Revert "[ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend (14fe1f71902)
- [ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend ([#134124](https://github.com/pytorch/pytorch/pull/134124))
- Revert "[export] don't decompose custom triton op when exporting (e9bd74d7636)
- Revert "[Dynamo] only import einops if version is lower than 0.7.0 (145fd5bad0c)
- Revert "[ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend (8136daff5a1)
- Revert "[Inductor] inplace padding (4462cc6375f)
- Revert "[AOTI] Emit a CMakeLists.txt when package_cpp_only (71479a9b9c6)
- [Dynamo] only import einops if version is lower than 0.7.0 ([#142847](https://github.com/pytorch/pytorch/pull/142847))
- Set `enable_trace_contextlib_contextmanager` flag to True ([#140604](https://github.com/pytorch/pytorch/pull/140604))
- [ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend ([#134124](https://github.com/pytorch/pytorch/pull/134124))
- Revert "[logging] A few fixes/updates to record_compilation_metrics (ad7ab5ef840)
- Revert "(MTIA) Move "empty_cache" API (dabc9566c49)
- Revert "[MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage (c7d7eff798e)
- Revert "Fix issue with setAttribute and int8_t vs int32_t variables (b89bfe0bacb)
- Revert "Fix unused-variable issues in caffe2 (97990f476d4)
- Revert "[pytorch/et] Allow ET to save additional resources for completing a trace like generated kernels and index tensor data (bee47b0663a)
- Revert "export AOTI_TORCH_EXPORT on Windows. (e15442a9b27)
- Enable more C++ warnings ([#143355](https://github.com/pytorch/pytorch/pull/143355))
- Handle meta tensors in FX quantization ([#142262](https://github.com/pytorch/pytorch/pull/142262))
- Revert "Handle meta tensors in FX quantization (197954e14b2)
- Revert "[reland][AMD] Turn on TF32 for aten::mm (448c16ac87f)
- Revert "Add FP8 support for eye (1519a9e30b6)
- Revert "Add a warning when a tensor with requires_grad=True is converted to a scalar (49fdc52fd2b)
- Revert "[Inductor XPU] Support max-autotune on XPU and reuse the corresponding Inductor UT. (844e6108f6b)
- Revert "Enable more C++ warnings (9255ffc8414)
- Revert "Use absolute path `path.resolve()` -> `path.absolute()` (cc4e70b7c3f)
- Revert "[BE][Easy] use `pathlib.Path` instead of `dirname` / `".."` / `pardir` (475656fd9c3)
- Revert "[dynamo] Remove dead code after introducing UserDefinedDictVariable (ee25daef5a4)
- Revert "[dynamo] Remove DICT_SUBCLASS_GUARD_MANAGER and use dict.keys (26364428f5b)
- Add torch._scaled_mm for CPU ([#139975](https://github.com/pytorch/pytorch/pull/139975))
- fix randint distribution for large max ([#143787](https://github.com/pytorch/pytorch/pull/143787))
- Revert "Add torch._scaled_mm for CPU (fca457b5db7)
- Enable more C++ warnings ([#143355](https://github.com/pytorch/pytorch/pull/143355))
- Revert "fix randint distribution for large max (35714767395)
- Revert "remove allow-untyped-defs from torch/ao/__init__.py (b5042cfa58b)
- Add torch._scaled_mm for CPU ([#139975](https://github.com/pytorch/pytorch/pull/139975))
- Revert "Add AOT inductor support for _scaled_mm for CPU (8cccc46e334)
- Revert "Add torch._scaled_mm for CPU (45a709d9ec5)
- Revert "[inductor] Make generated kernels deterministic (1b0d19a2cbb)
- Revert "Update low prec codegen for div/mod (eec30916e78)
- Revert "[Inductor UT] Generalize newly introduced device-bias hard code in (d8a2796fb67)
- Revert "Fix duplicate pattern error (a174ee2255a)
- Revert "Rewrite _reparametrize_module to use `contextmanager` (2409b49a33c)
- Revert "Set `enable_trace_contextlib_contextmanager` flag to True (8d63a4a4092)
- Revert "Use absolute path `path.resolve()` -> `path.absolute()` (99f2491af91)
- Revert "[dynamo][dicts] Guarding lazily on dict keys (b01556bd8a9)
- Revert "[ca] add test_dtensor_compile.py to compiled autograd tests (cb5fa17e443)
- Set `enable_trace_contextlib_contextmanager` flag to True ([#140604](https://github.com/pytorch/pytorch/pull/140604))
- Revert "export AOTI_TORCH_EXPORT on Windows. (aa14fcd96c3)
- Revert "Update torch.masked.mean to upcast dtype for bool tensors (f4e9aebbccd)
- Revert "[AsyncMM] re-enable and prepare for cutlass 3.5.1 update (778d953951f)
- [dtensor] move all tests to distribute/tensor folder ([#144166](https://github.com/pytorch/pytorch/pull/144166))
- Tests Generelization for multiple accelerator devices ([#139184](https://github.com/pytorch/pytorch/pull/139184))
- Revert "[dtensor] move all tests to distribute/tensor folder (6c54963f75e)
- [dtensor] move all tests to distribute/tensor folder ([#144166](https://github.com/pytorch/pytorch/pull/144166))
- Revert "Unskipped multiple inductor tests for ROCm (7d9f26de051)
- fix randint distribution for large max ([#143787](https://github.com/pytorch/pytorch/pull/143787))
- Revert "[Quant][Inductor][X86] Separate binary post op fusion and lowering for qlinear (3797143e065)
- [Submodule] Upgrade to Cutlass 3.6 ([#144180](https://github.com/pytorch/pytorch/pull/144180))
- Revert "[Submodule] Upgrade to Cutlass 3.6 (f71688f30da)
- [Submodule] Upgrade to Cutlass 3.6 ([#144180](https://github.com/pytorch/pytorch/pull/144180))
- Revert "Migrate from Tuple -> tuple in torch/_decomp (87c1f76e630)
- Revert "[dynamo] Avoid graph break on updates to `obj.__dict__` (473b745cb97)
- [AOTI] Add a boxed_run API ([#142213](https://github.com/pytorch/pytorch/pull/142213))
- Revert "Generalize at::manual_seed for all accelerators (db2a30932a1)
- Revert "Fix poision child process issue when call getAccelerator() (b80ecc4457d)
- Revert "Stop ignoring mypy errors in torch/testing/_internal/common_utils.py (3753d302732)
- Revert "[ez] add lint commits to .git-blame-ignore-revs (eaa24821f27)
- Revert "[mps/inductor] Add support for exp(). (4f406d22a2c)
- Revert "Collect packages with importlib in collect_env (0aa34e95919)
- Revert "[AOTI] Add a boxed_run API (4f74864c94e)
- Revert "Stop ignoring mypy errors in torch/testing/_internal/common_utils.py (dfe06e555d4)
- Revert "[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (64bcf39180c)
- Revert "Fix torch.normal ignores default_device (d21738f24a6)
- [AOTI] Add a boxed_run API ([#142213](https://github.com/pytorch/pytorch/pull/142213))
- Revert "Increase C10_COMPILE_TIME_MAX_GPUS to 128 (bdd942efd76)
- Revert "Removed unused _RequiredParameter (154185dcd0e)
- Revert "Enable s8s8s8 for qlinear with mkl-dnn (443de667b1e)
- Add generator parameter to rand*_like functions ([#136780](https://github.com/pytorch/pytorch/pull/136780))
- Revert "Fix global namespace pollution in ATen/Dispatch.h (0f051eaf66b)
- Enable s8s8s8 for qlinear with mkl-dnn ([#139887](https://github.com/pytorch/pytorch/pull/139887))
- Revert "restore rng generation for fbcode (d595b960597)
- Revert "Add tests for different dtypes with max autotune (1c290912e4a)
- Revert "Add flop formula for _scaled_mm (6559374494c)
- Cholesky mps implementation ([#144193](https://github.com/pytorch/pytorch/pull/144193))
- Revert "[mps] Massage test_full_truncation to work only on the supported dtypes. (829c4570ca9)
- Revert "[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (4ea189422de)
- Revert "Cholesky mps implementation (46b92c025d7)
- Revert "[dynamo][dicts] Consolidate dict(..) construction (5e6e6200bfa)
- Revert "cpp_wrapper: Move #includes to per-device header files (94c0f153025)
- Revert "Make functionalization `ViewMeta` serializable with pickle. (6c713ccb5e0)
- Revert "Add tests for different dtypes with max autotune (55b0819beec)
- [dcp] Add ZStandard transformer ([#143360](https://github.com/pytorch/pytorch/pull/143360))
- patch for block-wise quantization + pt2e ([#144492](https://github.com/pytorch/pytorch/pull/144492))
- Revert "patch for block-wise quantization + pt2e (f522502b972)
- Revert "parametrized test name handles class arguments (5802be698ef)
- Revert "PEP585 update - torch/distributed (6374332d339)
- Revert "PEP585 update -  torch/nn torch/optim torch/package torch/profiler torch/serialization torch/sparse torch/xpu (5fd881a5b67)
- Revert "[dcp] Add ZStandard transformer (c6986ca2e12)
- Revert "Fix RMSNorm epsilon value type for BF16 or FP16 (895659cb41c)
- Revert "[triton] Update triton pin to include warp specialization support (2d1649bc2a1)
- Fix triton masked loading for non-block tl.loads ([#144782](https://github.com/pytorch/pytorch/pull/144782))
- [BE]: Simplify set add with set update ([#145152](https://github.com/pytorch/pytorch/pull/145152))
- [compiled autograd] stop specializing on metadata during initial trace ([#143417](https://github.com/pytorch/pytorch/pull/143417))
- [compiled autograd] support Tensor Subclasses in AOTBackward ([#144115](https://github.com/pytorch/pytorch/pull/144115))
- Revert "Enable grep_linter to use -a (dddf52b1b91)
- Revert "[BE]: Simplify set add with set update (6e53588789c)
- Revert "Output of nonzero is transposed, fix fake tensor (f0a210bf5d2)
- Revert "Align CPU behavior with CUDA for `ConvTranspose` when `out_channels=0` (d95a6babcc5)
- Revert "Binary upload checksum (768ad0886ff)
- Enable fp16 linear layers in PyTorch via ACL ([#144992](https://github.com/pytorch/pytorch/pull/144992))
- Revert "Added swizzle searching, disabled fp16 accum, and enabled ping-pong for cutlass (ce4a097bf76)
- Revert "Fix deprecated pytorch_sphinx_theme editable installation (d7b6746470b)
- [BE]: Simplify set add with set update ([#145152](https://github.com/pytorch/pytorch/pull/145152))
- Enable clang-tidy on torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp ([#143806](https://github.com/pytorch/pytorch/pull/143806))
- Revert "[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (dad9bc34610)
- Revert "[compiled_autograd] Rename interface to pyinterface (bf62222d817)
- Revert "[compiled autograd] support Tensor Subclasses in AOTBackward (ab082863a16)
- Revert "[compiled autograd] stop specializing on metadata during initial trace (3f6cfd01564)
- Revert "[compiled autograd] Always proxy autograd.Function nodes; handle AOT backwards (16c4f8c3955)
- Revert "[compiled autograd] Proxy nodes for user-defined C++ torch::autograd::Function (9553301ade6)
- Revert "[compiled autograd] Proxy a node for CopyBackwards into the graph (c3fadacf84f)
- Revert "[compiled autograd] Proxy opaque nodes for built-in autograd nodes (6dd82833817)
- Revert "[dynamo] Log guard latency (6f60c65a3a4)
- Revert "Enable clang-tidy on torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp (6a2b4db0a1e)
- Revert "Add multi env variable support to configs (714f64329ba)
- Enable clang-tidy on torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp ([#143806](https://github.com/pytorch/pytorch/pull/143806))
- Revert "Fix triton masked loading for non-block tl.loads (9d6927715f8)
- Revert "Add generator parameter to rand*_like functions (ad36f4f42c4)
- Fix type annotation of `Linear.bias` ([#142326](https://github.com/pytorch/pytorch/pull/142326))
- [dcp] Add ZStandard transformer ([#143360](https://github.com/pytorch/pytorch/pull/143360))
- Revert "Fix type annotation of `Linear.bias` (09ae69a3648)
- Revert "Align CPU behavior with CUDA for `ConvTranspose` when `out_channels=0` (6a4fb4b6153)
- pickler for GraphModule ([#141659](https://github.com/pytorch/pytorch/pull/141659))
- Revert "[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (c986eba560b)
- Revert "pickler for GraphModule (2de53b3b659)
- Revert "Add option to serialization config to reduce random reads from get_record_offset when loading with mmap=True (90106492928)
- Record inputs at time of tracing, constrain to them for triton fn ([#145448](https://github.com/pytorch/pytorch/pull/145448))
- [Environment Variable][7/N] Use thread-safe getenv functions ([#140211](https://github.com/pytorch/pytorch/pull/140211))
- Revert "Remove lexicographical sorting of storage keys in torch.save (dbef2a9bc93)
- Revert "[inductor][BE] Enable test_cpu_cpp_wrapper in fbcode (cfbb27462eb)
- Revert "[dynamo] save/restore system random state more carefully (3481c2aec4c)
- [c10d] Add NCCL memory allocator ([#145675](https://github.com/pytorch/pytorch/pull/145675))
- Revert "Record inputs at time of tracing, constrain to them for triton fn (0d6343347fb)
- Revert "[Environment Variable][7/N] Use thread-safe getenv functions (284f217011e)
- Revert "inductor.config.descriptive_names = False is not actually supported (e0525dbca9c)
- Revert "[c10d] Add NCCL memory allocator (6371c25b91f)
- Revert "[CD] Install ninja and setuptools from PyPI (b52e8d521e2)
- Revert "[CMake] Find HomeBrew OpenMP on MacOS (b80482988fa)
- Revert "[dynamo] Use polyfill to implement comparison operators (1185b81c514)
- Revert "[ATen][CUDA] Implement 128 bit vectorization v2 (b60120d0df9)
- [c10d] Add NCCL memory allocator ([#145675](https://github.com/pytorch/pytorch/pull/145675))
- Revert "[c10d] Add NCCL memory allocator (5fa28bbe407)
- Revert "Update mi300 labels to account for multiple clusters. (967cf85f3a8)
- Record inputs at time of tracing, constrain to them for triton fn ([#145448](https://github.com/pytorch/pytorch/pull/145448))
- [c10d] Add NCCL memory allocator ([#145675](https://github.com/pytorch/pytorch/pull/145675))
- Revert "[triton] Update pin to tip of 3.2 release (7391cea857b)
- Revert "Advance past fc window for stft center (4280232f214)
- pickler for GraphModule ([#141659](https://github.com/pytorch/pytorch/pull/141659))
- Revert "inductor: Don't throw an internal error when a nn.module is missing a attribute (f5a61ba0a3d)
- Revert "[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (c3f71eb61b4)
- Revert "[inductor/profiler] add kernel kwargs instrumentation (16f44fee258)
- Revert "Tensor .cuda() very slow with specific array sizes (c39c6798131)
- [Environment Variable][7/N] Use thread-safe getenv functions ([#140211](https://github.com/pytorch/pytorch/pull/140211))
- move and fix logic to update unbacked bindings ([#146115](https://github.com/pytorch/pytorch/pull/146115))
- Revert "[hop][inductor] track the dependency on unbacked symbols correctly with constant_args for hops (c0979d72b5f)
- Revert "[ONNX] Create deprecation warning on dynamo_export (64fc9ff09ca)
- Revert "[Environment Variable][7/N] Use thread-safe getenv functions (00dc5b10f69)
- use copy2d in h2d/d2h copy when possible ([#146256](https://github.com/pytorch/pytorch/pull/146256))
- Revert "[inductor] Refactor op handlers part 1 (2f40f789daf)
- Revert "[inductor] Refactor CSEProxy into global scope (0061eb5b70f)
- Revert "[inductor] Finish typing common.py (ecbc725fad8)
- Revert "[inductor] Add typing to common.CSE (d3c7e4bb9cb)
- Revert "[inductor] Add typing to common.KernelArgs (7f796eb8b7e)
- Revert "[aoti] Assign proxy call args by name, and support default values. (106acf0eec8)
- Revert "add support for capturing provenance of unary operations (658e22d4956)
- Revert "[Testing] Reduce `test_exp` flakiness (3c0d2bc262b)
- Revert "[cutlass backend] fix bug for accuminator dtype (f35e60b21c8)
- Revert "move and fix logic to update unbacked bindings (f242da41c7c)
- Revert "[inductor] use ftz variant of exp (282d185ec19)
- Revert "Move get accelerator to use build time flags when possible (f27220e32af)
- Revert "[inductor] Pre-populate cache for simplify_with_ranges return value (9555bfce88f)
- Revert "[inductor] Refactor CaptureIndexing into global scope (7dc5cfe2ad3)
- Revert "[inductor] Minor compile time optimizations in DefaultHandler (93e1e6e07cb)
- Revert "[inductor] Refactor op handlers part 5 (49effa0deb4)
- Revert "[inductor] Refactor op handlers part 4 (68304dba7ac)
- Revert "[inductor] Refactor op handlers part 3 (2001066c617)
- Revert "[inductor] Refactor op handlers part 2 (e0cf519adec)
- [dynamo] check for incompatible configs ([#146513](https://github.com/pytorch/pytorch/pull/146513))
- Revert "[DTensor][Test] Create a simple unit test for tensordot (bd7d4fb2b5f)
- Revert "[dynamo] check for incompatible configs (1b79d476353)
- Revert "[while_loop][inductor] support sym expression as cond_fn output (076717785c1)
- windows Magma build for cu128 ([#146653](https://github.com/pytorch/pytorch/pull/146653))
- Revert "windows Magma build for cu128 (f17109bd96d)
- Revert "[cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces (80a16966795)
- move and fix logic to update unbacked bindings ([#146115](https://github.com/pytorch/pytorch/pull/146115))
- [dynamo] check for incompatible configs ([#146513](https://github.com/pytorch/pytorch/pull/146513))
- windows Magma build for cu128 ([#146653](https://github.com/pytorch/pytorch/pull/146653))
- Revert "[ONNX] Adjust and add deprecation messages (1557b7bf9a8)
- Revert "[ONNX] Create deprecation warning on dynamo_export (6aa924af688)
- Revert "windows Magma build for cu128 (b8261358ca8)
- windows Magma build for cu128 ([#146653](https://github.com/pytorch/pytorch/pull/146653))
- Exclude upsample_bilinear2d.vec from default core ATen decomposition table ([#141791](https://github.com/pytorch/pytorch/pull/141791))
- Revert "cpp_wrapper: Precompile device-specific header files (2fafcd37c38)
- Revert "move and fix logic to update unbacked bindings (f38f1dcd825)
- Replace is_same with is_same_v for concise syntax ([#145450](https://github.com/pytorch/pytorch/pull/145450))
- Revert "Exclude upsample_bilinear2d.vec from default core ATen decomposition table (fe94ece375f)
- Implement cuda graphs implementation of torch.cond and torch.while_loop ([#140979](https://github.com/pytorch/pytorch/pull/140979))
- Revert "Introduce new template heuristic for triton autotune configs (443437648a8)
- Revert "Update octokit/request-action to 2.4.0 (6105b6f15f8)
- Revert "Replace is_same with is_same_v for concise syntax (ce80865f134)
- Revert "[cutlass backend] Do not change dtype of GEMM template (8a975cb247d)
- Replace is_same with is_same_v for concise syntax ([#145450](https://github.com/pytorch/pytorch/pull/145450))
- Revert "Use 2022 as default VC_YEAR for windows builds (938209fb6ff)
- Revert "[cond] make cond re-dispatch in proxy mode (65e8862b9a8)
- Revert "Implement cuda graphs implementation of torch.cond and torch.while_loop (9a883007a2f)
- Add torch._scaled_mm for CPU ([#139975](https://github.com/pytorch/pytorch/pull/139975))
- Revert "update kineto submodule (059dfe20818)
- Revert "Nccl update to 2.25.1 for cuda 12.4-12.8  (e06ee4aa9fb)
- Revert "Add torch._scaled_mm for CPU (aac5d1a2890)
- Revert "[cutlass backend] Do not change dtype of GEMM template (5517eb4398b)
- Fix triton masked loading for non-block tl.loads ([#144782](https://github.com/pytorch/pytorch/pull/144782))
- Revert "Enable fp16 linear layers in PyTorch via ACL (6ca5c22e316)
- xpu: support sycl with torch.utils.cpp_extension APIs ([#132945](https://github.com/pytorch/pytorch/pull/132945))
- Revert "xpu: support sycl with torch.utils.cpp_extension APIs (dd5d0ea6bb0)
- xpu: support sycl with torch.utils.cpp_extension APIs ([#132945](https://github.com/pytorch/pytorch/pull/132945))
- Add torch._scaled_mm for CPU ([#139975](https://github.com/pytorch/pytorch/pull/139975))
- Revert "Add torch._scaled_mm for CPU (49e8f9c9651)
- Add torch._scaled_mm for CPU ([#139975](https://github.com/pytorch/pytorch/pull/139975))
- Revert "Fix non-bitwise type annotations for Tensor operators (see #145838) (302f56a1f2d)
- Revert "Nccl update to 2.25.1 for cuda 12.4-12.8  (7622e29a374)
- Revert "Add torch._scaled_mm for CPU (babb2dc2afd)
- Revert "Add cifllow/riscv64 label" (ead970c8d03)
- Revert "Increase memory for linux binary builds  (e5da9df421f)
- Revert "Build a storage reader/writer to write checkpoints in HF format (3395da7f7cb)
- Revert "[Inductor][Triton] Rework casting logic to avoid illegal bitcast (05e6f15966d)
- Revert "[trymerge] Post initial starting merge comment on stacked PRs (ef6b16ea9d2)
- [ROCm] OCP FP8 Support for new GPUs ([#146632](https://github.com/pytorch/pytorch/pull/146632))
- Revert "Delete Mixed MM Special Casing (3409cbd1770)
- Revert "[ROCm] Implemented dropout usage for RNN with MIOpen backend (bea72180ed7)
- Revert "[cuDNN][SDPA][Nested Tensor] Experimental cuDNN Nested Tensor SDPA Support (forward only) (fa8e3a28a7b)
- Revert "[ROCm] OCP FP8 Support for new GPUs (3e2d9d079e4)
- Revert "[TorchRec][PT2] disable contextlib in PT2 train pipeline (5b6ad682bc3)
- Revert "[ROCm] Update periodic.yml to use 2GPU runners (900a7747813)
- [ROCm] OCP FP8 Support for new GPUs ([#146632](https://github.com/pytorch/pytorch/pull/146632))
- Revert "Upgrade submodule oneDNN to v3.7 (e72b4c61bf1)
- Revert "[AOTI][refactor] Replace run_command_and_check with CppBuilder.build (9b06b304686)
- Revert "[AOTI][refactor] Rename use_absolute_path to use_relative_path (890213f65f9)
- Revert "use copy2d in h2d/d2h copy when possible (fb73b0c7c55)
- [ca] trace saved variable unpacking ([#147242](https://github.com/pytorch/pytorch/pull/147242))
- [ca] side-effect free initial trace: GraphTask ([#147796](https://github.com/pytorch/pytorch/pull/147796))
- [ca] side-effect free inital trace: compiled_args ([#147804](https://github.com/pytorch/pytorch/pull/147804))
- Revert "[inductor][triton] Ignore block ptr advances for removed buffers (0d31c621a37)
- Revert "Add option to limit number of SMs used by matmul kernels (1e894d26352)
- Revert "follow up to #147548, fix regression on MI300 (05bc8fe62e3)
- Revert "torch._scaled_mm with MXFP8 (c82c1411c61)
- Revert "[ca] side-effect free inital trace: compiled_args (143f0f0006f)
- Revert "[ca] side-effect free initial trace: GraphTask (4d614baa301)
- Revert "[ca] trace saved variable unpacking (90e3a3d86d6)
- Revert "[AOTI][refactor] Consolidate CppBuilder.build and CppBuilder.build_fbcode_cpu_re (acca9b9cb0a)
- Revert "torch._scaled_mm with MXFP8 (a84db75e1ba)
- [ca] trace saved variable unpacking ([#147242](https://github.com/pytorch/pytorch/pull/147242))
- [ca] side-effect free initial trace: GraphTask ([#147796](https://github.com/pytorch/pytorch/pull/147796))
- [ca] side-effect free inital trace: compiled_args ([#147804](https://github.com/pytorch/pytorch/pull/147804))
- Revert "[do not merge yet] update grammar  (dc7556f1bdb)
- Revert "[do not merge yet] update grammar  (7e7d05bf85f)
- Revert "[dynamo] add sourceless builder for `types.MethodType` (915eb012e1d)
- Support torch.compile rng selective activation checkpointing with cudagraph ([#146878](https://github.com/pytorch/pytorch/pull/146878))
- Revert "Support torch.compile rng selective activation checkpointing with cudagraph (17358ce7788)
- Revert "Build a storage reader/writer to write checkpoints in HF format (c622796cde9)
- Revert "optimize the decomposition of aten.native_group_norm (644d84d594b)
- Support torch.compile rng selective activation checkpointing with cudagraph ([#146878](https://github.com/pytorch/pytorch/pull/146878))
- Revert "Remove NO_MULTIPROCESSING_SPAWN checks (926b7b5027c)
- Initial implementation of host memory stats ([#147660](https://github.com/pytorch/pytorch/pull/147660))
- [async TP] insert reshape node to handle "reshape -> scaled mm -> reshape pattern" in async TP with rowwise scales ([#148001](https://github.com/pytorch/pytorch/pull/148001))
- Revert "[async TP] insert reshape node to handle "reshape -> scaled mm -> reshape pattern" in async TP with rowwise scales (ebc3f27bf41)
- Revert "introduce dynamism library (baf1c8fcdcc)
- [async TP] insert reshape node to handle "reshape -> scaled mm -> reshape pattern" in async TP with rowwise scales ([#148001](https://github.com/pytorch/pytorch/pull/148001))
- Revert "[async TP] insert reshape node to handle "reshape -> scaled mm -> reshape pattern" in async TP with rowwise scales (191c9bd013c)
- Revert "Initial implementation of host memory stats (a983b2b11ad)
- Revert "introduce dynamism library (8e004865dd5)
- Revert "stage 1 of depreate silent fallback of tuning gemm (1919e0de9aa)
- [async TP] insert reshape node to handle "reshape -> scaled mm -> reshape pattern" in async TP with rowwise scales ([#148001](https://github.com/pytorch/pytorch/pull/148001))
- [c10d] Add hccl distributed backend to c10d data structures ([#146478](https://github.com/pytorch/pytorch/pull/146478))
- Revert "[c10d] Add hccl distributed backend to c10d data structures (94afb165d94)
- [fx] Move map_aggregate to C++ ([#148243](https://github.com/pytorch/pytorch/pull/148243))
- [fx] Move Node._update_args_kwargs to C++ ([#148260](https://github.com/pytorch/pytorch/pull/148260))
- [fx] Move Node._prepend/Node._remove_from_list to C++ ([#148261](https://github.com/pytorch/pytorch/pull/148261))
- Revert "[import][inductor] Simplify grid handling (608377d3418)
- Subprocess compile ([#146134](https://github.com/pytorch/pytorch/pull/146134))
- [c10d] Add hccl distributed backend to c10d data structures ([#146478](https://github.com/pytorch/pytorch/pull/146478))
- [fx] Optimizations for node name generation ([#148288](https://github.com/pytorch/pytorch/pull/148288))
- [fx] Optimize TracerBase.create_arg and Graph._gen_python_code ([#148292](https://github.com/pytorch/pytorch/pull/148292))
- add supports_coalescing property in c10d::Backend  to determine whether backend supports coalescing ([#135338](https://github.com/pytorch/pytorch/pull/135338))
- Revert "[fx] Optimize TracerBase.create_arg and Graph._gen_python_code (ed9055c303e)
- Revert "[fx] Optimizations for node name generation (611b0e9bc4e)
- Revert "Bump onnxscript to 0.2.2 in CI (9d196edb7d1)
- Revert "[Inductor] Record Triton’s Base32 Cache Key in `.best_config` for Debugging (63778cb8a03)
- Revert "Better log message to update pr_time_benchmarks/expected_results.csv (6fb18ff6850)
- Revert "[fx] Move Node._prepend/Node._remove_from_list to C++ (97b9e68bc65)
- Revert "[fx] Move Node._update_args_kwargs to C++ (17d003fe75c)
- Revert "[fx] Move map_aggregate to C++ (92beda54c87)
- Initial implementation of host memory stats ([#147660](https://github.com/pytorch/pytorch/pull/147660))
- Revert "[dtensor] add aten._scaled_dot_product_cudnn_attention.default op support (c9edd37ffb8)
- Revert "Subprocess compile (897fd9b5142)
- Revert "[cutlass backend] Forward fix for less aligned gemm shapes (ae6bb584838)
- Revert "[Inductor] Avoid tensor slice overflow for large step (841451af9f8)
- Revert "[cutlass backend] fix assertion that prevent self multiplication  (28b68b46bc9)
- Revert "[ROCm] Bump AOTriton to 0.9.1b (96176e32a92)
- Fix `torch.nn.functional.hardswish` gradients corner case ([#148049](https://github.com/pytorch/pytorch/pull/148049))
- Revert "Enable onednn in pytorch for ppc64le architecture (cf9efbdf16c)
- Revert "Fix `torch.nn.functional.hardswish` gradients corner case (abcca2fcbbb)
- Revert "Move get accelerator to use build time flags when possible (b246cd7b820)
- Revert "Remove Cuda 12.4 from nightly Binaries  (99da439d109)
- Revert "[cutlass backend] fix assertion that prevent self multiplication  (bb94b65da77)
- Revert "[cutlass backend] Forward fix for less aligned gemm shapes (0f852641c29)
- [PGNCCL] Launch kernel on current stream & remove `record_stream` entirely ([#148590](https://github.com/pytorch/pytorch/pull/148590))
- Revert "[PGNCCL] Launch kernel on current stream & remove `record_stream` entirely (9cb25f0ea28)
- [PGNCCL] Launch kernel on current stream & remove `record_stream` entirely ([#148590](https://github.com/pytorch/pytorch/pull/148590))
- Revert "[dynamo] allow global import `from collections import deque` in user code (19a39a7a064)
- Move aoti_torch_cpu__weight_int4pack_mm_cpu_tensor to not be mangled ([#148834](https://github.com/pytorch/pytorch/pull/148834))
- [fx] Move map_aggregate to C++ ([#148243](https://github.com/pytorch/pytorch/pull/148243))
- [fx] Move Node._update_args_kwargs to C++ ([#148260](https://github.com/pytorch/pytorch/pull/148260))
- [fx] Move Node._prepend/Node._remove_from_list to C++ ([#148261](https://github.com/pytorch/pytorch/pull/148261))
- [fx] Optimizations for node name generation ([#148288](https://github.com/pytorch/pytorch/pull/148288))
- [fx] Optimize TracerBase.create_arg and Graph._gen_python_code ([#148292](https://github.com/pytorch/pytorch/pull/148292))
- Revert "Move aoti_torch_cpu__weight_int4pack_mm_cpu_tensor to not be mangled (2ec9aceaeb7)
- Revert "[pytree] add APIs to determine a class is a namedtuple or PyStructSequence (ebd087e4b59)
- Revert "[PGNCCL] Launch kernel on current stream & remove `record_stream` entirely (a95eb0c0a7d)
- [WIP] Initial implementation of Grouped Gemm API ([#148531](https://github.com/pytorch/pytorch/pull/148531))
- Revert "[WIP] Initial implementation of Grouped Gemm API (c983e1124cd)
- Revert "Refactor `test/test_torch.py` by moving testcase to `test_indexing.py` (16560d4e8f8)
- [PGNCCL] Launch kernel on current stream & remove `record_stream` entirely ([#148590](https://github.com/pytorch/pytorch/pull/148590))
- Revert "Use the device interface for detecting Triton availability (c916a8efc54)
- Revert "[logging] Set compile_id in the CachingAutotuner during compilation so we have it for dynamo_timed logging (b54cf1a2814)
- [WIP] Initial implementation of Grouped Gemm API ([#148531](https://github.com/pytorch/pytorch/pull/148531))
- Make dynamism code robust to NotImplementedException ([#148823](https://github.com/pytorch/pytorch/pull/148823))
### not user facing
- skip test dynamo for aot_dispatch tests on ci ([#142185](https://github.com/pytorch/pytorch/pull/142185))
- [hop][dynamo] support torch.SymInt inputs ([#141524](https://github.com/pytorch/pytorch/pull/141524))
- Fix undesired specialization on slice after split. ([#142372](https://github.com/pytorch/pytorch/pull/142372))
- Don't install lintrunner on S390 ([#142876](https://github.com/pytorch/pytorch/pull/142876))
- [TorchGen] Simplify argument_type_str ([#142491](https://github.com/pytorch/pytorch/pull/142491))
- [TorchGen] Remove cpp_type_registration_declarations ([#142452](https://github.com/pytorch/pytorch/pull/142452))
- [TorchGen] remove remove_non_owning_ref_types from valuetype_type ([#142449](https://github.com/pytorch/pytorch/pull/142449))
- filelock: Make waitcounter variant to use ([#139816](https://github.com/pytorch/pytorch/pull/139816))
- E2E composability testing ([#141398](https://github.com/pytorch/pytorch/pull/141398))
- Remove unneeded optional dereference ([#141578](https://github.com/pytorch/pytorch/pull/141578))
- [ROCm] Prune old gfx archs gfx900/gfx906 from binaries ([#142827](https://github.com/pytorch/pytorch/pull/142827))
- Hide torch_python symbols ([#142214](https://github.com/pytorch/pytorch/pull/142214))
- [aot] Functionalize aot backward prologue and epilogue wrappers ([#142415](https://github.com/pytorch/pytorch/pull/142415))
- Fix NJT backward tests ([#143072](https://github.com/pytorch/pytorch/pull/143072))
- [Easy] factor out inductor ophandler decompositions ([#142400](https://github.com/pytorch/pytorch/pull/142400))
- Clang-format aten/src/ATen/native/Tensor*{cpp,h} ([#143089](https://github.com/pytorch/pytorch/pull/143089))
- [ca] set autograd graph task state ([#143108](https://github.com/pytorch/pytorch/pull/143108))
- add float_args benchmark ([#143143](https://github.com/pytorch/pytorch/pull/143143))
- add README.md for compile time benchmarks ([#143145](https://github.com/pytorch/pytorch/pull/143145))
- [dynamo] Support multiple inheritance for custom dict construction ([#142416](https://github.com/pytorch/pytorch/pull/142416))
- [ca] fix flex attention backward HOP capture in initial graph ([#143155](https://github.com/pytorch/pytorch/pull/143155))
- [BE] Stop using deprecated APIs in mkldnn_pattern_matcher ([#143156](https://github.com/pytorch/pytorch/pull/143156))
- Require Config to have a default ([#143150](https://github.com/pytorch/pytorch/pull/143150))
- Add type to Config ([#143151](https://github.com/pytorch/pytorch/pull/143151))
- Migrate compiler config to Config ([#143152](https://github.com/pytorch/pytorch/pull/143152))
- dynamo tracing perf: Guard slots: 51.76 -> 51.34 ([#143060](https://github.com/pytorch/pytorch/pull/143060))
- [associative_scan] patch inductor tests to always run with static shape ([#143161](https://github.com/pytorch/pytorch/pull/143161))
- [CUDA] Follow up to clean up some `set_per_process_memory_fraction` usage in tests ([#142811](https://github.com/pytorch/pytorch/pull/142811))
- Add a pass which analyzes whether a prologue preserves zero mask ([#142401](https://github.com/pytorch/pytorch/pull/142401))
- Infer whether prologues can be computed without upcasting to fp32 without changing numerics ([#142402](https://github.com/pytorch/pytorch/pull/142402))
- move function before modifying it ([#143202](https://github.com/pytorch/pytorch/pull/143202))
- dont attempt to fuse in unaligned accesses to mm ([#142435](https://github.com/pytorch/pytorch/pull/142435))
- add additional CK BMM Instances (2) ([#142874](https://github.com/pytorch/pytorch/pull/142874))
- [Inductor UT] Generalize device-bias code in test_triton_syntax.py. ([#143178](https://github.com/pytorch/pytorch/pull/143178))
- Reraise worker errors as runtime errors in more cases when the original exception can't be constructed ([#140911](https://github.com/pytorch/pytorch/pull/140911))
- Update low prec codegen for div/mod ([#142350](https://github.com/pytorch/pytorch/pull/142350))
- Use static initialization to avoid once_flag in getCUDAHooks ([#143198](https://github.com/pytorch/pytorch/pull/143198))
- Add typechecking indirection for Config ([#143229](https://github.com/pytorch/pytorch/pull/143229))
- Migrate compiler config to Config ([#143152](https://github.com/pytorch/pytorch/pull/143152))
- [ca] re-enable disabled tests ([#143247](https://github.com/pytorch/pytorch/pull/143247))
- [AMD] Turn on TF32 for aten::mm ([#139869](https://github.com/pytorch/pytorch/pull/139869))
- Hide torch_python symbols ([#142214](https://github.com/pytorch/pytorch/pull/142214))
- [RELAND] Add UTs for accelerator device-agnostic runtime APIs ([#133572](https://github.com/pytorch/pytorch/pull/133572))
- Update slow tests ([#143278](https://github.com/pytorch/pytorch/pull/143278))
- [reland][dynamo][guards] Consider tensors as immutable for dict tag matches ([#141085](https://github.com/pytorch/pytorch/pull/141085))
- Add config alias ([#142088](https://github.com/pytorch/pytorch/pull/142088))
- [BE] Revert "Add conda to Manylinux Docker images (#139903)" ([#143300](https://github.com/pytorch/pytorch/pull/143300))
- update aten bmm CK heuristic ([#143294](https://github.com/pytorch/pytorch/pull/143294))
- Update low prec codegen for div/mod ([#142350](https://github.com/pytorch/pytorch/pull/142350))
- Add missing IValue overloads for SymInt lists ([#143167](https://github.com/pytorch/pytorch/pull/143167))
- remove allow-untyped-defs for torch/utils/_stats.py ([#143319](https://github.com/pytorch/pytorch/pull/143319))
- remove allow-untyped-defs for torch/__config__.py ([#143320](https://github.com/pytorch/pytorch/pull/143320))
- [foreach-map] Add tests for backward ([#143282](https://github.com/pytorch/pytorch/pull/143282))
- [ROCm] CK Flash Attention Backend ([#138947](https://github.com/pytorch/pytorch/pull/138947))
- [Inductor] inplace padding ([#140249](https://github.com/pytorch/pytorch/pull/140249))
- non strict sequential slicing ([#143298](https://github.com/pytorch/pytorch/pull/143298))
- make it clearer (in docs) one can double decorate with torch.library.impl_* APIs ([#137608](https://github.com/pytorch/pytorch/pull/137608))
- [AOTI] Add is_big_gpu checking to test_conv3d ([#143339](https://github.com/pytorch/pytorch/pull/143339))
- remove allow-untyped-defs for torch/masked/maskedtensor/creation.py ([#143321](https://github.com/pytorch/pytorch/pull/143321))
- remove allow-untyped-defs for torch/_C/_lazy.pyi ([#143370](https://github.com/pytorch/pytorch/pull/143370))
- remove allow-untyped-defs for torch/utils/benchmark/examples/simple_timeit.py ([#143368](https://github.com/pytorch/pytorch/pull/143368))
- remove allow-untyped-defs for torch/_lazy/device_context.py ([#143367](https://github.com/pytorch/pytorch/pull/143367))
- Fix unused variables in test_serialize_sym_float ([#143389](https://github.com/pytorch/pytorch/pull/143389))
- Remove stable_partition for ARM AOTI Runtimes ([#142394](https://github.com/pytorch/pytorch/pull/142394))
- [codemod] Decorate unused variables with `[[maybe_unused]]` ([#143381](https://github.com/pytorch/pytorch/pull/143381))
- Add 2 more APIs to the exposed public torch python APIs ([#143380](https://github.com/pytorch/pytorch/pull/143380))
- tools: Add a tool to build wheels for multiple python versions ([#143361](https://github.com/pytorch/pytorch/pull/143361))
- [MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage ([#143347](https://github.com/pytorch/pytorch/pull/143347))
- NJT linear_backward should not return inner tensor as-is ([#143333](https://github.com/pytorch/pytorch/pull/143333))
- Fix sample inputs leaked from subtest ([#143415](https://github.com/pytorch/pytorch/pull/143415))
- Fix unused Python variables in test/nn ([#143396](https://github.com/pytorch/pytorch/pull/143396))
- Record min/max of integral tensor in ET ([#143088](https://github.com/pytorch/pytorch/pull/143088))
- [ATen][Native][Special] Hermite polynomial prematurely return NaN if n is high ([#141955](https://github.com/pytorch/pytorch/pull/141955))
- remove allow-untyped-defs for torch/nn/parallel/__init__.py ([#143437](https://github.com/pytorch/pytorch/pull/143437))
- remove allow-untyped-defs for torch/_functorch/batch_norm_replacement.py ([#143438](https://github.com/pytorch/pytorch/pull/143438))
- Fix unused variables in test/test_transformers.py ([#143407](https://github.com/pytorch/pytorch/pull/143407))
- [3/N][Memory Profiling] Add memory profiling function for MTIA hooks ([#142149](https://github.com/pytorch/pytorch/pull/142149))
- Various fix for memory leak in test autograd and dataloader ([#143323](https://github.com/pytorch/pytorch/pull/143323))
- Add tests for non divisible inputs for flex decoding ([#143214](https://github.com/pytorch/pytorch/pull/143214))
- Remove iOS folder ([#143398](https://github.com/pytorch/pytorch/pull/143398))
- add grad_output shape check for adaptive_max_pool2d_backward and adaptive_max_pool3d_backward ([#141663](https://github.com/pytorch/pytorch/pull/141663))
- Fix unused variables in test/torch.py ([#143399](https://github.com/pytorch/pytorch/pull/143399))
- [1/N] Avoid const_cast ([#143169](https://github.com/pytorch/pytorch/pull/143169))
- [export] don't decompose custom triton op when exporting ([#142426](https://github.com/pytorch/pytorch/pull/142426))
- Skip test_conv2d_linear_add_broadcast_shapes_cpu on fbcode ([#143530](https://github.com/pytorch/pytorch/pull/143530))
- Update release matrix for 2.6 ([#143538](https://github.com/pytorch/pytorch/pull/143538))
- add batch_size check for max_pool2d_backward ([#141657](https://github.com/pytorch/pytorch/pull/141657))
- Upgrade submodule ideep for bf16f32 matmul changes ([#143508](https://github.com/pytorch/pytorch/pull/143508))
- leaking c++ singleton specifically ([#143509](https://github.com/pytorch/pytorch/pull/143509))
- [reland][dynamo][guards] Consider tensors as immutable for dict tag matches ([#141085](https://github.com/pytorch/pytorch/pull/141085))
- update expected results ([#143586](https://github.com/pytorch/pytorch/pull/143586))
- [cond] Change Autograd for cond ([#142518](https://github.com/pytorch/pytorch/pull/142518))
- add grad_output shape check for fractional_max_pool2d_backward ([#141666](https://github.com/pytorch/pytorch/pull/141666))
- remove allow-untyped-defs from torch/_lazy/config.py ([#143603](https://github.com/pytorch/pytorch/pull/143603))
- update kineto to XPU Windows fixed PR. [submodule kineto] ([#143445](https://github.com/pytorch/pytorch/pull/143445))
- [BE] Add a test to ensure grads are never inplaced into accidentally ([#143612](https://github.com/pytorch/pytorch/pull/143612))
- [user triton] Raise an exception when encountering nested @triton.autotune decorators or @triton.heuristics ([#143519](https://github.com/pytorch/pytorch/pull/143519))
- [DynamoBench] Handle accuracy results in benchmark records ([#143611](https://github.com/pytorch/pytorch/pull/143611))
- fix formatting in programming model doc ([#143587](https://github.com/pytorch/pytorch/pull/143587))
- export AOTI_TORCH_EXPORT on Windows. ([#140030](https://github.com/pytorch/pytorch/pull/140030))
- Fix false positive from f-strings in set_linter ([#143628](https://github.com/pytorch/pytorch/pull/143628))
- Rewrite _reparametrize_module to use `contextmanager` ([#138203](https://github.com/pytorch/pytorch/pull/138203))
- fix typo in autocast header ([#143625](https://github.com/pytorch/pytorch/pull/143625))
- test/dynamo/test_utils: logging - Stop testing for impossible things. ([#143535](https://github.com/pytorch/pytorch/pull/143535))
- (MTIA) Move "empty_cache" API ([#143402](https://github.com/pytorch/pytorch/pull/143402))
- FlexAttention Benchmark ([#139665](https://github.com/pytorch/pytorch/pull/139665))
- [GPT-fast] Support run spcific model or micro-benchmark ([#143607](https://github.com/pytorch/pytorch/pull/143607))
- Apply clang-format for ATen/core/dispatch headers ([#143620](https://github.com/pytorch/pytorch/pull/143620))
- [pytorch/et] Allow ET to save additional resources for completing a trace like generated kernels and index tensor data ([#143430](https://github.com/pytorch/pytorch/pull/143430))
- Fix test_serialization_zipfile_actually_jit when weights_only is not default ([#143668](https://github.com/pytorch/pytorch/pull/143668))
- [logging] A few fixes/updates to record_compilation_metrics ([#143332](https://github.com/pytorch/pytorch/pull/143332))
- [BE] Remove gcc-5 workaround for unused args ([#143685](https://github.com/pytorch/pytorch/pull/143685))
- [1/n] Support Dynamic Memory Budget in Auto AC ([#143539](https://github.com/pytorch/pytorch/pull/143539))
- [reland][AMD] Turn on TF32 for aten::mm ([#143549](https://github.com/pytorch/pytorch/pull/143549))
- Better fix for f-strings in set_linter for py3.12 ([#143725](https://github.com/pytorch/pytorch/pull/143725))
- Add FP8 support for eye ([#139974](https://github.com/pytorch/pytorch/pull/139974))
- Apply TorchFix TOR203 fixes ([#143691](https://github.com/pytorch/pytorch/pull/143691))
- [logging] A few fixes/updates to record_compilation_metrics ([#143332](https://github.com/pytorch/pytorch/pull/143332))
- [dynamo] Remove DICT_SUBCLASS_GUARD_MANAGER and use dict.keys ([#143722](https://github.com/pytorch/pytorch/pull/143722))
- [dynamo] Remove dead code after introducing UserDefinedDictVariable ([#143699](https://github.com/pytorch/pytorch/pull/143699))
- Simplify host_softmax ([#143251](https://github.com/pytorch/pytorch/pull/143251))
- [cumsum][CUDA][64-bit indexing] Add 64-bit indexing path for `cumsum` ([#143696](https://github.com/pytorch/pytorch/pull/143696))
- [Easy] Fix todo by enable tests for cuda ([#143637](https://github.com/pytorch/pytorch/pull/143637))
- [Inductor XPU] Support max-autotune on XPU and reuse the corresponding Inductor UT. ([#143266](https://github.com/pytorch/pytorch/pull/143266))
- Add FP8 support for eye ([#139974](https://github.com/pytorch/pytorch/pull/139974))
- Enabled force_shape_pad for test_pad_mm and test_slice_mm_bandwidth_computation ([#141768](https://github.com/pytorch/pytorch/pull/141768))
- [17/N] Fix extra warnings brought by clang-tidy-17 ([#143804](https://github.com/pytorch/pytorch/pull/143804))
- Sort requirements.txt ([#143778](https://github.com/pytorch/pytorch/pull/143778))
- [Submodule] Bump libfmt to 11.1.0 ([#143843](https://github.com/pytorch/pytorch/pull/143843))
- [Easy] Bump CUDA nightly version to 11.8 / 12.4 / 12.6 in nightly pull tool ([#143263](https://github.com/pytorch/pytorch/pull/143263))
- [pytorch/et] Allow ET to save additional resources for completing a trace like generated kernels and index tensor data ([#143775](https://github.com/pytorch/pytorch/pull/143775))
- [dynamo] Remove DICT_SUBCLASS_GUARD_MANAGER and use dict.keys ([#143722](https://github.com/pytorch/pytorch/pull/143722))
- [dynamo] Remove dead code after introducing UserDefinedDictVariable ([#143699](https://github.com/pytorch/pytorch/pull/143699))
- Fix duplicate pattern error ([#139321](https://github.com/pytorch/pytorch/pull/139321))
- [Easy] add quotes to shell activation commands ([#143902](https://github.com/pytorch/pytorch/pull/143902))
- [EZ] Update sympy to 1.13.3 ([#143908](https://github.com/pytorch/pytorch/pull/143908))
- remove allow-untyped-defs from utils/tensorboard/_convert_np.py ([#143918](https://github.com/pytorch/pytorch/pull/143918))
- restore 'unused' variable to fix test_cuda_device_memory_allocated ([#143885](https://github.com/pytorch/pytorch/pull/143885))
- [CUDA][CUDA graphs][RNG] Skip replay prologue if `wholegraph_increment` is 0 ([#143777](https://github.com/pytorch/pytorch/pull/143777))
- Add AOT inductor support for _scaled_mm for CPU ([#141961](https://github.com/pytorch/pytorch/pull/141961))
- remove allow-untyped-defs from torch/_size_docs.py ([#143942](https://github.com/pytorch/pytorch/pull/143942))
- [inductor] Make generated kernels deterministic ([#143951](https://github.com/pytorch/pytorch/pull/143951))
- Update torch-xpu-ops commit pin ([#143853](https://github.com/pytorch/pytorch/pull/143853))
- Update slow tests ([#143745](https://github.com/pytorch/pytorch/pull/143745))
- [Inductor UT] Generalize newly introduced device-bias hard code in ([#143975](https://github.com/pytorch/pytorch/pull/143975))
- [BE][Ez]: Update fmtlib submodule to 1.11.1 ([#143937](https://github.com/pytorch/pytorch/pull/143937))
- Fix flaky "Upload test stats" job ([#143991](https://github.com/pytorch/pytorch/pull/143991))
- [inductor] Make generated kernels deterministic ([#143951](https://github.com/pytorch/pytorch/pull/143951))
- [Inductor XPU] Support max-autotune on XPU and reuse the corresponding Inductor UT. ([#143266](https://github.com/pytorch/pytorch/pull/143266))
- Fix a bug for wrong stride in fake tensor ([#141427](https://github.com/pytorch/pytorch/pull/141427))
- Enable mkldnn pattern matcher tests for BF16 on AArch64 ([#144030](https://github.com/pytorch/pytorch/pull/144030))
- Add networkx as bazel dep to fix CI failure ([#143995](https://github.com/pytorch/pytorch/pull/143995))
- remove allow-untyped-defs from torch/mps/event.py ([#144092](https://github.com/pytorch/pytorch/pull/144092))
- remove allow-untyped-defs from utils/_import_utils.py ([#144089](https://github.com/pytorch/pytorch/pull/144089))
- [ROCm][Windows] Disable roctracer-related code ([#143329](https://github.com/pytorch/pytorch/pull/143329))
- [AOTI] Remove more AOTI_TORCH_EXPORT ([#144081](https://github.com/pytorch/pytorch/pull/144081))
- export AOTI_TORCH_EXPORT on Windows. ([#140030](https://github.com/pytorch/pytorch/pull/140030))
- [ROCm] Fix for ld failed to convert GOTPCREL relocation in PyTorch build ([#143986](https://github.com/pytorch/pytorch/pull/143986))
- AOTI fallback ops: remove ops that were never codegen'ed ([#143421](https://github.com/pytorch/pytorch/pull/143421))
- [while_loop][aot] auto-unspecialize int input and output to unbacked symints ([#143105](https://github.com/pytorch/pytorch/pull/143105))
- [ez] Use strip for arg sanitization in upload_metadata_file to improve readability ([#144155](https://github.com/pytorch/pytorch/pull/144155))
- [ScaledMM] Fix NaNs in test for garbage input data ([#144042](https://github.com/pytorch/pytorch/pull/144042))
- Make whl metadata public readable ([#144164](https://github.com/pytorch/pytorch/pull/144164))
- remove allow-untyped-defs from nn/utils/_deprecation_utils.py ([#144136](https://github.com/pytorch/pytorch/pull/144136))
- [Inductor UT] Generalize device-bias code in test_torchinductor.py introduced by #143884. ([#144057](https://github.com/pytorch/pytorch/pull/144057))
- [ROCm] Get rid of extra rpath-link that was needed for libtinfo. ([#143348](https://github.com/pytorch/pytorch/pull/143348))
- [ROCm][Windows] Fix export macros ([#144098](https://github.com/pytorch/pytorch/pull/144098))
- [Submodule] Bump Cutlass to 3.5.1 OSS PR ([#144000](https://github.com/pytorch/pytorch/pull/144000))
- [dynamo][dicts] Guarding lazily on dict keys ([#143997](https://github.com/pytorch/pytorch/pull/143997))
- [mps/BE] Fix linter warning/advice. ([#144199](https://github.com/pytorch/pytorch/pull/144199))
- [EZ][BE] Cleanup `test_mps_basic` ([#144194](https://github.com/pytorch/pytorch/pull/144194))
- Add check for unsupported sprase layout to resolve false INTERNAL ASSERT FAILED  ([#139198](https://github.com/pytorch/pytorch/pull/139198))
- [ca] add test_dtensor_compile.py to compiled autograd tests ([#144107](https://github.com/pytorch/pytorch/pull/144107))
- Update core.py to fix typo ([#144201](https://github.com/pytorch/pytorch/pull/144201))
- [reland][attempt2][AMD] Turn on TF32 for aten::mm ([#144145](https://github.com/pytorch/pytorch/pull/144145))
- [mps/BE] Enable a test that now passes. ([#144198](https://github.com/pytorch/pytorch/pull/144198))
- Fix duplicate pattern error ([#139321](https://github.com/pytorch/pytorch/pull/139321))
- Update slow tests ([#144236](https://github.com/pytorch/pytorch/pull/144236))
- Enable clang-analyzer checks of Clang-tidy ([#144222](https://github.com/pytorch/pytorch/pull/144222))
- Rewrite _reparametrize_module to use `contextmanager` ([#138203](https://github.com/pytorch/pytorch/pull/138203))
- fix memleak, detach instead of clone to not drag around graph ([#144154](https://github.com/pytorch/pytorch/pull/144154))
- [BE][Ez]: Update CUDNN Frontend submodule to 1.9.0 ([#144200](https://github.com/pytorch/pytorch/pull/144200))
- Fix subclass unwrapping bug ([#143945](https://github.com/pytorch/pytorch/pull/143945))
- [BE] Fix + parametrize `test_min_max_nan_propagation` ([#144250](https://github.com/pytorch/pytorch/pull/144250))
- Enhance provenance tracing unit test to cover `torch.compile()` ([#143684](https://github.com/pytorch/pytorch/pull/143684))
- update expected results ([#144274](https://github.com/pytorch/pytorch/pull/144274))
- Migrate from Tuple -> tuple in torch/profiler ([#144257](https://github.com/pytorch/pytorch/pull/144257))
- [TreeSpec] Support enum in defaultdict ([#144235](https://github.com/pytorch/pytorch/pull/144235))
- Update torch.masked.mean to upcast dtype for bool tensors ([#139999](https://github.com/pytorch/pytorch/pull/139999))
- Fix lint in `test_provenance_tracing.py` ([#144296](https://github.com/pytorch/pytorch/pull/144296))
- remove allow-untyped-defs from torch/nn/utils/_deprecation_utils.py ([#144231](https://github.com/pytorch/pytorch/pull/144231))
- Migrate from Tuple -> tuple in benchmarks ([#144259](https://github.com/pytorch/pytorch/pull/144259))
- [ca] add test_dtensor_compile.py to compiled autograd tests ([#144107](https://github.com/pytorch/pytorch/pull/144107))
- Fixed doc where more than one device specified since only one device is used (#17553) ([#144043](https://github.com/pytorch/pytorch/pull/144043))
- [Inductor UT] Remove excepted failure for aoti test_fft_c2c  ([#144238](https://github.com/pytorch/pytorch/pull/144238))
- Eliminate c10::optional usage in PyTorch ([#144346](https://github.com/pytorch/pytorch/pull/144346))
- Unskipped multiple inductor tests for ROCm ([#143581](https://github.com/pytorch/pytorch/pull/143581))
- [dynamo][dicts] Guarding lazily on dict keys ([#143997](https://github.com/pytorch/pytorch/pull/143997))
- [AMD] SDPA internal changes ([#144320](https://github.com/pytorch/pytorch/pull/144320))
- Update torch.masked.mean to upcast dtype for bool tensors ([#139999](https://github.com/pytorch/pytorch/pull/139999))
- use `torch.special.xlogy` to implement `x_log_x` ([#144220](https://github.com/pytorch/pytorch/pull/144220))
- `Dirichlet.mode`: use `dim=` instead of `axis=` ([#144402](https://github.com/pytorch/pytorch/pull/144402))
- [BE]: Remove redundant contiguous copy in flex attention ([#144467](https://github.com/pytorch/pytorch/pull/144467))
- Update #graph breaks for moco benchmark ([#144266](https://github.com/pytorch/pytorch/pull/144266))
- [CUDA] parse arch-conditional compute-capability when building extensions ([#144446](https://github.com/pytorch/pytorch/pull/144446))
- Migrate from Tuple -> tuple in benchmarks/instruction_counts/core ([#144253](https://github.com/pytorch/pytorch/pull/144253))
- Migrate from Tuple -> tuple in torch/_decomp ([#144260](https://github.com/pytorch/pytorch/pull/144260))
- update xnnpack for disable libm on Windows  [submodule XNNPACK] ([#141943](https://github.com/pytorch/pytorch/pull/141943))
- [BE] Fix extra-semi warnings in int4mm_kernel.cpp ([#144510](https://github.com/pytorch/pytorch/pull/144510))
- Stop ignoring mypy errors in torch/testing/_internal/common_utils.py ([#144483](https://github.com/pytorch/pytorch/pull/144483))
- [dynamo] Avoid graph break on updates to `obj.__dict__` ([#144419](https://github.com/pytorch/pytorch/pull/144419))
- Cleanup gpt_fast benchmark ([#144517](https://github.com/pytorch/pytorch/pull/144517))
- [Inductor UT] Add expected failure for newly added case on XPU, align CUDA. ([#144457](https://github.com/pytorch/pytorch/pull/144457))
- [Inductor UT] Generalize newly introduced device-bias hard code in ([#144456](https://github.com/pytorch/pytorch/pull/144456))
- Fix torch.normal ignores default_device ([#144070](https://github.com/pytorch/pytorch/pull/144070))
- Fix poision child process issue when call getAccelerator() ([#144368](https://github.com/pytorch/pytorch/pull/144368))
- Generalize at::manual_seed for all accelerators ([#144370](https://github.com/pytorch/pytorch/pull/144370))
- retracing in strict doesn't like dataclass registration ([#144487](https://github.com/pytorch/pytorch/pull/144487))
- [BE][Opinfo] Delete redundant `dtypesIfCUDA` ([#144512](https://github.com/pytorch/pytorch/pull/144512))
- fix typo: "assumbed"  ([#144543](https://github.com/pytorch/pytorch/pull/144543))
- use collective_comm activity for hccl traces ([#144490](https://github.com/pytorch/pytorch/pull/144490))
- Update the accuracy results for moco and llama ([#144523](https://github.com/pytorch/pytorch/pull/144523))
- docs: get rid of copyright year ([#144562](https://github.com/pytorch/pytorch/pull/144562))
- [aoti] Remove example inputs from aoti_compile_and_package ([#144520](https://github.com/pytorch/pytorch/pull/144520))
- Increase C10_COMPILE_TIME_MAX_GPUS to 128 ([#144138](https://github.com/pytorch/pytorch/pull/144138))
- update sleef for disable libm on Windows [submodule Sleef] ([#142245](https://github.com/pytorch/pytorch/pull/142245))
- [ez] add lint commits to .git-blame-ignore-revs ([#144576](https://github.com/pytorch/pytorch/pull/144576))
- [XPU] Fix TRITON_XPU_BUILD_FROM_SOURCE ([#142850](https://github.com/pytorch/pytorch/pull/142850))
- [CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt ([#144441](https://github.com/pytorch/pytorch/pull/144441))
- [mps/inductor] Add support for exp(). ([#144606](https://github.com/pytorch/pytorch/pull/144606))
- remove allow-untyped-defs from torch/distributions/pareto.py ([#144624](https://github.com/pytorch/pytorch/pull/144624))
- remove allow-untyped-defs from torch/_functorch/utils.py ([#144626](https://github.com/pytorch/pytorch/pull/144626))
- [BE] Enable test_public_bindings on MacOS ([#144591](https://github.com/pytorch/pytorch/pull/144591))
- [mps/inductor] Add support for exp(). ([#144606](https://github.com/pytorch/pytorch/pull/144606))
- Fix a bug for conj_physical ([#144391](https://github.com/pytorch/pytorch/pull/144391))
- Add AOTAutogradCache support for cache hot loading APIs ([#144499](https://github.com/pytorch/pytorch/pull/144499))
- Update slow tests ([#144670](https://github.com/pytorch/pytorch/pull/144670))
- Update the Triton DeviceInterface in test/inductor/extension_backends/triton/device_interface.py ([#144399](https://github.com/pytorch/pytorch/pull/144399))
- remove allow-untyped-defs from torch/nn/parameter.pyi ([#144654](https://github.com/pytorch/pytorch/pull/144654))
- remove allow-untyped-defs from torch/_C/_dynamo/eval_frame.pyi ([#144655](https://github.com/pytorch/pytorch/pull/144655))
- [dynamo] Avoid graph break on updates to `obj.__dict__` ([#144419](https://github.com/pytorch/pytorch/pull/144419))
- Enable grep_linter to use -a ([#144589](https://github.com/pytorch/pytorch/pull/144589))
- c10::string_view -> std::string_view in pytorch ([#143591](https://github.com/pytorch/pytorch/pull/143591))
- [dynamo][dicts] Consolidate dict(..) construction ([#144342](https://github.com/pytorch/pytorch/pull/144342))
- Stop ignoring mypy errors in torch/testing/_internal/common_utils.py ([#144483](https://github.com/pytorch/pytorch/pull/144483))
- [RELAND] Generalize at::manual_seed for all accelerators ([#144370](https://github.com/pytorch/pytorch/pull/144370))
- [Quant][Inductor][X86] Separate binary post op fusion and lowering for qlinear ([#144224](https://github.com/pytorch/pytorch/pull/144224))
- Mark CUDA-12.6 as experimental for 2.6 release ([#144769](https://github.com/pytorch/pytorch/pull/144769))
- [1/N] OpenReg: Replace `open_registration_extension.cpp` with openreg ([#141815](https://github.com/pytorch/pytorch/pull/141815))
- [minifier] Fix config generator for callables ([#144518](https://github.com/pytorch/pytorch/pull/144518))
- Support FunctionalTensor subclass in is_fake and maybe_get_fake_mode ([#144719](https://github.com/pytorch/pytorch/pull/144719))
- [PrivateUse1] Support parseDispatchKey with modified PrivateUse1 ([#144325](https://github.com/pytorch/pytorch/pull/144325))
- [ROCm] Improvements for vectorized elementwise kernels ([#143269](https://github.com/pytorch/pytorch/pull/143269))
- Stop ignoring mypy errors in torch/testing/_internal/common_utils.py ([#144483](https://github.com/pytorch/pytorch/pull/144483))
- EZ fix to make sure local pytest run succeeds in export ([#144764](https://github.com/pytorch/pytorch/pull/144764))
- Fix FakeTensor device creation for MPS ([#144796](https://github.com/pytorch/pytorch/pull/144796))
- Fix global namespace pollution in ATen/Dispatch.h ([#138626](https://github.com/pytorch/pytorch/pull/138626))
- [AMD] De-noise tf32 warnings ([#144797](https://github.com/pytorch/pytorch/pull/144797))
- refresh benchmarks results after recent recent regressions  ([#143075](https://github.com/pytorch/pytorch/pull/143075))
- Expose several APIs to public (torch python APIs) ([#144525](https://github.com/pytorch/pytorch/pull/144525))
- restore rng generation for fbcode ([#144819](https://github.com/pytorch/pytorch/pull/144819))
- [ez] add lint commits to .git-blame-ignore-revs ([#144576](https://github.com/pytorch/pytorch/pull/144576))
- [CUDAGraph][Docs] add `cuda` to `torch.randn` ([#144793](https://github.com/pytorch/pytorch/pull/144793))
- [PagedAttention] Support different input position for each batch index ([#144693](https://github.com/pytorch/pytorch/pull/144693))
- [CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt ([#144441](https://github.com/pytorch/pytorch/pull/144441))
- cpp_wrapper: Move #includes to per-device header files ([#143909](https://github.com/pytorch/pytorch/pull/143909))
- export AOTI_TORCH_EXPORT on Windows. ([#140030](https://github.com/pytorch/pytorch/pull/140030))
- Default Copies are not vectorized in v3.6.0 of cutlass ([#144837](https://github.com/pytorch/pytorch/pull/144837))
- OpenReg: Use device agnostic API ([#144840](https://github.com/pytorch/pytorch/pull/144840))
- Add tests for different dtypes with max autotune ([#144721](https://github.com/pytorch/pytorch/pull/144721))
- [c10d][fr] Fix the bug when we still mark mismatch when there are match case ([#144916](https://github.com/pytorch/pytorch/pull/144916))
- restore rng generation for fbcode ([#144819](https://github.com/pytorch/pytorch/pull/144819))
- Add flop formula for _scaled_mm ([#144872](https://github.com/pytorch/pytorch/pull/144872))
- [mps] Massage test_full_truncation to work only on the supported dtypes. ([#144877](https://github.com/pytorch/pytorch/pull/144877))
- [BE] Add missing throw of `std::runtime_error` in scrc/cuda/utils.cpp ([#144962](https://github.com/pytorch/pytorch/pull/144962))
- Make functionalization `ViewMeta` serializable with pickle. ([#143712](https://github.com/pytorch/pytorch/pull/143712))
- [mps] Massage test_full_truncation to work only on the supported dtypes. ([#144877](https://github.com/pytorch/pytorch/pull/144877))
- Unskipped multiple inductor tests for ROCm ([#143581](https://github.com/pytorch/pytorch/pull/143581))
- expose extra torch_python apis ([#144746](https://github.com/pytorch/pytorch/pull/144746))
- update IS_JETSON check ([#144725](https://github.com/pytorch/pytorch/pull/144725))
- [BE] Move `is_device_supported` to helper function ([#144971](https://github.com/pytorch/pytorch/pull/144971))
- easy: Fix missing tab in test/dynamo/test_compile.py ([#145013](https://github.com/pytorch/pytorch/pull/145013))
- Add tests for different dtypes with max autotune ([#144721](https://github.com/pytorch/pytorch/pull/144721))
- [Utilization Log] Concurrently collect aggregate data during the output interval ([#143235](https://github.com/pytorch/pytorch/pull/143235))
- [MPSInductor] More is_dtype_supported gating ([#144981](https://github.com/pytorch/pytorch/pull/144981))
- OpenReg: Split Allocator ([#144843](https://github.com/pytorch/pytorch/pull/144843))
- [mps/inductor] Introduce is_mps_backend/skip_if_mps decorators. ([#145035](https://github.com/pytorch/pytorch/pull/145035))
- Enhance running pr time benchmarks locally experience. ([#144838](https://github.com/pytorch/pytorch/pull/144838))
- Add flop formula for _scaled_mm ([#144973](https://github.com/pytorch/pytorch/pull/144973))
- Add SM89 support for f8f8bf16_rowwise() ([#144348](https://github.com/pytorch/pytorch/pull/144348))
- fix typo in doc and import for torch._library.triton ([#144882](https://github.com/pytorch/pytorch/pull/144882))
- Fix unbind_copy and add its decomposition ([#134319](https://github.com/pytorch/pytorch/pull/134319))
- Fix issue with test/nn/test_convolution:TestConvolutionNNDeviceTypeCUDA.test_conv_large_batch_1_cuda ([#145067](https://github.com/pytorch/pytorch/pull/145067))
- Fix NJT OpInfo entry for nn.functional.prelu ([#144582](https://github.com/pytorch/pytorch/pull/144582))
- Fix RMSNorm epsilon value type for BF16 or FP16 ([#142848](https://github.com/pytorch/pytorch/pull/142848))
- parametrized test name handles class arguments ([#133546](https://github.com/pytorch/pytorch/pull/133546))
- Skip test responsible for causing flakiness ([#145109](https://github.com/pytorch/pytorch/pull/145109))
- Delete torch._library.register_functional_op ([#145110](https://github.com/pytorch/pytorch/pull/145110))
- Update ci_expected_accuracy for TIMM levit_128 for further investigation ([#145112](https://github.com/pytorch/pytorch/pull/145112))
- Added swizzle searching, disabled fp16 accum, and enabled ping-pong for cutlass ([#144829](https://github.com/pytorch/pytorch/pull/144829))
- [ca] Use aot_eager on flex attention test ([#145097](https://github.com/pytorch/pytorch/pull/145097))
- Moved .all() checks for distributions to _is_all_true ([#145029](https://github.com/pytorch/pytorch/pull/145029))
- [mps/inductor] Skip "double" tests as 64-bits FP is not supported. ([#145123](https://github.com/pytorch/pytorch/pull/145123))
- Enable bfloat16 testing on MacOS14+ ([#145159](https://github.com/pytorch/pytorch/pull/145159))
- [dynamo][dicts] Consolidate dict(..) construction ([#144342](https://github.com/pytorch/pytorch/pull/144342))
- Add facility to run dynamo UTs for non-cuda devices ([#140929](https://github.com/pytorch/pytorch/pull/140929))
- Fix always true scaled_mm test ([#143912](https://github.com/pytorch/pytorch/pull/143912))
- [Intel CPU] Fix issue #143489. ([#145062](https://github.com/pytorch/pytorch/pull/145062))
- Use TORCH_CHECK instead of std::runtime_error in stack.h and ivalue.h ([#145280](https://github.com/pytorch/pytorch/pull/145280))
- [triton] Update triton pin to include warp specialization support ([#145120](https://github.com/pytorch/pytorch/pull/145120))
- [Inductor] inplace padding ([#140249](https://github.com/pytorch/pytorch/pull/140249))
- Refactor CPUReproTests to be more vector-length agnostic ([#141245](https://github.com/pytorch/pytorch/pull/141245))
- [BE][export] Remove disabled floordiv test in export ([#145292](https://github.com/pytorch/pytorch/pull/145292))
- Add MKLDNN support for Half GELU ([#145339](https://github.com/pytorch/pytorch/pull/145339))
- [torchbench] Fix mobilenetv2 inductor freezing fail_accuracy ([#145296](https://github.com/pytorch/pytorch/pull/145296))
- Fix deprecated pytorch_sphinx_theme editable installation ([#145347](https://github.com/pytorch/pytorch/pull/145347))
- [S481486] [MTIA] Correct mtia.device_count() API ([#145338](https://github.com/pytorch/pytorch/pull/145338))
- Align CPU behavior with CUDA for `ConvTranspose` when `out_channels=0` ([#142859](https://github.com/pytorch/pytorch/pull/142859))
- Move Dynamo test to skip from expected_failures ([#145390](https://github.com/pytorch/pytorch/pull/145390))
- Fix tests broken by #145176 ([#145393](https://github.com/pytorch/pytorch/pull/145393))
- [Doc] Add period at the end of the sentence ([#145384](https://github.com/pytorch/pytorch/pull/145384))
- Binary upload checksum ([#144887](https://github.com/pytorch/pytorch/pull/144887))
- [compiled autograd] Proxy opaque nodes for built-in autograd nodes ([#143296](https://github.com/pytorch/pytorch/pull/143296))
- [compiled autograd] Proxy a node for CopyBackwards into the graph ([#143304](https://github.com/pytorch/pytorch/pull/143304))
- [compiled autograd] Proxy nodes for user-defined C++ torch::autograd::Function ([#143387](https://github.com/pytorch/pytorch/pull/143387))
- [compiled autograd] Always proxy autograd.Function nodes; handle AOT backwards ([#143405](https://github.com/pytorch/pytorch/pull/143405))
- [CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt ([#144441](https://github.com/pytorch/pytorch/pull/144441))
- [S481486] Move MTIA dynamic library loading from __init__.py to a separate module ([#145322](https://github.com/pytorch/pytorch/pull/145322))
- [BE][export] Change custom_op registeration style ([#145315](https://github.com/pytorch/pytorch/pull/145315))
- Move privateuse1 test out of test_utils and make them serial ([#145380](https://github.com/pytorch/pytorch/pull/145380))
- Generalize pin memory logic for accelerator when non blocking copy happened ([#143783](https://github.com/pytorch/pytorch/pull/143783))
- [c10/metal] Add a vectype variant for `short`/`int`/`long` ([#145430](https://github.com/pytorch/pytorch/pull/145430))
- [BE] Remove test_modules from FIXME_inductor_dont_reset_dynamo ([#145306](https://github.com/pytorch/pytorch/pull/145306))
- Adapt Dynamo tests to HPUs using instantiate_device_type_tests ([#144387](https://github.com/pytorch/pytorch/pull/144387))
- Fix test_modules_can_be_imported ([#145387](https://github.com/pytorch/pytorch/pull/145387))
- [ca][bug_fix] Fix ref counting of objects in the set_autograd_compiler function. ([#145482](https://github.com/pytorch/pytorch/pull/145482))
- [BE] Remove test_ops_gradients from FIXME_inductor_dont_reset_dynamo ([#145308](https://github.com/pytorch/pytorch/pull/145308))
- [hop][be] add utils for more comprehensive input alias and mutation ([#145298](https://github.com/pytorch/pytorch/pull/145298))
- [be] fix flaky test aot_export_ cond caused by free symbol lifting and automatic dynamic shape ([#145330](https://github.com/pytorch/pytorch/pull/145330))
- [compiled_autograd] Rename interface to pyinterface ([#145495](https://github.com/pytorch/pytorch/pull/145495))
- Add multi env variable support to configs ([#145288](https://github.com/pytorch/pytorch/pull/145288))
- [dynamo] Log guard latency ([#145132](https://github.com/pytorch/pytorch/pull/145132))
- [BE/mps] Mark input args as `constant` to prevent incorrect usage. ([#145535](https://github.com/pytorch/pytorch/pull/145535))
- OpenReg: Remove REGISTER_GENERATOR_PRIVATEUSE1 ([#144841](https://github.com/pytorch/pytorch/pull/144841))
- Only include RMSNorm.h in layer_norm.cpp for MPS ([#145524](https://github.com/pytorch/pytorch/pull/145524))
- [cutlass backend tests] Manually clear cache, test more tests in fbcode and limit configs in some tests ([#145545](https://github.com/pytorch/pytorch/pull/145545))
- Document dispatch trace build flag ([#145517](https://github.com/pytorch/pytorch/pull/145517))
- [BE] Use `value_or` in layer_norm.cpp ([#145417](https://github.com/pytorch/pytorch/pull/145417))
- [CI][CUDA][Dynamic Shape] xfail: DynamicShapesCodegenGPUTests.test_linspace4_dynamic_shapes_cuda ([#145204](https://github.com/pytorch/pytorch/pull/145204))
- [CI][CUDA][MultiGPU][Regression] Skip a failure due to https://github.com/pytorch/pytorch/issues/139520 ([#145318](https://github.com/pytorch/pytorch/pull/145318))
- [Submodule] Add flash as third-party submodule [Prep for later PRs] ([#145502](https://github.com/pytorch/pytorch/pull/145502))
- [BE][export] Fix hop tests with flaky memory leak ([#145391](https://github.com/pytorch/pytorch/pull/145391))
- Add multi env variable support to configs ([#145288](https://github.com/pytorch/pytorch/pull/145288))
- [triton] Update triton pin to include warp specialization support ([#145120](https://github.com/pytorch/pytorch/pull/145120))
- [Intel GPU] Add TORCH_API macro to export symbol NestedTensor_to_mask for libtorch_xpu ([#145467](https://github.com/pytorch/pytorch/pull/145467))
- If mypy fails it should report the error back to lintrunner ([#145550](https://github.com/pytorch/pytorch/pull/145550))
- [inductor][BE] Enable test_cpu_cpp_wrapper in fbcode ([#145373](https://github.com/pytorch/pytorch/pull/145373))
- [torchbench] Add meta function for _cudnn_rnn_flatten_weight ([#145488](https://github.com/pytorch/pytorch/pull/145488))
- [BE] mv test/inductor_skips/* to test/inductor_expected_failures/ ([#145572](https://github.com/pytorch/pytorch/pull/145572))
- [torchbench] Increase tolerance for amp only poolformer_m36 ([#145375](https://github.com/pytorch/pytorch/pull/145375))
- Improve the caching allocator test for raw alloc ([#145269](https://github.com/pytorch/pytorch/pull/145269))
- Modify enable logic of COLLECTIVE_COMM profiler activity type ([#145478](https://github.com/pytorch/pytorch/pull/145478))
- [ATen][CUDA][Transformers] Add Blackwell support to SDPA ([#145602](https://github.com/pytorch/pytorch/pull/145602))
- Avoid data-dependent errors in NJT tests via capture_scalar_outputs=True ([#144588](https://github.com/pytorch/pytorch/pull/144588))
- [CI][CUDA][Blackwell] sm_\d\d no longer matches sm_100.  ([#145641](https://github.com/pytorch/pytorch/pull/145641))
- [utils] add try_import method for importing optional modules ([#145528](https://github.com/pytorch/pytorch/pull/145528))
- inductor: Explicitly test that torch.compile(option=...) does something ([#145321](https://github.com/pytorch/pytorch/pull/145321))
- Disable slow gradcheck for nn.Transformer ModuleInfo ([#145531](https://github.com/pytorch/pytorch/pull/145531))
- Update NJT linear_backward to return non-aliased tensor bias grad ([#145399](https://github.com/pytorch/pytorch/pull/145399))
- [dynamo] Log guard latency ([#145132](https://github.com/pytorch/pytorch/pull/145132))
- [autocast][pytorch] Support autocast for MTIA ([#145627](https://github.com/pytorch/pytorch/pull/145627))
- OpenReg: Refactor impl_registry ([#145465](https://github.com/pytorch/pytorch/pull/145465))
- Save integral tensor data for ET ([#144508](https://github.com/pytorch/pytorch/pull/144508))
- [ca] add test_reset for 2.6 release validation ([#145549](https://github.com/pytorch/pytorch/pull/145549))
- Remove unnecessary HPUHooksInterface method ([#145272](https://github.com/pytorch/pytorch/pull/145272))
- Align CPU behavior with CUDA for `ConvTranspose` when `out_channels=0` ([#142859](https://github.com/pytorch/pytorch/pull/142859))
- Fix Throughputbenchmark issue ([#144669](https://github.com/pytorch/pytorch/pull/144669))
- Set RUNPATH on CUDA and XPU tests ([#144305](https://github.com/pytorch/pytorch/pull/144305))
- Update slow tests ([#145206](https://github.com/pytorch/pytorch/pull/145206))
- solve apl dependency issue ([#145215](https://github.com/pytorch/pytorch/pull/145215))
- [BE] Use copy_method to import all tests ([#145718](https://github.com/pytorch/pytorch/pull/145718))
- Remove `public_allowlist` from `TestPublicBindings.test_correct_module_names` and ensure private_allowlist-ed things are actually private ([#145620](https://github.com/pytorch/pytorch/pull/145620))
- [CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt ([#144441](https://github.com/pytorch/pytorch/pull/144441))
- [BE] Remove test_ops from FIXME_inductor_dont_reset_dynamo ([#145307](https://github.com/pytorch/pytorch/pull/145307))
- [autocast][pytorch] Support autocast for MTIA (policy) ([#145666](https://github.com/pytorch/pytorch/pull/145666))
- [Custom Ops] Add a new API to allow users to register an autocast for the custom op ([#145588](https://github.com/pytorch/pytorch/pull/145588))
- [Custom Ops] Fix f-strings in custom ops error message ([#145673](https://github.com/pytorch/pytorch/pull/145673))
- Add ignorable commits on run_test.py to git blame ignore ([#145787](https://github.com/pytorch/pytorch/pull/145787))
- Remove lexicographical sorting of storage keys in torch.save ([#143879](https://github.com/pytorch/pytorch/pull/143879))
- [dynamo] save/restore system random state more carefully ([#145750](https://github.com/pytorch/pytorch/pull/145750))
- Log cache state for AOTAutograd in title of file ([#145715](https://github.com/pytorch/pytorch/pull/145715))
- [cond] remove warning for unsupported tuple returns ([#145766](https://github.com/pytorch/pytorch/pull/145766))
- [3/N] Remove unnecessary once flag usage ([#145672](https://github.com/pytorch/pytorch/pull/145672))
- [BE]: Update typing of OrderedSet ancestor ([#145783](https://github.com/pytorch/pytorch/pull/145783))
- Improve typing in torch/types.py ([#145237](https://github.com/pytorch/pytorch/pull/145237))
- OpenReg: fix issue of pin_memory ([#145046](https://github.com/pytorch/pytorch/pull/145046))
- [ROCm] fix test_cublas_workspace_explicit_allocation for gfx12 ([#145227](https://github.com/pytorch/pytorch/pull/145227))
- [AOTInductor] Refactor CPU and GPU to remove ifdef macros ([#145639](https://github.com/pytorch/pytorch/pull/145639))
- [ROCm] Bump AOTriton to 0.8.2b ([#145508](https://github.com/pytorch/pytorch/pull/145508))
- Log info for AOTAutogradCache bypasses instead of warning ([#145768](https://github.com/pytorch/pytorch/pull/145768))
- [BE] Include CheckFunctionExists in `FindBLAS.cmake` ([#145849](https://github.com/pytorch/pytorch/pull/145849))
- [aarch64] Rebuild everything with ArmPL ([#145742](https://github.com/pytorch/pytorch/pull/145742))
- [ATen] Implement exception handling for hipsolver APIs ([#145839](https://github.com/pytorch/pytorch/pull/145839))
- [CD] Install ninja and setuptools from PyPI ([#145871](https://github.com/pytorch/pytorch/pull/145871))
- Make sure to evaluate annotation strings in the context of where the prototype was created ([#145667](https://github.com/pytorch/pytorch/pull/145667))
- Fix RMSNorm epsilon value type for BF16 or FP16 ([#142848](https://github.com/pytorch/pytorch/pull/142848))
- Added swizzle searching, disabled fp16 accum, and enabled ping-pong for cutlass ([#144829](https://github.com/pytorch/pytorch/pull/144829))
- [be][pytorch] Fix backend in autocast ([#145859](https://github.com/pytorch/pytorch/pull/145859))
- Fix C++20 Wambiguous-reversed-operator warnings ([#144126](https://github.com/pytorch/pytorch/pull/144126))
- add test for capture_dynamic_output_shape_ops=True changing expected output between eager and compiled versions ([#145821](https://github.com/pytorch/pytorch/pull/145821))
- [pytorch] raise exception when calling dim order on sparse tensor ([#145888](https://github.com/pytorch/pytorch/pull/145888))
- [ATen][CUDA] Implement 128 bit vectorization v2 ([#145746](https://github.com/pytorch/pytorch/pull/145746))
- [triton] Update pin to tip of 3.2 release ([#145867](https://github.com/pytorch/pytorch/pull/145867))
- Update mi300 labels to account for multiple clusters. ([#145923](https://github.com/pytorch/pytorch/pull/145923))
- [dynamo] Use polyfill to implement comparison operators ([#144485](https://github.com/pytorch/pytorch/pull/144485))
- [CD] Install ninja and setuptools from PyPI ([#145871](https://github.com/pytorch/pytorch/pull/145871))
- Don't use mypy daemon in CI ([#145961](https://github.com/pytorch/pytorch/pull/145961))
- [linter] Grep linter batches long command ([#145950](https://github.com/pytorch/pytorch/pull/145950))
- Disable AOTAutogradCache for triton version < 3.2 ([#145937](https://github.com/pytorch/pytorch/pull/145937))
- [BE]: Update CUTLASS submodule to 3.7.0 ([#145172](https://github.com/pytorch/pytorch/pull/145172))
- Better hop_db comment; move test to a non-export test file ([#145938](https://github.com/pytorch/pytorch/pull/145938))
- Require that all HOPs be imported at `import torch` time ([#145939](https://github.com/pytorch/pytorch/pull/145939))
- re-use FloorDiv for RShift ([#145898](https://github.com/pytorch/pytorch/pull/145898))
- config: Support str env variables ([#145980](https://github.com/pytorch/pytorch/pull/145980))
- Copy model before benchmark warmup runs ([#145858](https://github.com/pytorch/pytorch/pull/145858))
- [pytorch] Sprinkle in a few `template` keywords ([#145877](https://github.com/pytorch/pytorch/pull/145877))
- Use std::string_view ([#145906](https://github.com/pytorch/pytorch/pull/145906))
- config: Don't spam warnings about reference type configs ([#145800](https://github.com/pytorch/pytorch/pull/145800))
- Advance past fc window for stft center ([#145437](https://github.com/pytorch/pytorch/pull/145437))
- inductor: Don't throw an internal error when a nn.module is missing a attribute ([#145122](https://github.com/pytorch/pytorch/pull/145122))
- [CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt ([#144441](https://github.com/pytorch/pytorch/pull/144441))
- [Torch] Extract arange_out resizing logic into a helper function that can be used by other devices ([#145747](https://github.com/pytorch/pytorch/pull/145747))
- Enable C++ API parity tests on AArch64 ([#145370](https://github.com/pytorch/pytorch/pull/145370))
- [inductor/profiler] add kernel kwargs instrumentation ([#145573](https://github.com/pytorch/pytorch/pull/145573))
- Add manual override flag for core ATen op detection during bc check ([#146052](https://github.com/pytorch/pytorch/pull/146052))
- Simplify handling of max jobs in CMake builds ([#145820](https://github.com/pytorch/pytorch/pull/145820))
- torchgen: support exception boundary for ExecuTorch functions ([#144341](https://github.com/pytorch/pytorch/pull/144341))
- [Break XPU] Fix Inductor cuda bias UT ([#145934](https://github.com/pytorch/pytorch/pull/145934))
- [export] Add distributed test ([#146050](https://github.com/pytorch/pytorch/pull/146050))
- [AOTI] Cache treespec_loads calculation ([#145815](https://github.com/pytorch/pytorch/pull/145815))
- [ATen][CUDA] Implement 128 bit vectorization v2 ([#145746](https://github.com/pytorch/pytorch/pull/145746))
- [CMake] Delete Caffe2 inspect_gpu binary ([#146105](https://github.com/pytorch/pytorch/pull/146105))
- fix a small typo in comments ([#145323](https://github.com/pytorch/pytorch/pull/145323))
- Remove lexicographical sorting of storage keys in torch.save ([#143879](https://github.com/pytorch/pytorch/pull/143879))
- Tensor .cuda() very slow with specific array sizes ([#138964](https://github.com/pytorch/pytorch/pull/138964))
- s390x: disable test_model_exports_to_core_aten.py test ([#145835](https://github.com/pytorch/pytorch/pull/145835))
- Binary upload checksum ([#144887](https://github.com/pytorch/pytorch/pull/144887))
- [hop][inductor] track the dependency on unbacked symbols correctly with constant_args for hops ([#143456](https://github.com/pytorch/pytorch/pull/143456))
- Torch device backend autoload fix ([#145611](https://github.com/pytorch/pytorch/pull/145611))
- Remove trivial dispatch_key_allowlist_check function ([#146169](https://github.com/pytorch/pytorch/pull/146169))
- [ca][hop] test CA on all HOPs ([#145429](https://github.com/pytorch/pytorch/pull/145429))
- torch/nn/utils/rnn.py: docs: improvements ([#138628](https://github.com/pytorch/pytorch/pull/138628))
- execution trace export supports gzip format ([#146179](https://github.com/pytorch/pytorch/pull/146179))
- Enable ruff F841 on numpy tests ([#146126](https://github.com/pytorch/pytorch/pull/146126))
- [draft_export] Clear pending unbacked symbols when overriding mismatched fake kernels ([#146089](https://github.com/pytorch/pytorch/pull/146089))
- add speculation log divergence test ([#145659](https://github.com/pytorch/pytorch/pull/145659))
- Fix aten.to when input is a tensor constant ([#146220](https://github.com/pytorch/pytorch/pull/146220))
- Add torch.utils._pytree.register_dataclass ([#146059](https://github.com/pytorch/pytorch/pull/146059))
- [inductor] Add typing to common.KernelArgs ([#145916](https://github.com/pytorch/pytorch/pull/145916))
- [inductor] Add typing to common.CSE ([#145993](https://github.com/pytorch/pytorch/pull/145993))
- [BE][Ez]: Make c10/special arrays constexpr ([#146246](https://github.com/pytorch/pytorch/pull/146246))
- [inductor] Finish typing common.py ([#146225](https://github.com/pytorch/pytorch/pull/146225))
- add WaitCounter type interface and get rid of type errors ([#146175](https://github.com/pytorch/pytorch/pull/146175))
- Fix unreachable code ([#146262](https://github.com/pytorch/pytorch/pull/146262))
- [metal] Refactor digamma in preparation for moving it. ([#146281](https://github.com/pytorch/pytorch/pull/146281))
- [metal] Move digamma to special_math.h ([#146284](https://github.com/pytorch/pytorch/pull/146284))
- Remove unused import in tests ([#146266](https://github.com/pytorch/pytorch/pull/146266))
- Update slow tests ([#146301](https://github.com/pytorch/pytorch/pull/146301))
- [hop] enable while_loop return torch.ones with unbacked symbol expression. ([#146194](https://github.com/pytorch/pytorch/pull/146194))
- Add non-strict export while_loop test back ([#146195](https://github.com/pytorch/pytorch/pull/146195))
- [aoti] Assign proxy call args by name, and support default values. ([#146263](https://github.com/pytorch/pytorch/pull/146263))
- [Utilization] Convert timestamp to str for datetime64  ([#145985](https://github.com/pytorch/pytorch/pull/145985))
- [inductor] Refactor CSEProxy into global scope ([#146226](https://github.com/pytorch/pytorch/pull/146226))
- [inductor] Refactor op handlers part 1 ([#146235](https://github.com/pytorch/pytorch/pull/146235))
- [mps/inductor] Adjust more tests that expect float64 as input. ([#146366](https://github.com/pytorch/pytorch/pull/146366))
- [BE] reduce log spew from test_triton_kernels.py ([#145895](https://github.com/pytorch/pytorch/pull/145895))
- Fix random crash in PyPer ([#146327](https://github.com/pytorch/pytorch/pull/146327))
- Use std::string_view in tests ([#146120](https://github.com/pytorch/pytorch/pull/146120))
- [ROCm][TunableOp] Support leading dimensions in TunableOp signature. ([#146358](https://github.com/pytorch/pytorch/pull/146358))
- [inductor] Add typing to common.KernelArgs ([#145916](https://github.com/pytorch/pytorch/pull/145916))
- [inductor] Add typing to common.CSE ([#145993](https://github.com/pytorch/pytorch/pull/145993))
- Add `@nikitaved` to torch.linalg `CODEOWNERS/persons_of_interest` ([#141803](https://github.com/pytorch/pytorch/pull/141803))
- Use OrderedSet in _functorch/partitioners ([#146102](https://github.com/pytorch/pytorch/pull/146102))
- [hop][inductor] track the dependency on unbacked symbols correctly with constant_args for hops ([#143456](https://github.com/pytorch/pytorch/pull/143456))
- [Metal][BE] Add `#pragma once` to all headers ([#146423](https://github.com/pytorch/pytorch/pull/146423))
- [Metal] Small speedup for `sum`/`prod` ([#146428](https://github.com/pytorch/pytorch/pull/146428))
- [experimental] filter logs by subgraph ([#146047](https://github.com/pytorch/pytorch/pull/146047))
- use DTRACE_ENV_VAR as the trace logs directory of set ([#146412](https://github.com/pytorch/pytorch/pull/146412))
- [Testing] Reduce `test_exp` flakiness ([#146436](https://github.com/pytorch/pytorch/pull/146436))
- [metal] Add a missing cast to make the call to copysign unambiguous. ([#146422](https://github.com/pytorch/pytorch/pull/146422))
- [TEST][Sparse] Force CUTLASS backend in TestSparseSemiStructuredCUTLASS ([#146398](https://github.com/pytorch/pytorch/pull/146398))
- [cutlass backend] fix bug for accuminator dtype ([#146356](https://github.com/pytorch/pytorch/pull/146356))
- [inductor] Finish typing common.py ([#146225](https://github.com/pytorch/pytorch/pull/146225))
- [inductor] Refactor CSEProxy into global scope ([#146226](https://github.com/pytorch/pytorch/pull/146226))
- [inductor] Refactor op handlers part 1 ([#146235](https://github.com/pytorch/pytorch/pull/146235))
- [inductor] Refactor op handlers part 2 ([#146252](https://github.com/pytorch/pytorch/pull/146252))
- [inductor] Refactor op handlers part 3 ([#146254](https://github.com/pytorch/pytorch/pull/146254))
- [inductor] Refactor op handlers part 4 ([#146255](https://github.com/pytorch/pytorch/pull/146255))
- [inductor] Refactor op handlers part 5 ([#146257](https://github.com/pytorch/pytorch/pull/146257))
- [inductor] Minor compile time optimizations in DefaultHandler ([#146282](https://github.com/pytorch/pytorch/pull/146282))
- [inductor] Refactor CaptureIndexing into global scope ([#146297](https://github.com/pytorch/pytorch/pull/146297))
- [inductor] Pre-populate cache for simplify_with_ranges return value ([#146373](https://github.com/pytorch/pytorch/pull/146373))
- [triton] Update pin to tip of 3.2 release ([#145867](https://github.com/pytorch/pytorch/pull/145867))
- [BE][Ez]: Enable ruff rule E731. use `def` instead of anonymous lambda  ([#146410](https://github.com/pytorch/pytorch/pull/146410))
- [MPSInductor] Scope-down test_prod running in MPS ([#146460](https://github.com/pytorch/pytorch/pull/146460))
- Enable some tests on Windows ([#146243](https://github.com/pytorch/pytorch/pull/146243))
- inductor: Don't throw an internal error when a nn.module is missing a attribute ([#145122](https://github.com/pytorch/pytorch/pull/145122))
- [inductor] use ftz variant of exp ([#146216](https://github.com/pytorch/pytorch/pull/146216))
- [ROCm][TunableOp] Improve identification of fastest solution ([#144942](https://github.com/pytorch/pytorch/pull/144942))
- [aoti] Assign proxy call args by name, and support default values. ([#146263](https://github.com/pytorch/pytorch/pull/146263))
- Add check that envvar configs are boolean ([#145454](https://github.com/pytorch/pytorch/pull/145454))
- Add retain-output argument ([#145921](https://github.com/pytorch/pytorch/pull/145921))
- [BE][Metal] Fix signed unsigned comparison warning ([#146549](https://github.com/pytorch/pytorch/pull/146549))
- Make `inductor_utils.requires_gpu` accept MPS ([#145156](https://github.com/pytorch/pytorch/pull/145156))
- [DTensor][Test] Create a simple unit test for tensordot ([#146514](https://github.com/pytorch/pytorch/pull/146514))
- [cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces ([#145130](https://github.com/pytorch/pytorch/pull/145130))
- [CCA] remove TODO for hardware_destructive_interference_size ([#145591](https://github.com/pytorch/pytorch/pull/145591))
- [dynamo] Use polyfill to implement comparison operators ([#144485](https://github.com/pytorch/pytorch/pull/144485))
- [METAL] inline bfloat min/max ([#146588](https://github.com/pytorch/pytorch/pull/146588))
- [CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt ([#144441](https://github.com/pytorch/pytorch/pull/144441))
- [inductor] use ftz variant of exp ([#146216](https://github.com/pytorch/pytorch/pull/146216))
- [while_loop][inductor] support sym expression as cond_fn output ([#146222](https://github.com/pytorch/pytorch/pull/146222))
- [BE][Ez]: Enable some additional pylint ruff warnings ([#146609](https://github.com/pytorch/pytorch/pull/146609))
- [CUDA][SDPA] Compute reference in `test_triton_scaled_dot_product_attention_block_size_16_cuda_float32` in `float64` ([#146461](https://github.com/pytorch/pytorch/pull/146461))
- fix tf32 issue in test_inductor_freezing.py unit tests ([#146444](https://github.com/pytorch/pytorch/pull/146444))
- [Windows][ROCm] Fix c10 hip tests ([#146599](https://github.com/pytorch/pytorch/pull/146599))
- [ROCm][Windows] Fix unrecognized _BitScanReverse intrinsic ([#146606](https://github.com/pytorch/pytorch/pull/146606))
- [BE]: Inline special functions for MPS ([#146627](https://github.com/pytorch/pytorch/pull/146627))
- Enable TemporaryFileName tests on Windows ([#146311](https://github.com/pytorch/pytorch/pull/146311))
- [CUDA][CUDA Graphs] Fix debug mode warning message ([#145996](https://github.com/pytorch/pytorch/pull/145996))
- [inductor/profiler] add kernel kwargs instrumentation ([#145573](https://github.com/pytorch/pytorch/pull/145573))
- [cutlass backend] Set no fallback to aten, disabled a few broken tests, default to test on H100 ([#146554](https://github.com/pytorch/pytorch/pull/146554))
- [TreeSpec] Add custom comparision function ([#146442](https://github.com/pytorch/pytorch/pull/146442))
- Clean up op BC check list ([#146577](https://github.com/pytorch/pytorch/pull/146577))
- [MTIA] (2/n) Implement PyTorch APIs to query/reset device peak memory usage ([#146659](https://github.com/pytorch/pytorch/pull/146659))
- Enable Windows tests ([#146666](https://github.com/pytorch/pytorch/pull/146666))
- Fix linter F821 error ([#146665](https://github.com/pytorch/pytorch/pull/146665))
- [inductor] Refactor op handlers part 2 ([#146252](https://github.com/pytorch/pytorch/pull/146252))
- [inductor] Refactor op handlers part 3 ([#146254](https://github.com/pytorch/pytorch/pull/146254))
- [inductor] Refactor op handlers part 4 ([#146255](https://github.com/pytorch/pytorch/pull/146255))
- [inductor] Refactor op handlers part 5 ([#146257](https://github.com/pytorch/pytorch/pull/146257))
- [inductor] Minor compile time optimizations in DefaultHandler ([#146282](https://github.com/pytorch/pytorch/pull/146282))
- [inductor] Refactor CaptureIndexing into global scope ([#146297](https://github.com/pytorch/pytorch/pull/146297))
- [inductor] Pre-populate cache for simplify_with_ranges return value ([#146373](https://github.com/pytorch/pytorch/pull/146373))
- [EZ] Add logic to build Metal shader with debug info ([#146768](https://github.com/pytorch/pytorch/pull/146768))
- Update strided test to float32 ([#146748](https://github.com/pytorch/pytorch/pull/146748))
- [dim order]  solve broken doc ([#146641](https://github.com/pytorch/pytorch/pull/146641))
- fix: replace stderr with stdout for download messages in hub.py ([#146475](https://github.com/pytorch/pytorch/pull/146475))
- [ROCm] Unskip std:bad_alloc failures ([#146407](https://github.com/pytorch/pytorch/pull/146407))
- Test typing of arithmetic operators on Tensor (see #145838) ([#146426](https://github.com/pytorch/pytorch/pull/146426))
- [MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage ([#146710](https://github.com/pytorch/pytorch/pull/146710))
- cpp_wrapper: Precompile device-specific header files ([#144002](https://github.com/pytorch/pytorch/pull/144002))
- [cutlass backend] fix bug for accuminator dtype ([#146356](https://github.com/pytorch/pytorch/pull/146356))
- [while_loop][inductor] support sym expression as cond_fn output ([#146222](https://github.com/pytorch/pytorch/pull/146222))
- Fix bazel job after #144489 ([#146840](https://github.com/pytorch/pytorch/pull/146840))
- [ROCm] Update periodic.yml to use 2GPU runners ([#146839](https://github.com/pytorch/pytorch/pull/146839))
- [MTIA] (4/n) Implement PyTorch APIs to query/reset device peak memory usage ([#146751](https://github.com/pytorch/pytorch/pull/146751))
- Introduce new template heuristic for triton autotune configs ([#144985](https://github.com/pytorch/pytorch/pull/144985))
- [BE] Strip `#pragma once` when embedding the headers ([#146871](https://github.com/pytorch/pytorch/pull/146871))
- [DTensor][Test] Create a simple unit test for tensordot ([#146514](https://github.com/pytorch/pytorch/pull/146514))
- Update octokit/request-action to 2.4.0 ([#146940](https://github.com/pytorch/pytorch/pull/146940))
- [torch][amdsmi] Avoid ODR violation when loading amdsmi ([#146324](https://github.com/pytorch/pytorch/pull/146324))
- [CUDA][CUDNN][SDPA] Pass dropout seed and offset to cuDNN in `int64` ([#146734](https://github.com/pytorch/pytorch/pull/146734))
- [BE] Unskip some tensor creation tests on Mac ([#146952](https://github.com/pytorch/pytorch/pull/146952))
- Update instructions about faster linker ([#146750](https://github.com/pytorch/pytorch/pull/146750))
- Update octokit/request-action to 2.4.0 ([#146940](https://github.com/pytorch/pytorch/pull/146940))
- [BE][Ez]: Remove unnecessary type ignores from orderedset ([#146902](https://github.com/pytorch/pytorch/pull/146902))
- [pytree][Easy] preserve `dict` keys in insertion order in CXX pytree ([#130140](https://github.com/pytorch/pytorch/pull/130140))
- Update dynamo expected 20250210 ([#146856](https://github.com/pytorch/pytorch/pull/146856))
- [ROCm][TunableOp] Close offline tuning results file when offline tuning is disabled. ([#146574](https://github.com/pytorch/pytorch/pull/146574))
- [cutlass backend] Do not change dtype of GEMM template ([#146877](https://github.com/pytorch/pytorch/pull/146877))
- Add `make_dynamo_test` ([#146491](https://github.com/pytorch/pytorch/pull/146491))
- Turn on autograd local caches in fbcode ([#146996](https://github.com/pytorch/pytorch/pull/146996))
- [cond] make cond re-dispatch in proxy mode ([#146954](https://github.com/pytorch/pytorch/pull/146954))
- [aoti_debug_printer][BE] explicitly dumping float32, bfloat16, float16 data type ([#147020](https://github.com/pytorch/pytorch/pull/147020))
- [BE][OpInfo] Introduce generic `dtypesIf` ([#146905](https://github.com/pytorch/pytorch/pull/146905))
- [BE][Ez]: Update fmtlib submodule to 11.1.3 ([#146985](https://github.com/pytorch/pytorch/pull/146985))
- [AOTInductor] Align behavior between CPU and GPU ([#145459](https://github.com/pytorch/pytorch/pull/145459))
- Make GetCPUAllocatorMaybePinned to be Device-Agnostic ([#146687](https://github.com/pytorch/pytorch/pull/146687))
- Use 2022 as default VC_YEAR for windows builds ([#147053](https://github.com/pytorch/pytorch/pull/147053))
- [BE]: Make OrderedSet reversible ([#146904](https://github.com/pytorch/pytorch/pull/146904))
- update kineto submodule ([#147015](https://github.com/pytorch/pytorch/pull/147015))
- Disable test with dynamo for schema gen ([#146865](https://github.com/pytorch/pytorch/pull/146865))
- Update slow tests ([#146822](https://github.com/pytorch/pytorch/pull/146822))
- [subclass] testing WrapperSubclass respect outer_size, outer_stride ([#146897](https://github.com/pytorch/pytorch/pull/146897))
- Fix `DispatchStub.cpp` compilation for gcc 14 ([#146512](https://github.com/pytorch/pytorch/pull/146512))
- [Submodule]: Update KleidiAI submodule to v1.3.0 ([#146480](https://github.com/pytorch/pytorch/pull/146480))
- [Fix]: Disable KleidiAI if unsupported gcc/clang compiler is detected ([#146836](https://github.com/pytorch/pytorch/pull/146836))
- [cutlass backend] Do not change dtype of GEMM template ([#146877](https://github.com/pytorch/pytorch/pull/146877))
- [BE][Ez]: Enable ruff rule banning print in assert ([#146615](https://github.com/pytorch/pytorch/pull/146615))
- Remove outdated comment in ATen/mkl/Sparse.h about lack of Windows support ([#147125](https://github.com/pytorch/pytorch/pull/147125))
- [Intel GPU] Avoid copy when the input of Matmul is broadcasted ([#143784](https://github.com/pytorch/pytorch/pull/143784))
- [torch][amdsmi] Look for amdsmi in ROCM_HOME/ROCM_PATH before using rpath ([#147117](https://github.com/pytorch/pytorch/pull/147117))
- [Break XPU][Inductor UT] Fix XPU Inductor UT failures introduced from community. ([#146762](https://github.com/pytorch/pytorch/pull/146762))
- [Break XPU][Inductor UT] Set input tensors to corresponding device for test case in test_aot_indutor.py ([#145248](https://github.com/pytorch/pytorch/pull/145248))
- [XPU] Align XPU convolution_backward output layout between fake tensor and real output tensor. ([#146880](https://github.com/pytorch/pytorch/pull/146880))
- Add Structured Tracing for Traced Graph Edge Details for AC Debugging ([#146634](https://github.com/pytorch/pytorch/pull/146634))
- Update torch-xpu-ops commit pin ([#146671](https://github.com/pytorch/pytorch/pull/146671))
- [export] Add meta for aten.bincount ([#147129](https://github.com/pytorch/pytorch/pull/147129))
- Nccl update to 2.25.1 for cuda 12.4-12.8  ([#146073](https://github.com/pytorch/pytorch/pull/146073))
- Introduce new template heuristic for triton autotune configs ([#144985](https://github.com/pytorch/pytorch/pull/144985))
- [cutlass backend][BE] refactor tests to remove duplicate logic ([#146743](https://github.com/pytorch/pytorch/pull/146743))
- Nccl update to 2.25.1 for cuda 12.4-12.8  ([#146073](https://github.com/pytorch/pytorch/pull/146073))
- [cond] make cond re-dispatch in proxy mode ([#146954](https://github.com/pytorch/pytorch/pull/146954))
- [AOTInductor] Guard RAII_cpuMalloc with macro ([#147150](https://github.com/pytorch/pytorch/pull/147150))
- update kineto submodule to include fix for windows build ([#147195](https://github.com/pytorch/pytorch/pull/147195))
- Remove code for Python < 3.9 ([#147181](https://github.com/pytorch/pytorch/pull/147181))
- remove unnecessary xpu availability check when retrieving aot flags ([#146966](https://github.com/pytorch/pytorch/pull/146966))
- Skip unsupported types by MPS in `test_torchinductor.py` ([#147211](https://github.com/pytorch/pytorch/pull/147211))
- Update slow tests ([#147308](https://github.com/pytorch/pytorch/pull/147308))
- Fix test_device_memory_allocated ([#147311](https://github.com/pytorch/pytorch/pull/147311))
- Fix non-bitwise type annotations for Tensor operators (see #145838) ([#146845](https://github.com/pytorch/pytorch/pull/146845))
- [pt2-benchmarks] Compiler reset on every run ([#147313](https://github.com/pytorch/pytorch/pull/147313))
- Update torch-xpu-ops commit pin ([#147302](https://github.com/pytorch/pytorch/pull/147302))
- Update torch-xpu-ops commit pin ([#147358](https://github.com/pytorch/pytorch/pull/147358))
- Add link to non_blocking/pinmem tutorial in `Tensor.to` docstrings ([#145651](https://github.com/pytorch/pytorch/pull/145651))
- [cutlass backend] forward fix of standalone runner for fbcode ([#147158](https://github.com/pytorch/pytorch/pull/147158))
- Nccl update to 2.25.1 for cuda 12.4-12.8  ([#146073](https://github.com/pytorch/pytorch/pull/146073))
- realize stride symbols in estimate_runtime ([#146752](https://github.com/pytorch/pytorch/pull/146752))
- [Inductor UT][XPU] Skip fft_c2c case since it's not implemented on XPU. ([#147351](https://github.com/pytorch/pytorch/pull/147351))
- [ROCm] TopK optimizations for AMD GPUs ([#146387](https://github.com/pytorch/pytorch/pull/146387))
- [Inductor][Triton] Rework casting logic to avoid illegal bitcast ([#147395](https://github.com/pytorch/pytorch/pull/147395))
- [BE] Fix tensor stub ([#147384](https://github.com/pytorch/pytorch/pull/147384))
- [util] fetch logical count cpu ([#147413](https://github.com/pytorch/pytorch/pull/147413))
- Add hint message for `pack_padded_sequence` ([#146747](https://github.com/pytorch/pytorch/pull/146747))
- [CI] Do not overwrite return code of test file when fails for rerun disabled tests ([#147484](https://github.com/pytorch/pytorch/pull/147484))
- [CPU Stream] Add noop for CPU stream record_event() and wait_event() ([#145935](https://github.com/pytorch/pytorch/pull/145935))
- [cutlass backend] remove triton from most tests and add an integration test ([#147169](https://github.com/pytorch/pytorch/pull/147169))
- [cutlass backend] add subproc tests ([#147173](https://github.com/pytorch/pytorch/pull/147173))
- [cutlass backend] enable mixed mm test (cutlass2x) for H100 ([#147474](https://github.com/pytorch/pytorch/pull/147474))
- Fix c++ implementation of strip_function_call ([#147436](https://github.com/pytorch/pytorch/pull/147436))
- Increase memory for linux binary builds  ([#147542](https://github.com/pytorch/pytorch/pull/147542))
- [ROCm][TunableOp] Fix TunableOp warmup environment variable. ([#147412](https://github.com/pytorch/pytorch/pull/147412))
- Build a storage reader/writer to write checkpoints in HF format ([#146352](https://github.com/pytorch/pytorch/pull/146352))
- [cuDNN][SDPA][Nested Tensor] Experimental cuDNN Nested Tensor SDPA Support (forward only) ([#141178](https://github.com/pytorch/pytorch/pull/141178))
- [ROCm] Implemented dropout usage for RNN with MIOpen backend ([#144572](https://github.com/pytorch/pytorch/pull/144572))
- Increase memory for linux binary builds  ([#147542](https://github.com/pytorch/pytorch/pull/147542))
- Delete Mixed MM Special Casing ([#147151](https://github.com/pytorch/pytorch/pull/147151))
- [Intel GPU] Enable BUILD_GRAPH for xpu_mkldnn ([#147608](https://github.com/pytorch/pytorch/pull/147608))
- Use __qualname__ in add_safe_globals and update Unpickling error raised for Unsupported GLOBAL  ([#146815](https://github.com/pytorch/pytorch/pull/146815))
- [trymerge] Post initial starting merge comment on stacked PRs ([#147028](https://github.com/pytorch/pytorch/pull/147028))
- [trymerge] Post initial starting merge comment on stacked PRs ([#147028](https://github.com/pytorch/pytorch/pull/147028))
- constexpr all the things in irange.h ([#147633](https://github.com/pytorch/pytorch/pull/147633))
- [ROCm] change is_hip_clang() to always return True ([#147646](https://github.com/pytorch/pytorch/pull/147646))
- move _strobelight/example to avoid graph breaks ([#147547](https://github.com/pytorch/pytorch/pull/147547))
- Enable strobelight profiling  specific compile frame ids using COMPILE_STROBELIGHT_FRAME_FILTER ([#147549](https://github.com/pytorch/pytorch/pull/147549))
- [Submodule] [Cutlass] Update to 3.8.0 tag ([#147655](https://github.com/pytorch/pytorch/pull/147655))
- [mps/inductor] XFAIL adaptive_avg_pool_with_output_size_0. ([#147676](https://github.com/pytorch/pytorch/pull/147676))
- [MPS/inductor] Adjust more tests that depends on non-divisible input sizes ([#147681](https://github.com/pytorch/pytorch/pull/147681))
- [BE] add missing overload annotations for `tree_map_only` ([#147699](https://github.com/pytorch/pytorch/pull/147699))
- [cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces ([#145130](https://github.com/pytorch/pytorch/pull/145130))
- Update slow tests ([#147728](https://github.com/pytorch/pytorch/pull/147728))
- Upgrade submodule oneDNN to v3.7 ([#147498](https://github.com/pytorch/pytorch/pull/147498))
- [Intel GPU] Add SDPA implementation on XPU with OneDNN ([#147612](https://github.com/pytorch/pytorch/pull/147612))
- [ROCm][Windows] Fix unrecognized constexpr std::memcpy for HIP-clang ([#147316](https://github.com/pytorch/pytorch/pull/147316))
- [Intel GPU] qlinear.pointwise with mixed dtype support ([#136753](https://github.com/pytorch/pytorch/pull/136753))
- remove prints from partitioner ([#147749](https://github.com/pytorch/pytorch/pull/147749))
- [AOTI][refactor] Rename use_absolute_path to use_relative_path ([#147679](https://github.com/pytorch/pytorch/pull/147679))
- [AOTI][refactor] Replace run_command_and_check with CppBuilder.build ([#147680](https://github.com/pytorch/pytorch/pull/147680))
- Fix `ReferenceError: weakly-referenced object no longer exists` in cycle detector ([#146922](https://github.com/pytorch/pytorch/pull/146922))
- [Docs] Add `OpDTypes.any_common_cpu_cuda_one` ([#147605](https://github.com/pytorch/pytorch/pull/147605))
- torch._scaled_mm with MXFP8 ([#147548](https://github.com/pytorch/pytorch/pull/147548))
- [Quant] flip: throw runtime error for QUInt4x2 and QUInt2x4 input ([#147430](https://github.com/pytorch/pytorch/pull/147430))
- Delete Mixed MM Special Casing ([#147151](https://github.com/pytorch/pytorch/pull/147151))
- Update torch-xpu-ops commit pin ([#147743](https://github.com/pytorch/pytorch/pull/147743))
- [AOTI][refactor] Consolidate CppBuilder.build and CppBuilder.build_fbcode_cpu_re ([#147803](https://github.com/pytorch/pytorch/pull/147803))
- [BE] Parameterize TestSDPA in test_mps.py ([#147856](https://github.com/pytorch/pytorch/pull/147856))
- Fix the tiny doc descriptions ([#147319](https://github.com/pytorch/pytorch/pull/147319))
- Enabled force_shape_pad for triton tests in test_kernel_benchmark ([#147620](https://github.com/pytorch/pytorch/pull/147620))
- Add XuehaiPan to CODEOWNERS for C++ PyTree utilities ([#137408](https://github.com/pytorch/pytorch/pull/137408))
- [inductor][triton] Ignore block ptr advances for removed buffers ([#147193](https://github.com/pytorch/pytorch/pull/147193))
- Remove link to search survey ([#147751](https://github.com/pytorch/pytorch/pull/147751))
- cpp_wrapper: fix test_torchinductor* tests ([#146424](https://github.com/pytorch/pytorch/pull/146424))
- Updated test_cuda.py to rerun tests ([#147040](https://github.com/pytorch/pytorch/pull/147040))
- follow up to #147548, fix regression on MI300 ([#147878](https://github.com/pytorch/pytorch/pull/147878))
- [cutlass backend] try fix standlone runner test ([#147811](https://github.com/pytorch/pytorch/pull/147811))
- Add sparse tensors constructed via legacy constructor to _sparse_tensors_to_validate ([#147759](https://github.com/pytorch/pytorch/pull/147759))
- optimize the decomposition of aten.native_group_norm ([#144733](https://github.com/pytorch/pytorch/pull/144733))
- [cutlass backend] force_disable_caches for test_number_mm_precompiles ([#147901](https://github.com/pytorch/pytorch/pull/147901))
- [BE][EZ] Delete MacOS-12.3 xfail list ([#147905](https://github.com/pytorch/pytorch/pull/147905))
- torch._scaled_mm with MXFP8 ([#147548](https://github.com/pytorch/pytorch/pull/147548))
- Fix test_halide.py report invocation to re-run failed tests ([#147640](https://github.com/pytorch/pytorch/pull/147640))
- Split test_transformers.py ([#147441](https://github.com/pytorch/pytorch/pull/147441))
- Add MSVC version condition to "Fix for MSVC problem on Windows Arm64 (#136765)" ([#145076](https://github.com/pytorch/pytorch/pull/145076))
- Add basic Gaudi support to benchmarks/dynamo ([#145920](https://github.com/pytorch/pytorch/pull/145920))
- [dynamo] add sourceless builder for `types.MethodType` ([#147880](https://github.com/pytorch/pytorch/pull/147880))
- [Inductor][Triton] Rework casting logic to avoid illegal bitcast ([#147395](https://github.com/pytorch/pytorch/pull/147395))
- [logs][qol] Print log options alphabetically ([#147888](https://github.com/pytorch/pytorch/pull/147888))
- ROCm: Remove static specifier for allow_tf32 variable. ([#147186](https://github.com/pytorch/pytorch/pull/147186))
- [aotd] Alias of intermediate unwrap TensorAlias ([#147638](https://github.com/pytorch/pytorch/pull/147638))
- [hop] Support more output types for `flat_apply` ([#146714](https://github.com/pytorch/pytorch/pull/146714))
- Build a storage reader/writer to write checkpoints in HF format ([#147622](https://github.com/pytorch/pytorch/pull/147622))
- [do not merge yet] update grammar  ([#147996](https://github.com/pytorch/pytorch/pull/147996))
- [do not merge yet] update grammar  ([#147996](https://github.com/pytorch/pytorch/pull/147996))
- [ROCm][TunableOp] Remove extra transpose characters in hipBLASLt signature. ([#147900](https://github.com/pytorch/pytorch/pull/147900))
- torch.utils._content_store: fix error in hash_storage on XPU ([#147785](https://github.com/pytorch/pytorch/pull/147785))
- cpp_wrapper: use largeTensorTest for test memory checks ([#146991](https://github.com/pytorch/pytorch/pull/146991))
- Address source code building command for Intel GPU support ([#143476](https://github.com/pytorch/pytorch/pull/143476))
- Remove binaries/benchmark_args.h ([#147920](https://github.com/pytorch/pytorch/pull/147920))
- Udpate hw requirement for FP64 on "Getting Started on Intel GPU" ([#147802](https://github.com/pytorch/pytorch/pull/147802))
- Add _fft_r2c as core ATen ([#147998](https://github.com/pytorch/pytorch/pull/147998))
- torch._scaled_mm with MXFP8 ([#147548](https://github.com/pytorch/pytorch/pull/147548))
- [inductor][triton] Ignore block ptr advances for removed buffers ([#147193](https://github.com/pytorch/pytorch/pull/147193))
- Update torch-xpu-ops commit pin ([#147968](https://github.com/pytorch/pytorch/pull/147968))
- [ca] side-effect free initial trace: RAII PyCompilerInterface ([#147891](https://github.com/pytorch/pytorch/pull/147891))
- [Intel GPU] Avoid unnecessary copy when the dst of Matmul is non-contiguous ([#144759](https://github.com/pytorch/pytorch/pull/144759))
- [aotd] Log torch._functorch.config in tlparse ([#147883](https://github.com/pytorch/pytorch/pull/147883))
- [Intel GPU] Avoid including CPU oneDNN header files for Intel GPU ([#147969](https://github.com/pytorch/pytorch/pull/147969))
- Fix overflow in checkInBoundsForStorage ([#147352](https://github.com/pytorch/pytorch/pull/147352))
- Make Tensor.set_ validate storage_offset when sizes/strides are unchanged ([#147354](https://github.com/pytorch/pytorch/pull/147354))
- Validate inputs to _nested_view_from_buffer to prevent overflows ([#147356](https://github.com/pytorch/pytorch/pull/147356))
- [BE/metal] Rename REGISTER_I0_I1 to REGISTER_SPECIAL. ([#148036](https://github.com/pytorch/pytorch/pull/148036))
- [BE][CI] bump ruff to 0.9.8 ([#145606](https://github.com/pytorch/pytorch/pull/145606))
- [CI] test upload: better check for if job is rerun disabled tests ([#148027](https://github.com/pytorch/pytorch/pull/148027))
- Fix minor typo in python_nccl ([#148088](https://github.com/pytorch/pytorch/pull/148088))
- [dynamo] add sourceless builder for `types.MethodType` ([#147880](https://github.com/pytorch/pytorch/pull/147880))
- [Intel GPU] Add synchronize() in torch.utils.benchmark ([#147835](https://github.com/pytorch/pytorch/pull/147835))
- [Intel GPU] Decompule Intel GPU oneDNN from other backends ([#147926](https://github.com/pytorch/pytorch/pull/147926))
- [torchgen] Add support for schema with namespace ([#148038](https://github.com/pytorch/pytorch/pull/148038))
- stage 1 of depreate silent fallback of tuning gemm ([#147798](https://github.com/pytorch/pytorch/pull/147798))
- [user-triton] handle inline_asm_case ([#148043](https://github.com/pytorch/pytorch/pull/148043))
- [Submodule][FlashAttention] Bump to 2.7.4 ([#148147](https://github.com/pytorch/pytorch/pull/148147))
- Add cuda 11.8 guard for cufile preload ([#148184](https://github.com/pytorch/pytorch/pull/148184))
- add skips to test_notifies_oom and test_set_per_process_memory_fraction ([#148134](https://github.com/pytorch/pytorch/pull/148134))
- [triton 3.3] Fix inductor/test_profiler.py test ([#148230](https://github.com/pytorch/pytorch/pull/148230))
- [Break XPU][Inductor UT] Avoid custom op registration conflicts in test_auto_functionalize.py. ([#148155](https://github.com/pytorch/pytorch/pull/148155))
- [EZ][BE] Increase tolerances for interpolate op ([#148224](https://github.com/pytorch/pytorch/pull/148224))
- Remove unneeded Clang-tidy suppression ([#148246](https://github.com/pytorch/pytorch/pull/148246))
- separate f16 vectorized class from bf16 ([#146596](https://github.com/pytorch/pytorch/pull/146596))
- Fix macro for bit_cast in c10/util/bit_cast.h - one line change ([#148265](https://github.com/pytorch/pytorch/pull/148265))
- [fr] Added protection against missing stack frames in fr ([#148203](https://github.com/pytorch/pytorch/pull/148203))
- [ci] disable cudagraph for tts_angular on dashboard ([#148221](https://github.com/pytorch/pytorch/pull/148221))
- [import][inductor] Simplify grid handling ([#147583](https://github.com/pytorch/pytorch/pull/147583))
- Add AppendingByteSerializer class ([#148226](https://github.com/pytorch/pytorch/pull/148226))
- [Inductor] Avoid tensor slice overflow for large step ([#147433](https://github.com/pytorch/pytorch/pull/147433))
- [BE][Ez]: Update fmt submodule to 11.1.4 ([#148264](https://github.com/pytorch/pytorch/pull/148264))
- [BE] Fix extra semicolon warning ([#148284](https://github.com/pytorch/pytorch/pull/148284))
- Separate transpose from memory load/store and add load size support for convert_to_int32 ([#147067](https://github.com/pytorch/pytorch/pull/147067))
- Enable XPU for Inductor MM Triton Kernel Benchmark ([#148237](https://github.com/pytorch/pytorch/pull/148237))
- Significantly speed up save_cache_artifacts ([#148227](https://github.com/pytorch/pytorch/pull/148227))
- Fix extra semicolon warning ([#148291](https://github.com/pytorch/pytorch/pull/148291))
- [cutlass backend] Add main tests for mm, addmm and bmm - step 1 ([#148229](https://github.com/pytorch/pytorch/pull/148229))
- [invoke_subgraph] Run joint passes on the hop graphs ([#139325](https://github.com/pytorch/pytorch/pull/139325))
- [cutlass backend] Benchmark compared to aten and triton ([#148347](https://github.com/pytorch/pytorch/pull/148347))
- Better log message to update pr_time_benchmarks/expected_results.csv ([#148303](https://github.com/pytorch/pytorch/pull/148303))
- Enable ASAN in CUDA tests ([#147812](https://github.com/pytorch/pytorch/pull/147812))
- Bump onnxscript to 0.2.2 in CI ([#148388](https://github.com/pytorch/pytorch/pull/148388))
- updates to benchmarks ([#144831](https://github.com/pytorch/pytorch/pull/144831))
- Create unique test report files for distributed tests ([#148325](https://github.com/pytorch/pytorch/pull/148325))
- [Inductor] Record Triton’s Base32 Cache Key in `.best_config` for Debugging ([#147019](https://github.com/pytorch/pytorch/pull/147019))
- Fix code descriptions in the test package. ([#148145](https://github.com/pytorch/pytorch/pull/148145))
- Bump onnxscript to 0.2.2 in CI ([#148388](https://github.com/pytorch/pytorch/pull/148388))
- [cuDNN][SDPA][Nested Tensor] Experimental cuDNN Nested Tensor SDPA Support (forward only) ([#141178](https://github.com/pytorch/pytorch/pull/141178))
- stage 1 of depreate silent fallback of tuning gemm ([#147798](https://github.com/pytorch/pytorch/pull/147798))
- [ROCm] [TunableOp] Track top solutions during tuning process ([#147243](https://github.com/pytorch/pytorch/pull/147243))
- [TEST][SPARSE] Simplify branching in test_cusparselt_backend ([#148318](https://github.com/pytorch/pytorch/pull/148318))
- add input shape check for _local_scalar_dense ([#145717](https://github.com/pytorch/pytorch/pull/145717))
- [ROCm] fix CK compile for gfx1200 ([#148496](https://github.com/pytorch/pytorch/pull/148496))
- Add note to get start xpu ([#148168](https://github.com/pytorch/pytorch/pull/148168))
- [ROCm] Bump AOTriton to 0.9.1b ([#148433](https://github.com/pytorch/pytorch/pull/148433))
- [cutlass backend] fix assertion that prevent self multiplication  ([#148233](https://github.com/pytorch/pytorch/pull/148233))
- Re-enable test_torchinductor:test_buffer_batch_norm ([#148573](https://github.com/pytorch/pytorch/pull/148573))
- [ROCm] Fix sort for non-standard bool ([#147459](https://github.com/pytorch/pytorch/pull/147459))
- Remove CAFFE2_USE_EXCEPTION_PTR ([#147247](https://github.com/pytorch/pytorch/pull/147247))
- [cutlass backend] Forward fix for less aligned gemm shapes ([#148521](https://github.com/pytorch/pytorch/pull/148521))
- [Break XPU][Inductor UT] Generalize device-bias code introduced by #146866. ([#148534](https://github.com/pytorch/pytorch/pull/148534))
- Add information about checkpoint offset to untyped storages when torch.load under FakeTensorMode ([#147787](https://github.com/pytorch/pytorch/pull/147787))
- Make torch.serialization.skip_data work with torch.load ([#148018](https://github.com/pytorch/pytorch/pull/148018))
- [triton 3.3] test_triton_kernel_constants fix ([#148626](https://github.com/pytorch/pytorch/pull/148626))
- [BE] Remove `onlyCPU` decorator from test_local_scalar_dense ([#148559](https://github.com/pytorch/pytorch/pull/148559))
- [pytree] fix previously failed dynamo tests ([#148669](https://github.com/pytorch/pytorch/pull/148669))
- [AOTI][dashboard] Skip torchbench models not supported by export ([#148359](https://github.com/pytorch/pytorch/pull/148359))
- Update CPU tolerance for f16 triplet margin loss ([#147742](https://github.com/pytorch/pytorch/pull/147742))
- [pytree] add APIs to determine a class is a namedtuple or PyStructSequence ([#113257](https://github.com/pytorch/pytorch/pull/113257))
- [cutlass backend] Forward fix for less aligned gemm shapes ([#148521](https://github.com/pytorch/pytorch/pull/148521))
- [cutlass backend] fix assertion that prevent self multiplication  ([#148233](https://github.com/pytorch/pytorch/pull/148233))
- ROCm: Disable torch check for Multiplication of two Float8_e5m2 matrices ([#148228](https://github.com/pytorch/pytorch/pull/148228))
- Documents torch.cuda.MemPool API ([#148374](https://github.com/pytorch/pytorch/pull/148374))
- [Intel GPU][pt2e]: Collapse 3D input to 2D for matmul in qlinear_pointwise_binary fusion ([#148423](https://github.com/pytorch/pytorch/pull/148423))
- Add sparsity ([#148513](https://github.com/pytorch/pytorch/pull/148513))
- Skip buffer in dense update ([#148533](https://github.com/pytorch/pytorch/pull/148533))
- Expose functions used in custom backend in torch_python dll ([#148213](https://github.com/pytorch/pytorch/pull/148213))
- [XPU] Add test/kernel.errors.txt to .gitignore. ([#148538](https://github.com/pytorch/pytorch/pull/148538))
- Throws error when using torch.cuda.MemPool with expandable segments ([#148378](https://github.com/pytorch/pytorch/pull/148378))
- Remove deprecated std::aligned_storage_t ([#148660](https://github.com/pytorch/pytorch/pull/148660))
- [Intel GPU] Fix SDPA dummy LSE output to match meta function ([#148652](https://github.com/pytorch/pytorch/pull/148652))
- [ROCm] [TunableOp] Enable logging of BLAS parameters ([#147034](https://github.com/pytorch/pytorch/pull/147034))
- [Intel GPU][quant] Refine zero-point memory creation ([#148640](https://github.com/pytorch/pytorch/pull/148640))
- Re-enable tests ([#148732](https://github.com/pytorch/pytorch/pull/148732))
- [ROCm][Windows] Disable Composable Kernels and Triton for Windows builds ([#147334](https://github.com/pytorch/pytorch/pull/147334))
- Fix Wc++98-compat-extra-semi ([#148757](https://github.com/pytorch/pytorch/pull/148757))
- Remove Cuda 12.4 from nightly Binaries  ([#148625](https://github.com/pytorch/pytorch/pull/148625))
- [pytree][easy] lock global registry containers properly for thread-safety ([#148750](https://github.com/pytorch/pytorch/pull/148750))
- [Docs][TunableOp] TunableOp documentation update ([#148384](https://github.com/pytorch/pytorch/pull/148384))
- [ROCm] Bump AOTriton to 0.9.2b ([#148433](https://github.com/pytorch/pytorch/pull/148433))
- [ca] remove compiled_autograd_tracing ([#148381](https://github.com/pytorch/pytorch/pull/148381))
- [aot cache][ca] remove restriction on caching ca's aot inference graph ([#148491](https://github.com/pytorch/pytorch/pull/148491))
- [triton 3.3] support both specialize_impl and create_specialize_impl ([#148806](https://github.com/pytorch/pytorch/pull/148806))
- [CUDA][TF32] Account for tf32 in `test_efficient_conv_bn_eval` ([#148802](https://github.com/pytorch/pytorch/pull/148802))
- Fix typos in SpectralOps.cpp ([#148818](https://github.com/pytorch/pytorch/pull/148818))
- Change nvcc arch flags for sm100 ([#148774](https://github.com/pytorch/pytorch/pull/148774))
- [set_linter] allow x in {...} ([#148422](https://github.com/pytorch/pytorch/pull/148422))
- [dynamo] allow global import `from collections import deque` in user code ([#148676](https://github.com/pytorch/pytorch/pull/148676))
- [Inductor UT][XPU] Skip test case test_cat_max_autotune_triton  for known issue. ([#148734](https://github.com/pytorch/pytorch/pull/148734))
- Better log message to update pr_time_benchmarks/expected_results.csv ([#148303](https://github.com/pytorch/pytorch/pull/148303))
- [cutlass backend] Forward fix for less aligned gemm shapes ([#148521](https://github.com/pytorch/pytorch/pull/148521))
- [cutlass backend] fix assertion that prevent self multiplication  ([#148233](https://github.com/pytorch/pytorch/pull/148233))
- [ROCm][Windows] Fix ROCm/HIP version header ([#148560](https://github.com/pytorch/pytorch/pull/148560))
- Update slow tests ([#148873](https://github.com/pytorch/pytorch/pull/148873))
- [dynamo] allow global import `from collections import deque` in user code ([#148676](https://github.com/pytorch/pytorch/pull/148676))
- Add timm_efficientnet to flaky models after cuda 12.6 update in CI/CD ([#148788](https://github.com/pytorch/pytorch/pull/148788))
- Add ccode for FloorDiv ([#148727](https://github.com/pytorch/pytorch/pull/148727))
- Bump Clang-tidy to 19.1.4 ([#148648](https://github.com/pytorch/pytorch/pull/148648))
- Enable Direct Use of Arm Compute Library (ACL) in ATen ([#148584](https://github.com/pytorch/pytorch/pull/148584))
- Update RELEASE.md with latest changes to release process and release 2.7 information ([#148888](https://github.com/pytorch/pytorch/pull/148888))
- Update torch-xpu-ops commit pin ([#148881](https://github.com/pytorch/pytorch/pull/148881))
- [cutlass backend][ez] Incorporate AOTI dynamic shape test into main test of MM ([#148786](https://github.com/pytorch/pytorch/pull/148786))
- [ROCm][Windows] Enable hipblaslt for Windows ([#148563](https://github.com/pytorch/pytorch/pull/148563))
- setuptools pinning ([#148879](https://github.com/pytorch/pytorch/pull/148879))
- [ROCm][Windows] Fix OpenMP Flags for clang-cl ([#148097](https://github.com/pytorch/pytorch/pull/148097))
- Enable ASAN on inductor CUDA tests ([#148749](https://github.com/pytorch/pytorch/pull/148749))
- Fix invalid nested int guarding in broadcast_shapes() ([#145957](https://github.com/pytorch/pytorch/pull/145957))
- Refactor `test/test_torch.py` by moving testcase to `test_indexing.py` ([#148875](https://github.com/pytorch/pytorch/pull/148875))
- [AOTI] Remove aoti_torch_cpu__weight_int4pack_mm_cpu_tensor ([#148907](https://github.com/pytorch/pytorch/pull/148907))
- [Window][Inductor UT] Fix for tempfile.NamedTemporaryFile(delete=True) not work on Windows. ([#148632](https://github.com/pytorch/pytorch/pull/148632))
- Update low prec codegen for div/mod ([#142350](https://github.com/pytorch/pytorch/pull/142350))
- Backout D70075331 ([#148824](https://github.com/pytorch/pytorch/pull/148824))
- Remove outdated skipIfRocmVersionLessThan decorations ([#148941](https://github.com/pytorch/pytorch/pull/148941))
- Reland: [inductor] Simplify grid handling ([#148305](https://github.com/pytorch/pytorch/pull/148305))
- [ROCm] CK Memory-Efficient Attention (attention bias support) ([#147778](https://github.com/pytorch/pytorch/pull/147778))
- [ROCm] Fix TORCH_CHECK for hdim 512 support added in AOTriton 0.9b ([#148967](https://github.com/pytorch/pytorch/pull/148967))
- [cutlass backend] Add addmm and bmm tests for AOTI ([#148929](https://github.com/pytorch/pytorch/pull/148929))
- [logging] Set compile_id in the CachingAutotuner during compilation so we have it for dynamo_timed logging ([#148693](https://github.com/pytorch/pytorch/pull/148693))
- [CI] Update crossvit_9_240 as pass ([#148989](https://github.com/pytorch/pytorch/pull/148989))
- [CI] Don't clean workspace when fetching repo ([#147994](https://github.com/pytorch/pytorch/pull/147994))
- [ROCm][TunableOp] Fix TunableOp BLAS logging for online tuning case. ([#148979](https://github.com/pytorch/pytorch/pull/148979))
- [ez] include config as part of __all__ in torch.compiler ([#148978](https://github.com/pytorch/pytorch/pull/148978))
- test/dynamo/test_utils: Fix one broken test on different python versions ([#148987](https://github.com/pytorch/pytorch/pull/148987))
### security
