
# Release Notes worksheet distributed

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please clean up and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## distributed
### bc breaking
### deprecation
### new features
### improvements
- [BE] Correctly pass exceptions raised from `rpc_init` to CPython ([#154325](https://github.com/pytorch/pytorch/pull/154325))
### bug fixes
- [c10d] Fix extra CUDA context created by barrier ([#149144](https://github.com/pytorch/pytorch/pull/149144))
- Make torch importable if compiled without TensorPipe ([#154382](https://github.com/pytorch/pytorch/pull/154382))
### performance
### docs
### devs
### Untopiced
- Create and send `full_tensor` on `ProcessGroup`-supported device in `_broadcast_tensors` ([#148865](https://github.com/pytorch/pytorch/pull/148865))
- [partitioner] always ban compiler-driven recompute of collectives by default ([#147561](https://github.com/pytorch/pytorch/pull/147561))
- [DTensor] Fix `local_map` with multi-threading ([#149070](https://github.com/pytorch/pytorch/pull/149070))
- [c10d] Make getDefaultBackend more fault tolerant without relying on exceptions ([#149152](https://github.com/pytorch/pytorch/pull/149152))
- [c10d] Add param recording for uniqueID broadcasting and allgather ([#149166](https://github.com/pytorch/pytorch/pull/149166))
- add PrivateUse1 backend in fsdp collecitves ([#147260](https://github.com/pytorch/pytorch/pull/147260))
- update error message in get_backend() more detail_ ([#141796](https://github.com/pytorch/pytorch/pull/141796))
- [FSDP2] Update ignored_params docstring and add unit test ([#149074](https://github.com/pytorch/pytorch/pull/149074))
- Support subclass constructor capturing in export ([#147014](https://github.com/pytorch/pytorch/pull/147014))
- [DCP] Avoid in-place update and deepcopy during dudpe ([#149320](https://github.com/pytorch/pytorch/pull/149320))
- [FSDP2] Add set_reshard_after_forward ([#149103](https://github.com/pytorch/pytorch/pull/149103))
- [State_dict] Remove functools.cache and add unit test ([#149354](https://github.com/pytorch/pytorch/pull/149354))
- [c10d] Add a collective time estimator for NCCL comms ([#149343](https://github.com/pytorch/pytorch/pull/149343))
- [AOTI][reland] Update test runner to use the new APIs ([#149412](https://github.com/pytorch/pytorch/pull/149412))
- Specify the default PyTorch Distributed backend for MPS ([#149538](https://github.com/pytorch/pytorch/pull/149538))
- [distributed] fix: use group rank instead of global rank when possible ([#149488](https://github.com/pytorch/pytorch/pull/149488))
- [Distributed] Add `repr` methods for `ParallelStyle`s ([#149478](https://github.com/pytorch/pytorch/pull/149478))
- Supporting non-tensor-data write_size in planner write items. ([#149699](https://github.com/pytorch/pytorch/pull/149699))
- [FSDP2] warning that reshard_after_forward=1 and True are different ([#149750](https://github.com/pytorch/pytorch/pull/149750))
- DTensor: more generically support CompositeImplicitAutograd ops under inference mode ([#149514](https://github.com/pytorch/pytorch/pull/149514))
- [torch/c10d] change class variable from private to protected (#149579) ([#149645](https://github.com/pytorch/pytorch/pull/149645))
- ProcessGroupGloo: support ReduceOp::AVG ([#149781](https://github.com/pytorch/pytorch/pull/149781))
- ProcessGroupGloo: support reduce_scatter + update support chart ([#149869](https://github.com/pytorch/pytorch/pull/149869))
- [DCP] Cache save plan metadata to reduce the collective overhead ([#149785](https://github.com/pytorch/pytorch/pull/149785))
- [Async TP] More robust support for rowwise scales when fusing matmul reduce-scatter ([#149247](https://github.com/pytorch/pytorch/pull/149247))
- TCPStoreLibUvBackend: support masterListenFd ([#150215](https://github.com/pytorch/pytorch/pull/150215))
- [BE] Remove outdated RPC benchmark ([#146716](https://github.com/pytorch/pytorch/pull/146716))
- fix et trace collection of all_to_all ([#149485](https://github.com/pytorch/pytorch/pull/149485))
- [Async TP] Fuse matmul-reduce-scatters when reduce scatters have multiple users, and save fused node for backward instead of reduce_scatter node ([#149946](https://github.com/pytorch/pytorch/pull/149946))
- [c10d][fr] Allow multiple writer registration with warnings ([#150232](https://github.com/pytorch/pytorch/pull/150232))
- Fix space typo in warning message ([#143473](https://github.com/pytorch/pytorch/pull/143473))
- Fix typo ([#150363](https://github.com/pytorch/pytorch/pull/150363))
- [dtensor][tp] add a ParallelStyle PrepareModuleInputOutput ([#150372](https://github.com/pytorch/pytorch/pull/150372))
- gloo: use shared Stores ([#150230](https://github.com/pytorch/pytorch/pull/150230))
- [fr] Add logger config for flight record in PGNCCL ([#150356](https://github.com/pytorch/pytorch/pull/150356))
- assert on all_reduce_event only if it's not CPU device. ([#150316](https://github.com/pytorch/pytorch/pull/150316))
- [export] specialize for aten.to ([#149235](https://github.com/pytorch/pytorch/pull/149235))
- [c10d] Add logging for desync debug report ([#150513](https://github.com/pytorch/pytorch/pull/150513))
- Fix detection of GPU multicast ([#150563](https://github.com/pytorch/pytorch/pull/150563))
- [c10d] Surface error type when we unlink and create named pipe for DumpPipe ([#150648](https://github.com/pytorch/pytorch/pull/150648))
- [c10d][fr] Improve FR dump robustness with all watchdog broadcast wait and more frequent store check ([#150652](https://github.com/pytorch/pytorch/pull/150652))
- [DCP][OSS] Introduce barrier util in the DistWrapper for rank local checkpointing ([#150748](https://github.com/pytorch/pytorch/pull/150748))
- Support having no metadata file for HuggingFaceStorageReader ([#150701](https://github.com/pytorch/pytorch/pull/150701))
- Safer bookkeeping of NCCL communicators ([#150681](https://github.com/pytorch/pytorch/pull/150681))
- Clarify behavior of TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK ([#150682](https://github.com/pytorch/pytorch/pull/150682))
- [ez][c10d] Disable start event recording for coalesced col and improve profile title ([#150863](https://github.com/pytorch/pytorch/pull/150863))
- add reduce_scatter to symm mem ops ([#150813](https://github.com/pytorch/pytorch/pull/150813))
- [DTensor] StridedShard support uneven sharding ([#150490](https://github.com/pytorch/pytorch/pull/150490))
- Code Clean: Using the new builtin function provides by python 3.8 later ([#150839](https://github.com/pytorch/pytorch/pull/150839))
- [c10d][tcp_store] Fix connection reset caused by wrong socket close ([#150987](https://github.com/pytorch/pytorch/pull/150987))
- [C10D] Document object collectives limitations ([#150815](https://github.com/pytorch/pytorch/pull/150815))
- [c10d][libuv] Add back correct EOF case check ([#151052](https://github.com/pytorch/pytorch/pull/151052))
- Reapply "ProcessGroupGloo: support lazy_init (#150801)" ([#151031](https://github.com/pytorch/pytorch/pull/151031))
- [1/N] Use internal linkage in torch/csrc C++ files. ([#150930](https://github.com/pytorch/pytorch/pull/150930))
- c10d/Store: add clone feature (#150966) (#150966) ([#151045](https://github.com/pytorch/pytorch/pull/151045))
- [dtensor] add op support for torch.cumsum ([#151071](https://github.com/pytorch/pytorch/pull/151071))
- Register also future allocations in mempool with NCCL ([#150684](https://github.com/pytorch/pytorch/pull/150684))
- [c10d][fr] Add logging of nccl_version into fr and its dump ([#151048](https://github.com/pytorch/pytorch/pull/151048))
- c10d/Store: add queues ([#150969](https://github.com/pytorch/pytorch/pull/150969))
- [dtensor] add op support for torch._grouped_mm ([#151072](https://github.com/pytorch/pytorch/pull/151072))
- [DTensor] Add DTensor redistribute fwd/bwd datatype conversion to enable SimpleFSDP mixed precision training ([#150740](https://github.com/pytorch/pytorch/pull/150740))
- [c10d] Add `_allgather_base` , `reduce_scatter` , and `_reduce_scatter_base` into ProcessGroupMPI to enable FSDP with MPI backend ([#150162](https://github.com/pytorch/pytorch/pull/150162))
- [PP] Add schedule visualizer ([#150347](https://github.com/pytorch/pytorch/pull/150347))
- [c10d][fr] Record each individual collective being coalesced ([#151238](https://github.com/pytorch/pytorch/pull/151238))
- [c10d][fr] Add counters for FR dump and reduce its timeout to finish dump before watchdog timeout ([#151329](https://github.com/pytorch/pytorch/pull/151329))
- Fix lint (c4688af254c)
- [DDP] add one option to allow skipping all reduce unused parameters ([#151503](https://github.com/pytorch/pytorch/pull/151503))
- [C10D] avoid computing global_rank when group_rank is used ([#151373](https://github.com/pytorch/pytorch/pull/151373))
- c10d/Store: add nonblocking mode to queue_pop ([#151485](https://github.com/pytorch/pytorch/pull/151485))
- Turn off symm_mem when cuda version is <12.3 ([#151203](https://github.com/pytorch/pytorch/pull/151203))
- add privateuse1 device type to pre forward hook of fsdp ([#149487](https://github.com/pytorch/pytorch/pull/149487))
- Add api to enable/disable NaN detector per-PG ([#151723](https://github.com/pytorch/pytorch/pull/151723))
- [symmem] Add some code comments to rendezvous code ([#151716](https://github.com/pytorch/pytorch/pull/151716))
- logging start of torch elastic workers. ([#150849](https://github.com/pytorch/pytorch/pull/150849))
- Updates NCCLConfig with QOS variable ([#151821](https://github.com/pytorch/pytorch/pull/151821))
- Reland fast gather and index implementation ([#151917](https://github.com/pytorch/pytorch/pull/151917))
- [FSDP1] print fqns when debug FlatParamHandle ([#151336](https://github.com/pytorch/pytorch/pull/151336))
- Reducer: add check on received data to avoid segfault ([#152143](https://github.com/pytorch/pytorch/pull/152143))
- Fix xrefs ([#151888](https://github.com/pytorch/pytorch/pull/151888))
- Add option to use mempool on OOM ([#151487](https://github.com/pytorch/pytorch/pull/151487))
- [DTensor] Error on illegal view op during sharding prop ([#149764](https://github.com/pytorch/pytorch/pull/149764))
- Fix an incorrect link markup ([#152239](https://github.com/pytorch/pytorch/pull/152239))
- Fix redistribute new_local_tensor be None case ([#152303](https://github.com/pytorch/pytorch/pull/152303))
- Add rich support to torch.distributed.tensor.debug.visualize_sharding ([#152027](https://github.com/pytorch/pytorch/pull/152027))
- [pt2d] Add reorder_comms_preserving_peak_memory pass ([#146562](https://github.com/pytorch/pytorch/pull/146562))
- [1/N] Use std::filesystem ([#152288](https://github.com/pytorch/pytorch/pull/152288))
- Expose NCCL communicator from ProcessGroupNCCL via an unsafe API ([#152496](https://github.com/pytorch/pytorch/pull/152496))
- add split sizes info dump for uneven all2all bw calculation ([#151438](https://github.com/pytorch/pytorch/pull/151438))
- [SymmMem] Experimental NVSHMEM integration ([#151261](https://github.com/pytorch/pytorch/pull/151261))
- [SymmMem] Add all-to-all ([#151498](https://github.com/pytorch/pytorch/pull/151498))
- [SymmMem] Add all_to_all_vdev ([#151819](https://github.com/pytorch/pytorch/pull/151819))
- [ROCm] Add support for SymmetricMemory ([#150580](https://github.com/pytorch/pytorch/pull/150580))
- [DCP] Add 30min timeout for IPC communications in async checkpointing ([#152629](https://github.com/pytorch/pytorch/pull/152629))
- [SymmMem] Use cub's BlockScan instead of in-house impl for offset calculation ([#151993](https://github.com/pytorch/pytorch/pull/151993))
- add support for 0 size shardedTensor and recalculate metadata from all_gather ([#152583](https://github.com/pytorch/pytorch/pull/152583))
- [BE]: Cleanup traceutils with fmtlib ([#152265](https://github.com/pytorch/pytorch/pull/152265))
- [Flight Recorder] Added logging after FR dump completed ([#152648](https://github.com/pytorch/pytorch/pull/152648))
- [PGNCCL] Add FP8 support ([#152706](https://github.com/pytorch/pytorch/pull/152706))
- [ROCm] Fix SymmetricMemory build error on NAVI arch ([#152838](https://github.com/pytorch/pytorch/pull/152838))
- [c10d] Fix unused `group` input argument in `new_subgroups()` ([#152765](https://github.com/pytorch/pytorch/pull/152765))
- Fix bug visualizing 1D Tensor using rich ([#152871](https://github.com/pytorch/pytorch/pull/152871))
- [c10d][fr] Decouple the core logic of FR with the entry and event type ([#152585](https://github.com/pytorch/pytorch/pull/152585))
- [c10d][fr] Make FR vendor neutral so that other backends can use it ([#152563](https://github.com/pytorch/pytorch/pull/152563))
- Detect NVSHMEM location ([#153010](https://github.com/pytorch/pytorch/pull/153010))
- fix shard tensor gather when a local tensor on certain ranks has zero elements ([#150914](https://github.com/pytorch/pytorch/pull/150914))
- [C10D] Move getNcclDataType into NCCLUtils ([#153113](https://github.com/pytorch/pytorch/pull/153113))
- c10d/gloo: add ibverbs backend ([#153015](https://github.com/pytorch/pytorch/pull/153015))
- [FSDP2][Doc] add pointer to torchtitan ([#153079](https://github.com/pytorch/pytorch/pull/153079))
- [Ez][BE]: Fix KeyError LOGNAME ([#153324](https://github.com/pytorch/pytorch/pull/153324))
- add needs_contiguous_strides tag ([#153399](https://github.com/pytorch/pytorch/pull/153399))
- [c10d][fr] Add try catch to update entry due to cuda error ([#153414](https://github.com/pytorch/pytorch/pull/153414))
- gloo: support ibverbs in cmake ([#153425](https://github.com/pytorch/pytorch/pull/153425))
- elastic: do not shutdown rendezvous on leaving workers ([#152525](https://github.com/pytorch/pytorch/pull/152525))
- [SymmMem][a2av] Fix TODO: change stride unit ([#153483](https://github.com/pytorch/pytorch/pull/153483))
- [SymmMem][a2av] Use more CTAs for intra-node case ([#153509](https://github.com/pytorch/pytorch/pull/153509))
- [device_mesh] improve device selection logic ([#150897](https://github.com/pytorch/pytorch/pull/150897))
- [DCP] Use multiprocess Pipes instead of Queues to improve communication contract with checkpointer process ([#153488](https://github.com/pytorch/pytorch/pull/153488))
- Add needs_contiguous_strides to more collective ops ([#153523](https://github.com/pytorch/pytorch/pull/153523))
- c10d/TCPStore: better logs on remote shutdown ([#153586](https://github.com/pytorch/pytorch/pull/153586))
- [c10d] Allow split_group to work with non nccl backends ([#152175](https://github.com/pytorch/pytorch/pull/152175))
- gloo: cuda ([#153406](https://github.com/pytorch/pytorch/pull/153406))
- [ddp] propagate use_python_reducer to C++ reducer ([#152735](https://github.com/pytorch/pytorch/pull/152735))
- [c10d] Fix `new_subgroups(group=)` bug ([#153798](https://github.com/pytorch/pytorch/pull/153798))
- [c10d] Simplify `new_subgroups()` by using `new_subgroups_by_enumeration()` ([#153843](https://github.com/pytorch/pytorch/pull/153843))
- make only current thread allocate to pool in NcclPG ([#153990](https://github.com/pytorch/pytorch/pull/153990))
- [c10d] Enhance Error Logging in `new_subgroups()` for Non-Divisible World Sizes ([#154124](https://github.com/pytorch/pytorch/pull/154124))
- Fix tcp init when using port 0 ([#154156](https://github.com/pytorch/pytorch/pull/154156))
- [PP] Allow unused kwargs in ZB path ([#153498](https://github.com/pytorch/pytorch/pull/153498))
- [a2av] Improve tuning for 4 GPUs ([#154580](https://github.com/pytorch/pytorch/pull/154580))
- [FSDP2] allow different dtypes for no grad model params ([#154103](https://github.com/pytorch/pytorch/pull/154103))
- Resubmit Remove MemPoolContext  (#154042) ([#154746](https://github.com/pytorch/pytorch/pull/154746))
- [FSDP2] respect reshard_after_forward=True for root model ([#154704](https://github.com/pytorch/pytorch/pull/154704))
- [c10d][fr] Split cuda and non-cuda fr logic into two cpp file ([#154929](https://github.com/pytorch/pytorch/pull/154929))
- [dtensor] fix simplefsdp mixed-precision training bugs ([#154975](https://github.com/pytorch/pytorch/pull/154975))
- Turn on compile with NVSHMEM ([#154538](https://github.com/pytorch/pytorch/pull/154538))
- [c10d][gloo] Integrate vendor generic FR into gloo ([#152614](https://github.com/pytorch/pytorch/pull/152614))
- [PT] expose FlightRecord API for building ([#154866](https://github.com/pytorch/pytorch/pull/154866))
- fix numpy compatibility for 2d small list indices ([#154806](https://github.com/pytorch/pytorch/pull/154806))
- Switch to _apply_to_tensors for dataclass input ([#154897](https://github.com/pytorch/pytorch/pull/154897))
- [c10d][gloo] Enable using c10::Half for gloo ([#153862](https://github.com/pytorch/pytorch/pull/153862))
- Release GIL in PG destructor ([#154976](https://github.com/pytorch/pytorch/pull/154976))
- [c10d][fr] Add the log of thread name and thread id into fr ([#155142](https://github.com/pytorch/pytorch/pull/155142))
- [a2av] 2D all-to-all-vdev ([#155058](https://github.com/pytorch/pytorch/pull/155058))
- [a2av] Align length of major dimension in output of 2D a2av ([#155172](https://github.com/pytorch/pytorch/pull/155172))
- [FSDP2] keep root unsharded when not specifying reshard_after_forward ([#155319](https://github.com/pytorch/pytorch/pull/155319))
- [1/n]adding torch.distributed.run option to provide destination for event logging (#154644) ([#155268](https://github.com/pytorch/pytorch/pull/155268))
- [symm_mem] Move all symm mem code into a dedicated folder ([#155573](https://github.com/pytorch/pytorch/pull/155573))
- Changes to HFStorageWriter to support saving shards of tensors (#154742) ([#155566](https://github.com/pytorch/pytorch/pull/155566))
- Updates to HFStorageReader to use TensorStorageMetadata instead of BytesStorageMetadata ([#154518](https://github.com/pytorch/pytorch/pull/154518))
- [c10d] Enhance `get_process_group_ranks()` to accept `group=None` ([#154902](https://github.com/pytorch/pytorch/pull/154902))
- [SymmMem] Enable NVSHMEM for Triton ([#155506](https://github.com/pytorch/pytorch/pull/155506))
- [2/n]passing  event log handler to record function calls ([#155457](https://github.com/pytorch/pytorch/pull/155457))
- Support re-sharding for safetensors checkpoints ([#154519](https://github.com/pytorch/pytorch/pull/154519))
- Skip updating the default device distributed backend if already registered ([#155320](https://github.com/pytorch/pytorch/pull/155320))
- [ROCm] Skip *_stress_cuda and test_ddp_apply_optim_in_backward* ([#155724](https://github.com/pytorch/pytorch/pull/155724))
- Support XPU in memory tracker ([#150703](https://github.com/pytorch/pytorch/pull/150703))
- [BE][c10d/Store]add check in pyi (#155855) ([#155865](https://github.com/pytorch/pytorch/pull/155865))
- [c10d][PGNCCL] Make watchdog thread a class ([#155831](https://github.com/pytorch/pytorch/pull/155831))
- Change _hfstorage to hfstorage ([#155837](https://github.com/pytorch/pytorch/pull/155837))
- Fix DDPOptimizer issue on static tensor index ([#155746](https://github.com/pytorch/pytorch/pull/155746))
- [ProcessGroupNCCL] Added log when fr dump triggered from pipe ([#155754](https://github.com/pytorch/pytorch/pull/155754))
- [SymmMem] Remove unused ptr_to_symm_mem_ ([#155968](https://github.com/pytorch/pytorch/pull/155968))
- [SymmMem] Remove wrappers around nvshmem APIs ([#155971](https://github.com/pytorch/pytorch/pull/155971))
- [c10d][fr] Shrink the range of mutex lock to avoid deadlock ([#155949](https://github.com/pytorch/pytorch/pull/155949))
- [SymmMem] Add nvshmem_free ([#155975](https://github.com/pytorch/pytorch/pull/155975))
- [BE][Ez]: Optimize nvshmem alloc with missing move ([#156000](https://github.com/pytorch/pytorch/pull/156000))
- [ca] Functionalize AccumulateGrad ([#155521](https://github.com/pytorch/pytorch/pull/155521))
- [SymmMem] Add NVSHMEM GET support to Triton ([#155890](https://github.com/pytorch/pytorch/pull/155890))
- Add FSDP2 logging ([#155826](https://github.com/pytorch/pytorch/pull/155826))
- Add warning for incorrected grad results at world size 1 ([#154928](https://github.com/pytorch/pytorch/pull/154928))
- [SymmMem] Cache rank_to_global_rank exchange ([#156116](https://github.com/pytorch/pytorch/pull/156116))
- [SymmMem] Make get_rank_to_global_rank return const ref ([#156117](https://github.com/pytorch/pytorch/pull/156117))
- Fix a small sphinx markup error ([#156061](https://github.com/pytorch/pytorch/pull/156061))
- Add get_pipeline_order() for Gpipe and 1F1B ([#155935](https://github.com/pytorch/pytorch/pull/155935))
- [c10d] Add a logger for all nccl collectives with its time duration when completed ([#156008](https://github.com/pytorch/pytorch/pull/156008))
- Allow forcing FSDP2 to always use SUM reductions ([#155915](https://github.com/pytorch/pytorch/pull/155915))
- Enable querying the build and runtime NCCL versions ([#156305](https://github.com/pytorch/pytorch/pull/156305))
- [dcp] add new checkpoint staging to preserve storage sharing and support mutable state_dicts ([#155192](https://github.com/pytorch/pytorch/pull/155192))
- [c10d] Disable stack trace call in logging ([#156362](https://github.com/pytorch/pytorch/pull/156362))
- [SymmMem] Add runtime detection of NVSHMEM ([#156291](https://github.com/pytorch/pytorch/pull/156291))
- [SymmMem] Add NVSHMEM PUT with Signal support to Triton ([#156211](https://github.com/pytorch/pytorch/pull/156211))
- [c10d] Disable NCCL NVLS when using deterministic mode ([#156381](https://github.com/pytorch/pytorch/pull/156381))
- add device generalisation support for distributed tests ([#152471](https://github.com/pytorch/pytorch/pull/152471))
- [c10d] init_process_group supports index-only device id ([#156214](https://github.com/pytorch/pytorch/pull/156214))
- [symm_mem] Add nccl as a backend for symmetric memory ([#155740](https://github.com/pytorch/pytorch/pull/155740))
- [SymmMem] Add NVSHMEM wait_until support to Triton ([#156472](https://github.com/pytorch/pytorch/pull/156472))
- [SymmMem] Add NVSHMEM signal_wait_until support to Triton  ([#156473](https://github.com/pytorch/pytorch/pull/156473))
- [symm_mem] Add one side put API for nvshvem ([#156443](https://github.com/pytorch/pytorch/pull/156443))
- [SymmMem] Add NVSHMEM Fence support to Triton  ([#156474](https://github.com/pytorch/pytorch/pull/156474))
- [SymmMem] Add NVSHMEM Quiet support to Triton  ([#156475](https://github.com/pytorch/pytorch/pull/156475))
- Enables NCCL symmetric memory kernels through mempool registration ([#155134](https://github.com/pytorch/pytorch/pull/155134))
- Improve All to All Perf for inter-node use-case (#156376) ([#156389](https://github.com/pytorch/pytorch/pull/156389))
- [SymmMem] Rename all_to_all_vdev ops ([#156582](https://github.com/pytorch/pytorch/pull/156582))
- [ROCm][SymmetricMemory] Avoid bf16 to float conversion during reduce ([#155587](https://github.com/pytorch/pytorch/pull/155587))
- Clean up HF components ([#155707](https://github.com/pytorch/pytorch/pull/155707))
- [fr] Use a vector to temporarily keep the reference to future object to avoid block ([#156653](https://github.com/pytorch/pytorch/pull/156653))
### not user facing
- Migrate aten.split.Tensor from using Sharding Rule to Sharding Strategy ([#149106](https://github.com/pytorch/pytorch/pull/149106))
- Add batch dim sharding rule to sdpa ([#149253](https://github.com/pytorch/pytorch/pull/149253))
- Let pointwise sharding take arg with largest number of dims in case of ties ([#149721](https://github.com/pytorch/pytorch/pull/149721))
- [ca] fix accumulate grad polyfill when different strides between param and grad ([#149651](https://github.com/pytorch/pytorch/pull/149651))
- [CUDA]][SymmetricMemory] Interpret empty string as `std::nullopt` in `rendezvous` ([#149793](https://github.com/pytorch/pytorch/pull/149793))
- Implement aten.select.int sharding strategy ([#149842](https://github.com/pytorch/pytorch/pull/149842))
- Unify use of `enableCollectiveHashDebug_` and trivial updates ([#142865](https://github.com/pytorch/pytorch/pull/142865))
- [ROCm] Enable several fsdp related UTs ([#149369](https://github.com/pytorch/pytorch/pull/149369))
- [ROCm] update test buffer fudge factor for hipblaslt ([#150348](https://github.com/pytorch/pytorch/pull/150348))
- [CUDA]][SymmetricMemory] Interpret empty string as `std::nullopt` in `rendezvous` ([#149793](https://github.com/pytorch/pytorch/pull/149793))
- [torchrec] update local_shards_wrapper to latest version ([#150469](https://github.com/pytorch/pytorch/pull/150469))
- [DTensor] add _explicit_order_placements util ([#150493](https://github.com/pytorch/pytorch/pull/150493))
- [CI][CUDA][Distributed]Update test_composability.py ([#148578](https://github.com/pytorch/pytorch/pull/148578))
- [DTensor] clean up _local_shard_size_and_offset ([#150650](https://github.com/pytorch/pytorch/pull/150650))
- Document poison fork note for accelerator APIs ([#147507](https://github.com/pytorch/pytorch/pull/147507))
- [elastic][test] fix race condition in test_barrier_timeout_rank_tracing ([#150768](https://github.com/pytorch/pytorch/pull/150768))
- [async TP] Fix handling of case where scatter dim = 0 for 2D output tensor ([#150935](https://github.com/pytorch/pytorch/pull/150935))
- [c10d][fr] Fix the false positive in the dtype check in fr analysis script ([#151063](https://github.com/pytorch/pytorch/pull/151063))
- [DTensor] Fix empty shard global-offset calculation ([#150862](https://github.com/pytorch/pytorch/pull/150862))
- test_store: fix timeout for test_queues ([#151252](https://github.com/pytorch/pytorch/pull/151252))
- clang-format CUDASymmetricMemory.cu ([#151260](https://github.com/pytorch/pytorch/pull/151260))
- [c10d][fr] Enable FR analysis script for all fast-path coalesce op ([#151243](https://github.com/pytorch/pytorch/pull/151243))
- Add @requires_multicast_support to test_multimem_all_gather ([#151227](https://github.com/pytorch/pytorch/pull/151227))
- [c10d][fr] Enable FR analysis script for rest of all coalesce op ([#151247](https://github.com/pytorch/pytorch/pull/151247))
- improve noop elimination for view ([#151095](https://github.com/pytorch/pytorch/pull/151095))
- [c10d][fr] Fix script for uneven reduce scatter and update test cases ([#151475](https://github.com/pytorch/pytorch/pull/151475))
- [BE][Easy]: Normalize Dim typing in torch distributed ([#151566](https://github.com/pytorch/pytorch/pull/151566))
- [c10d][fr] Fix another bug when we should continue when the op list is empty ([#151798](https://github.com/pytorch/pytorch/pull/151798))
- Fix DTensorTestBase to barrier with device ids ([#150896](https://github.com/pytorch/pytorch/pull/150896))
- [BE] follow autoformating and linter ([#151507](https://github.com/pytorch/pytorch/pull/151507))
- Move verbose warning to warning_once ([#152044](https://github.com/pytorch/pytorch/pull/152044))
- Refactor to use torch.accelerator.device_index instead of torch.cuda.device for generic device context manager ([#148880](https://github.com/pytorch/pytorch/pull/148880))
- [CP] Use TorchFunctionMode to dispatch SDPA for CP ([#147902](https://github.com/pytorch/pytorch/pull/147902))
- [BE][Easy]: Change typing to DimsType in dim_reduction ([#151677](https://github.com/pytorch/pytorch/pull/151677))
- Fix common_distributed.py to NOT set root logger ([#152319](https://github.com/pytorch/pytorch/pull/152319))
- [DTensor] make test_dtensor_ops report dtensor_args ([#152045](https://github.com/pytorch/pytorch/pull/152045))
- [CUDA][CUTLASS] CUTLASS 3.9 submodule upgrade ([#151253](https://github.com/pytorch/pytorch/pull/151253))
- Respect checkpointed boundaries when using knapsack formulation in the partitioner ([#141684](https://github.com/pytorch/pytorch/pull/141684))
- [CP] Fix the offsets to KV in backward ([#152625](https://github.com/pytorch/pytorch/pull/152625))
- [ROCm][CI] Enabled fp8 distributed tests in test_micro_pipeline_tp.py for MI300 ([#151977](https://github.com/pytorch/pytorch/pull/151977))
- Implement util function compute_global_tensor_shape for 1D device mesh ([#152751](https://github.com/pytorch/pytorch/pull/152751))
- Add a test for AsyncCollectiveTensor handling for maybe-view ops ([#152688](https://github.com/pytorch/pytorch/pull/152688))
- [BE][lint] fix PYFMT for PT-D code under torch.testing._internal, add them to the lint list ([#153114](https://github.com/pytorch/pytorch/pull/153114))
- [c10d] Remove unordered PG destroy test ([#153110](https://github.com/pytorch/pytorch/pull/153110))
- [c10d] Test multiple CUDA Graph captures ([#150040](https://github.com/pytorch/pytorch/pull/150040))
- [c10d] Reduce test verbosity ([#153116](https://github.com/pytorch/pytorch/pull/153116))
- [Graph Partition] Maintain relative order within partition during reordering ([#153111](https://github.com/pytorch/pytorch/pull/153111))
- [BE][DTensor] move torch.distributed._tensor import to torch.distributed.tensor in test files ([#153225](https://github.com/pytorch/pytorch/pull/153225))
- [Pipelining] Fix _batch_p2p bug for non-NCCL backends (#132644) ([#152938](https://github.com/pytorch/pytorch/pull/152938))
- [BE] fix failing test_dp_state_dict_save_load on ROCm CI where world_size=7 ([#153283](https://github.com/pytorch/pytorch/pull/153283))
- Fix test_fused_scaled_matmul_reduce_scatter when scatter_dim is 0 ([#153286](https://github.com/pytorch/pytorch/pull/153286))
- [device_mesh] replace dim_group_info with group_name ([#150898](https://github.com/pytorch/pytorch/pull/150898))
- Fix negative dim issue in for parallel loss context manager ([#152785](https://github.com/pytorch/pytorch/pull/152785))
- Add TEST_HPU flag to set device type ([#153461](https://github.com/pytorch/pytorch/pull/153461))
- Fix typo ([#153561](https://github.com/pytorch/pytorch/pull/153561))
- [Async TP] Fix dim swapping before reduction in fused_scaled_matmul_reduce_scatter ([#153595](https://github.com/pytorch/pytorch/pull/153595))
- Refactoring FSDP2 (_composable/fsdp) test cases to be device agnostic ([#149848](https://github.com/pytorch/pytorch/pull/149848))
- [BE] Remove extra semicolons from SymmetricMemory.hpp ([#154034](https://github.com/pytorch/pytorch/pull/154034))
- [Graph Partition] support removed arguments, NoneLayout, and mutation ([#153899](https://github.com/pytorch/pytorch/pull/153899))
- [DTensor] enable SimpleFSDP's composability with Tensor Parallel ([#152286](https://github.com/pytorch/pytorch/pull/152286))
- [c10d] Add support for testing SIGABRT return ([#153167](https://github.com/pytorch/pytorch/pull/153167))
- [c10d] Add support for testing SIGABRT return ([#153167](https://github.com/pytorch/pytorch/pull/153167))
- PYFMT lint grandfathered files 1 ([#154261](https://github.com/pytorch/pytorch/pull/154261))
- [c10d] Add support for testing SIGABRT return ([#153167](https://github.com/pytorch/pytorch/pull/153167))
- [SymmMem] Speed up tests ([#153677](https://github.com/pytorch/pytorch/pull/153677))
- Add `torch.Tensor._make_wrapper_subclass` to `torch/_C/__init__.pyi` ([#154022](https://github.com/pytorch/pytorch/pull/154022))
- [c10d][CI] Change expected return code in Sandcastle for Nan tests ([#154441](https://github.com/pytorch/pytorch/pull/154441))
- [BE][Ez] Update deprecated pybind11 functions ([#154798](https://github.com/pytorch/pytorch/pull/154798))
- [BE]: Replace printf with fmtlib call ([#154814](https://github.com/pytorch/pytorch/pull/154814))
- [CI][CUDA][UCC] Update test_c10d_ucc.py - remove xfailIfLinux because it now succeeds ([#150979](https://github.com/pytorch/pytorch/pull/150979))
- Add __main__ guards to distributed tests ([#154628](https://github.com/pytorch/pytorch/pull/154628))
- Add NVSHMEM to PYTORCH_EXTRA_INSTALL_REQUIREMENTS ([#154568](https://github.com/pytorch/pytorch/pull/154568))
- [Graph Partition] add symints to get_graph_inputs ([#154679](https://github.com/pytorch/pytorch/pull/154679))
- DOC: Convert to markdown: ddp_comm_hooks.rst, debugging_environment_variables.rst, deploy.rst, deterministic.rst, distributed.algorithms.join.rst ([#155298](https://github.com/pytorch/pytorch/pull/155298))
- Adapt dtensor tests to be device agnostic ([#154840](https://github.com/pytorch/pytorch/pull/154840))
- [NCCL] Expose new `ncclConfig_t` flags in 2.27 ([#155379](https://github.com/pytorch/pytorch/pull/155379))
- Convert rst files to md ([#155369](https://github.com/pytorch/pytorch/pull/155369))
- [dtensor] refactor PlacementStrategy -> OpSpec, move utils to OpSchema ([#155592](https://github.com/pytorch/pytorch/pull/155592))
- [DTensor] Support in gradient placement for local_map() ([#155181](https://github.com/pytorch/pytorch/pull/155181))
- rename distributed.rst to md ([#155767](https://github.com/pytorch/pytorch/pull/155767))
- [a2av] Test must allocate tensors symmetrically ([#155835](https://github.com/pytorch/pytorch/pull/155835))
- Convert to markdown: distributed.tensor.parallel.rst, distributed.tensor.rst, distributions.rst, dlpack.rst ([#155297](https://github.com/pytorch/pytorch/pull/155297))
- Allow MultiProcContinuousTest to set world_size ([#155920](https://github.com/pytorch/pytorch/pull/155920))
- Update test_schedule_multiproc to use world_size=2 ([#155921](https://github.com/pytorch/pytorch/pull/155921))
- [FSDP2] explain user contract for fully_shard ([#156070](https://github.com/pytorch/pytorch/pull/156070))
- [fr] Fix one error in analysis script when subPG world size is smaller than global size ([#156156](https://github.com/pytorch/pytorch/pull/156156))
- remove skipifrocm from composability tests ([#156036](https://github.com/pytorch/pytorch/pull/156036))
- Make the NCCL PG Options and Config copyable and safe to init standalone ([#155700](https://github.com/pytorch/pytorch/pull/155700))
- Make dtensor tests device agnostic ([#155687](https://github.com/pytorch/pytorch/pull/155687))
- [BE][11/16] fix typos in torch/ (torch/csrc/distributed/) ([#156321](https://github.com/pytorch/pytorch/pull/156321))
- [BE][11/16] fix typos in torch/ (torch/csrc/distributed/) ([#156321](https://github.com/pytorch/pytorch/pull/156321))
- Register hpu device to fake backend ([#156076](https://github.com/pytorch/pytorch/pull/156076))
- [ROCm] update state check for test_trace_while_active* ([#153545](https://github.com/pytorch/pytorch/pull/153545))
- [BE][PYFMT] migrate PYFMT for `test/inductor/` to `ruff format` ([#148186](https://github.com/pytorch/pytorch/pull/148186))
### security
